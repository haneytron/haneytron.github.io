[{"categories":["blog"],"content":" \u0026ldquo;Any sufficiently advanced technology is indistinguishable from magic\u0026rdquo; - Arthur C. Clarke\nToday I\u0026rsquo;d like to talk about LLMs. But first, I\u0026rsquo;d like to talk about an impressive invention from the late 1700s.\nThe Mechanical Turk The Mechanical Turk\r- or \u0026ldquo;The Turk\u0026rdquo; as people called it - was an autonomous chess-playing machine. Built in 1770, it went on tour across parts of the world for 84 years as a robot playing chess against human opponents.\nA Drawing of the Mechanical Turk People would stand in line for a chance to view the Turk at exhibitions. A select few were chosen to play against it. Players were always informed that The Turk would use the white pieces. In 1809, Napoleon I of France played against the machine. According to some records, the Turk went so far as to salute him before the match started!\nAs the popularity grew, so too did the sophistication of the machine. In 1819, some new changes to The Turk debuted:\nIt now allowed the opponent to make the first move. The king\u0026rsquo;s bishop\u0026rsquo;s pawn was eliminated from the Turk\u0026rsquo;s pieces. This \u0026ldquo;pawn handicap\u0026rdquo; created further discussion of, and interest in, the Turk. It even led to a book being written by W. J. Hunneman chronicling the matches played with this disadvantage. Despite the \u0026ldquo;pawn handicap\u0026rdquo;, when all was said and done the Turk\u0026rsquo;s record was 45 wins, 3 losses, and 2 stalemates. Not bad! Sadly, the Turk\u0026rsquo;s chess-playing career was ended when it was destroyed in a fire.\nMany books and articles were written during the Turk\u0026rsquo;s life about how it worked. Most were inaccurate, drawing incorrect inferences from external observations - because people didn\u0026rsquo;t know or understand how it actually worked.\nThe Turk Was a Fraud The Turk was a lie. It was never an automaton. It was always a sophisticated illusion. The design of the machine was such that it convinced people that it was not possible to hide a person within the machine. After all, part of the exhibition was opening all of the front cabinets and drawers to \u0026ldquo;prove\u0026rdquo; that no one was inside!\nBut after its demise, it was finally revealed that a human chess master hid inside the machine, operating it with a simple candle for light and a series of sophisticated levers. A truly ingenious device, especially given the period in which it existed.\nA Cross-Section Drawing of the Mechanical Turk Magical Thinking As with the Turk, a lack of understanding can create a sense of wonder, mystery, or even supernatural influence. We see this often in children who are developing their understanding of the world. We also see this in situations where complex technology is not fully grasped. Rather than apply a scientific eye and critical thinking, people instead choose to embrace the fantasy of possibility.\nArguably no real \u0026ldquo;harm\u0026rdquo; was done by believing in The Turk, as it was a spectacle for amusement, so perhaps people going along with the ruse was harmless. But today, a $100 billion dollar ruse is operating and unfolding right in front of us: LLMs (often referred to as \u0026ldquo;AI\u0026rdquo;).\nJust like the era when the Turk captivated audiences, today people are writing feverishly and constantly about LLMs. And just like the stuff written about the Turk, most articles on LLMs are inaccurate, drawing incorrect inferences from external observations - because people don\u0026rsquo;t know or understand how they actually work.\nWith this blog post, I aim to teach you exactly (at a very high level) how LLMs actually work. My only goal is to empower you with knowledge that you can use to draw better conclusions and make better decisions. So let\u0026rsquo;s dive in.\nLLMs Today Today, every social media site, blog, and tech-adjacent news site or subreddit thread seems to be talking 24/7 about LLMs. This is not surprising when you consider how many billions of dollars investors are pumping into AI startups.\n\u0026ldquo;In under three years, AI has come to dominate global tech spending in ways researchers are just starting to quantify. In 2024, for example, AI companies nabbed 45 percent of all US venture capital tech investments, up from only nine percent in 2022.\u0026rdquo; (source)\rThey are currently an ever-present part of our lives, with many major companies trying to force the latest models into our collective lives via forced updates of software and hardware that we never really owned in the first place. My Android phone is now slathered in Gemini whether I like it or not, and I am sure Apple is doing similarly with iOS. Every time I go to LinkedIn they offer a premium feature where an LLM writes my posts for me (gross). And I don\u0026rsquo;t even need to detail how people use LLMs to constantly generate images of Miyazaki people doing things, often with 1 finger too few or too many (which is quite frankly hilarious).\nLLMs are EVERYWHERE.\nLLMs Are Grossly Misunderstood I see a lot of people talking about how they have \u0026ldquo;tuned\u0026rdquo; an LLM to their writing style, ways of thinking, or even to be their pal. How the LLM has \u0026ldquo;learned\u0026rdquo; to communicate more effectively with them from their writing. And of course, naturally, how the LLMs are going to take all of the Junior level jobs - especially in the field of programming!\nBut it\u0026rsquo;s clear to me that most people don\u0026rsquo;t understand how LLMs actually work. So let\u0026rsquo;s walk through it together. Buckle up.\n0. LLMs Are Advanced Neural Networks Neural networks are complicated. They\u0026rsquo;ve been around for a long time - well before the rise of LLMs. The simplest way I can think of to explain them is to use an analogy of walls of doors:\nImagine you are standing in front of a wall of 100 doors. You can only go through 1 of these doors. The door you go through is based on the first word of the last sentence you said or wrote. This door leads to only SOME of the next row of doors being available for you to open (your choices have blocked off some paths). Once you open the next door based on the second word of that sentence, the NEXT layer behind it reveals itself, again with only a subset of the doors available for you to open based on the doors you opened previously. Eventually you progress through all of the walls of doors and end up at a destination. Doing all of this takes some non-trivial amount of time, perhaps 10 seconds. In this analogy, the walls are the neural network layers and the doors are the neurons. In a classic neural network, a neuron is a graph node that receives input (from the prior layer that led to it) and generates output (the path to the next layer). Each neuron layer usually only connects to the layers immediately preceding and following it. The first layer is usually the \u0026ldquo;input\u0026rdquo; point, and the last layer is typically the \u0026ldquo;output\u0026rdquo; result.\nOne of the most successful and earliest examples of a Machine Learning Neural Network is the online implementation of the classic kids game \u0026ldquo;20 Questions\u0026rdquo;. It\u0026rsquo;s hosted on an old site at http://www.20q.net\r- feel free to play a round or two (if you feel like ignoring the scary HTTP insecure site errors)!\nA Diagram Depicting A Neural Network LLMs are a special kind of advanced neural network. They combine the concepts of a traditional neural network with bleeding-edge techniques such as \u0026ldquo;self-attention\u0026rdquo;, \u0026ldquo;attention heads\u0026rdquo; and \u0026ldquo;feed-forward networks\u0026rdquo;. LLMs define their depth with parameters rather than neurons and layers. These are not neurons, and they don\u0026rsquo;t map in a 1:1 fashion. Each parameter represents a trained set of weights and biases that make up the overall model behavior. To give you an idea of how large these LLM networks are, they often have 200 or even 500 billion parameters - and sometimes even more. They are MASSIVE.\n1. LLMs Don\u0026rsquo;t Understand English (or other languages) This is confusing right? You type English, and it sends English back, so how can it possibly not understand English?\nYou can say the same as above for any other language too.\nLLMs don\u0026rsquo;t speak in the languages we do; they only speak in tokens. And tokens are created by tokenizers.\nWhen you type something to your favorite LLM, it will first tokenize what you wrote. This process breaks your sentence into multiple tokens, which are the true \u0026ldquo;language\u0026rdquo; that LLMs speak. Classic tokenizers will tokenize sentences into individual words based on white space and punctuation. For example, if we sent this sentence:\nIt was the best of times, it was the worst\nA whitespace tokenizer might produce:\n['It', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst']\nHowever, LLMs do not operate on character or string tokens. For performance and memory efficiency reasons, they operate on numbers (the \u0026ldquo;bare metal\u0026rdquo; data format of computers). LLMs use number tokens, with a unique number assigned for each unique token.\nGPT-4 uses a Byte Pair Encoding (BPE) tokenizer called cl100k_base. This tokenizer processes text sequentially to produce numeric tokens based on a machine learning heuristic. ck100k_base would tokenize our sentence as:\n[2181, 574, 279, 1888, 315, 3115, 11, 433, 574, 279, 12047]\nThe total set of tokens known and defined by an LLM constitute its vocabulary. It works just like your vocabulary: you know the words you know, and you don\u0026rsquo;t know the words you don\u0026rsquo;t. Same for the LLM. It can only understand and reply to tokens it has in its vocabulary.\n2. LLMs Are Immutable (Read-Only) LLMs are \u0026ldquo;frozen in time\u0026rdquo; with the knowledge they were trained on. They do not learn. They feel like they learn, because they employ some tricks used to better pattern-match how you write or talk, but they are in fact read-only and not learning anything from your inputs.\nThis creates a few interesting challenges:\nAn LLM last updated on March 1, 2025 will have no knowledge of news or events that have happened after that date. We can\u0026rsquo;t just insert new facts, because the neural network generated by LLM training is incredibly tightly-coupled and interdependent, weighted and trained very carefully, and actually quite fragile. Adding something new might drastically change the model\u0026rsquo;s behavior. Therefore a new model must be released every so often, trained on the latest data, so that the LLM can respond to current events and stay \u0026ldquo;up to date\u0026rdquo;. Training models is very expensive and time consuming. Because the model consists of billions of neurons which have numeric values and connect in multiple ways to each other, there is no way to cite sources of data or provide reliable attribution. This is because results are \u0026ldquo;synthetically generated\u0026rdquo; as the sum of multiple original sources, and those sources are not included directly in the embeddings. Additionally, even if you could open an LLM up to learning from prompts, you probably don\u0026rsquo;t want to. As Microsoft learned back in the 2010s with the launch of a bot named Tay on Twitter, letting people on the Internet actively train your bot is usually a bad idea\r. So these things are also read-only so that no one runs into these problems.\nThe companies creating LLMs are smart. They have figured out some clever workarounds for the limitation of things being read-only.\nRAG One such workaround is what\u0026rsquo;s known as RAG or Retrieval-Augmented Generation\r. This was one of the earliest \u0026ldquo;hacks\u0026rdquo; everyone figured out, and it\u0026rsquo;s pretty useful. Essentially you send the LLM the data you want it to use, and ask it to summarize that data. And it will be able to do so assuming the data that you sent it exists within its vocabulary which it typically will (how often do we really invent new words in language?). This data can be from a source that\u0026rsquo;s more recent than the model\u0026rsquo;s read-only state (such as a blog post written today).\nThis approach is very useful for things like Google\u0026rsquo;s Gemini working off of search results. Essentially Google will retrieve the results for your search, then send Gemini a prompt that contains content from these results and ask it to summarize things for you.\nA very naive example (for learning purposes) would be as follows:\nYou search \u0026ldquo;how to get rid of groundhogs in my yard\u0026rdquo; (as I recently did, because I have one, and it\u0026rsquo;s a nuisance!)\nGoogle finds the results for that query, just like it used to before LLMs were a thing\nGoogle takes the CONTENT of the top n results (say 3 or 5), and generates a prompt for Gemini\nThe prompt is essentially (crudely, for learning purposes):\ngiven the context of X (the content from the top 3 or 5 results), summarize it in at most 5 paragraphs of no more than 4 sentences each. For each paragraph, cite the website as the source of your statement. Phrase your response as an answer to this statement: \u0026ldquo;how to get rid of groundhogs in my yard\u0026rdquo;\nGemini spits out some LLM content in response, and Google displays it to you above the search results\nAn Example Gemini Response for Google Search Long Prompts As Context Ever noticed how when you ask an LLM a question, it gives you a (usually detailed) answer, but then if you ask a follow-up without providing context, it will still know what you\u0026rsquo;re talking about and the conversation will flow naturally?\nFor example, ask it to write you a rhyming poem about cats. And then when it does, simply respond with \u0026ldquo;try again and make it funnier\u0026rdquo; (note the total lack of context defining \u0026ldquo;it\u0026rdquo;). And it\u0026rsquo;ll do it again!\nIf the LLM only considered each prompt you sent it by itself, it would have no idea what \u0026ldquo;it\u0026rdquo; was and would hallucinate wildly (or tell you it doesn\u0026rsquo;t know, depending on how it has been trained).\nWhat typically happens now is that whenever you send a prompt, the LLM front-end app will actually send a bunch of your most recent prompts and sometimes its prior answers ahead of your next prompt, to provide context. This allows the conversation to flow \u0026ldquo;naturally\u0026rdquo; and for you to feel like you\u0026rsquo;re talking to intelligence. This is also why you\u0026rsquo;ll get told that you\u0026rsquo;ve run out of space for a given conversation with an LLM, and need to start a new session or conversation. With all that context, you\u0026rsquo;ll eventually hit the token limits.\nThis is the method by which people \u0026ldquo;tune\u0026rdquo; their LLMs. But as we discussed prior, the LLM doesn\u0026rsquo;t actually learn anything. It\u0026rsquo;s a parlor trick - though a convincing and often useful one.\n3. LLMs Don\u0026rsquo;t Understand, They Just Guess Fundamentally, all LLMs do is just pattern match. They use sophisticated algorithms to do so (and this involves more advanced methods of tokenization). But the high-level process is:\nThe LLM converts your prompt into numeric tokens. It walks the giant LLM Neural Network with these tokens, in order, because order matters in language meaning. When it reaches the end of your prompt, it continues down the network by guessing the next sequential tokens using heuristics based on training data. The training data is typically the corpus of the entire public Internet, plus many copyrighted and even pirated works. Fun fact: the large number of formally written documents (which use em-dashes) used to train LLMs are why they often prefer to use em-dashes in generated text! It uses a decoder-only transformer to return these \u0026ldquo;guessed\u0026rdquo; numeric tokens back to conversational language (usually in whatever language you\u0026rsquo;re communicating in). It returns that human-readable language back to you. Confused? Let\u0026rsquo;s use a simple example. Say you give an LLM this sentence as your prompt:\nThe sky is\nThe LLM will turn that into numeric tokens, walk the graph of parameters, and then guess at the next parameters past your prompt to \u0026ldquo;complete\u0026rdquo; the statement.\nThe response could start with:\nblue (what most people would guess and thus have written in training data) grey (or gray) because sometimes it\u0026rsquo;s cloudy and rainy, and because we spell it differently in different parts of the world above us which is also true for most people a movie made in 2020\rwhich has probably also been written about The LLM can respond with any of these answers, or many others that you aren\u0026rsquo;t expecting as well. The most extreme of these \u0026ldquo;off-base\u0026rdquo; answers are what we call hallucinations.\nHallucinations are when the LLM, when guessing what comes next from its enormous amount of training data, gets it very \u0026ldquo;wrong\u0026rdquo; with respect to what you expected (because it lacks understanding), and then goes down a \u0026ldquo;path\u0026rdquo; in the neural network that seems nonsensical and completely off-topic.\nAs for how it picks what words and sentences come next for your prompt\u0026hellip;\n4. LLMs Are Random By Design Ever talked to a \u0026ldquo;dumb\u0026rdquo; robot that only answers yes and no? It gets pretty old, pretty fast. The novelty wears off real quick.\nGoing beyond binary bots, think of the Magic 8-Ball toy and its finite set of a dozen-ish answers. It\u0026rsquo;s random, it varies, but it\u0026rsquo;s not THAT random and so the novelty quickly fades. Outlook not so good.\nWhat if you talked to an LLM and it always gave you the exact same answer for the exact same question? That would be pretty boring too. It also wouldn\u0026rsquo;t feel smart, human, or conversational. You\u0026rsquo;d probably think \u0026ldquo;yeah this is definitely a bot\u0026rdquo;.\nBut the LLM creators don\u0026rsquo;t want you to see it for what it is: a bot with a sophisticated \u0026ldquo;guess the next words from the training data\u0026rdquo; algorithm.\nThey want you to see it as a thinking, feeling, learning, reasoning, problem-solving, decision-making solutions to all of your problems.\nAs a result, most LLMs implement a variable called temperature which usually ranges in value from 0 to 1. This variable defines how random the LLM replies are, balancing predictability with creativity (which is the key to making the whole thing feel like an actual thinking AI). A value of 0 will be almost fully deterministic - giving the exact same answer to the same question every time. A value of 1 will make it almost completely random, making the odds of you getting the same answer (or even a similar answer) much lower (but possible, because again it\u0026rsquo;s random).\nThe key mechanic to making LLMs feel human and smart is randomness. They are semi-random by design.\nThis becomes more fascinating the more you think about it, because it has serious implications - especially around work.\nLLMs Aren\u0026rsquo;t Replacing People Right now people are trying (and failing) to use LLMs to cut staff and save money by automating work:\nA Fortune article from May 18th 2025 reveals that a study looking at AI chatbots in 7,000 workplaces finds ‘no significant impact on earnings or recorded hours in any occupation’\rKlarna was perhaps the poster-child for experimenting with downsizing staff in favor of LLMs. It took less than 2 years for Klarna to regret this decision: Company Regrets Replacing All Those Pesky Human Workers With AI, Just Wants Its Humans Back, \u0026ldquo;What you end up having is lower quality.\u0026rdquo;\rNo businesses are seeing measurable revenue gains from LLMs, and the ones trying to replace people are failing and walking it back.\nWhy? If you\u0026rsquo;ve read this far, you already know the answers:\nLLMs are read-only (frozen in time). LLMs are random by design, and most business processes should not be random (especially programming and money stuff). LLMs don\u0026rsquo;t understand English (or any other human language). LLMs do not think, and are not capable of reasoning or logic. LLMs also don\u0026rsquo;t understand math (which programming and business processes are typically based on). What About Copilot, Claude, and Cursor? Spoken languages are non-deterministic. Coding languages are both Turing-complete\rand deterministic.\nYou can think of your favorite coding LLM as a \u0026ldquo;non-deterministic transpiler\u0026rdquo; that uses a guessing heuristic and a configurable level of randomness to generate some deterministic code.\nBut as logic dictates, a non-deterministic system cannot generate a deterministic result.\nNon-deterministic systems are characterized by unpredictability and inherent randomness, meaning that the same input can lead to different outputs. Determinism, on the other hand, implies a system where the output is entirely determined by the input, with no room for chance or variation.\nHumans aren\u0026rsquo;t always deterministic, but we have the ability to learn and reason. LLMs don\u0026rsquo;t. That\u0026rsquo;s why we can fix their code but they can\u0026rsquo;t reliably fix ours.\nAs developers, what we\u0026rsquo;re actually paid for is NOT our ability to generate code. It\u0026rsquo;s our ability to reason, learn, think critically, understand the context of the code around the code we\u0026rsquo;re writing, and to solve business problems as well as make presentations, draw diagrams, and present to others. LLMs can do NONE of that reliably.\nIf your next argument is \u0026ldquo;well we just need a few humans in the loop to supervise the outputs of the coding LLMs\u0026rdquo;\rthen I say to you: sure. And what do those humans need to know? All the stuff I wrote above, making programmers still important and valuable in organizations even if this unfortunate future were to occur.\nWhat About Agentic LLMs? Businesses and people generally discover that a single LLM cannot reliably solve their automation or process problems. Enter Agentic LLMs - systems of LLMs with software overseeing and orchestrating things and adding tools to try and produce better results!\nAgentic systems are basically just management and oversight software that attempts to orchestrate multiple LLMs at the same time with other tools in intelligent and cohesive ways. They take a prompt, talk to LLM A, then feed those results into B with some context for more results, and then pass that to a bash shell to execute a command, and so on to ideally create a \u0026ldquo;tool chain\u0026rdquo; that does what you want by automating a process for you.\nDo they work? Sure, for some definition of \u0026ldquo;work\u0026rdquo;. But fundamentally, these Agentic systems still use LLMs which are random by design and non-deterministic, and so any results you get from an Agentic system will be\u0026hellip; random and non-deterministic!\nLLMs Are Mechanical Turks Here\u0026rsquo;s the dirty secret no one wants you to know: these LLMs, when freshly trained on data, have such complicated neural networks that no one can test every possible output path. And because there are billions and billions of paths, there are probably a few that are problematic or not ideal. As LLMs have no real concept of bad and good topics (again they do not reason or understand language), they can sometimes produce very harmful outputs (like telling someone to harm themselves, or providing the recipe for home-made napalm).\nThe way they are made \u0026ldquo;good\u0026rdquo; is by a final \u0026ldquo;polishing\u0026rdquo; step in the training process which is called RLHF (Reinforcement Learning from Human Feedback). This process employs tens of thousands of people, often in off-shore countries, typically for a meager dollar rate (I heard $15 an hour once) to use their human brains and reasoning and critical thinking skills to \u0026ldquo;polish\u0026rdquo; the outputs of the LLM. The RLHF tuner will look at outputs that are not desired (like recipes for napalm) and reinforce learning manually back into the LLM to teach it never to pick that route through the neural network again.\nIn the same way that the Mechanical Turk had a human thinking and doing things under the guise of an automaton, LLMs have many thousands of humans fine-tuning the end results to be \u0026ldquo;safe\u0026rdquo; and \u0026ldquo;socially responsible\u0026rdquo;.\nAnd this is a never-ending cat-and-mouse game. Smart hackers find new and exciting ways to \u0026ldquo;jailbreak\u0026rdquo; the finely-tuned safeguards of an LLM, and then the RLHF crew patches that exploit, then the hackers find another one, and they patch that, and on and on the dance goes forever.\nThe secret ingredient is people.\nRead About Model Collapse Model Collapse\ris a complicated and important topic in the world of machine learning. It\u0026rsquo;s especially important in the context of LLMs.\nWhen OpenAI released ChatGPT 3 in late 2022, it was revolutionary. This was partially because nothing like it had ever hit the mainstream before. This was also because OpenAI had used a large part of the Internet to train the model, and up until that time, the Internet mostly consisted of human-written articles, blogs, and forum posts. So the quality and quantity of training data was exceptional.\nHave you noticed that almost all of the new models released since then have been underwhelming, offering diminishing returns?\nTraining an LLM is a lossy process, where data is not perfectly mapped 1:1 and some data entropy occurs as a result of the process of tokenization and neural network mapping and generation.\nSince that \u0026ldquo;initial\u0026rdquo; big release, a lot of people have used LLMs to generate content for things like their websites, social media and forum posts, Reddit, LinkedIn - you name it. And the smart people at the LLM companies realized that training new models on their (or other models\u0026rsquo;) outputs compounds errors and makes them essentially \u0026ldquo;dumber\u0026rdquo;.\nThe LLM creators had poisoned their well of training data by enabling LLM generated content to saturate the Internet.\nTo continue to make solid progress, they realized they needed trustworthy (human-created) data to train on. They tried to solve this in a few ways.\nDetecting LLM-Generated Content The first attempt to solve this problem was to detect content that was generated by LLMs rather than humans. This facade was kept up for about a year before OpenAI finally admitted that no detectors \u0026ldquo;reliably distinguish between AI-generated and human-generated content.\u0026rdquo;\r.\nTheir source of data (the Internet) was now \u0026ldquo;polluted\u0026rdquo; with a significant amount of LLM-generated content, and they had no way to know what to train on and what to disregard and avoid. So now the LLM companies have the problem that training a newer version of their model on the \u0026ldquo;latest\u0026rdquo; Internet data means that they are consuming their own outputs, compounding errors and entropy, and making their models dumber.\nSynthetic Data Another approach that LLM companies tried was the concept of Synthetic Data. This is data that is generated as if a human wrote it, for the LLM to train on.\nI predict that you\u0026rsquo;ll hear urgently increasing talk about this idea in the coming months.\nMake no illusions: synthetic data doesn\u0026rsquo;t exist and it isn\u0026rsquo;t a real thing that we can do.\nIf it were real, our problems would be solved. The LLMs would just train on suitable synthetic data when creating new models, and they would continue to get smarter (or at least not get dumber).\nBut it\u0026rsquo;s not real. Because there are currently only 2 ways to generate training data for LLMs:\nHumans create data for the LLM to consume and train on (this is \u0026ldquo;good\u0026rdquo;) Machine (like LLMs) generate synthetic data for LLMs to consume (this is \u0026ldquo;bad\u0026rdquo; and causes Model Collapse) Data Partnerships Some LLM companies are striking deals with popular forums and websites to gain licensed access to their content and data for training purposes. Sites like Reddit and Stack Overflow surely participate in these arrangements, and not-so-coincidentally have adopted \u0026ldquo;no bot content allowed\u0026rdquo; policies for users as well. The problem is that, because we cannot detect LLM-generated content, this data is not as valuable or trustworthy as one would like it to be. As a result these partnerships, in my opinion, are most likely ephemeral and little more than wishful thinking.\nPrediction: LLMs Will Only Get Dumber I\u0026rsquo;m calling it right now, and I pledge to never edit this statement out of this blog post in the future:\nModel collapse has begun, and LLMs will only get dumber from this date forward.\nHopefully this will lead to the AI hype bubble finally bursting. But what do I know? Investors continue to pour billions of dollars into AI startups, despite the reality that currently:\nNo company has proven that they\u0026rsquo;re saving money and succeeding by replacing workers with LLMs. Even if a company COULD replace developers with LLMs successfully, they will still need skilled developers to oversee and correct the outputs, which necessitates the continuing software developer career path. LLM companies like OpenAI are not profitable. In 2024, AI companies nabbed 45 percent of all US venture capital tech investments, up from only nine percent in 2022.\rTime\rand time\rand time again,\rexperiments in having LLMs write code begin to fail as soon as the code moves beyond basic examples and use cases. In any case, LLMs are not the path to AGI.\rUse Your Head The set of differences between you and an LLM are vast, but most potently rooted in what makes us human: critical thinking, learning, problem solving, and reasoning skills. I implore you to keep sharpening and building these skills.\nDon\u0026rsquo;t outsource thinking to a system that cannot think, even if the temptation is strong and everybody else seems to be doing it. It might seem easy right now, especially in low-accountability work environments, but you\u0026rsquo;re only hurting yourself in the long-run.\nAs the saying goes: \u0026ldquo;if you don\u0026rsquo;t use it, you lose it\u0026rdquo;. If you don\u0026rsquo;t continuously sharpen your skills, they fade.\nAnd if your only skill at work is making LLMs spit out code or generic emails full of em-dashes, then they actually can (and probably will) replace you.\nDavid Haney is the creator of CodeSession\r- a platform that helps companies do collaborative coding interviews AND avoid LLM cheating. None of this blog post was written by LLMs.\nSpecial thanks to my good friend balpha\rfor his invaluable efforts in proof-reading this post.\n","permalink":"https://www.davidhaney.io/llms-will-not-replace-you/","tags":["llm","ai","snake oil","hype cycle"],"title":"LLMs Will Not Replace You"},{"categories":["blog"],"content":"Have you heard the news?\nAI is replacing all developers next week - or in 3 or 6 or 12 or 18 months! And somehow at the same time, AI is helping developers get jobs by \u0026ldquo;cheating\u0026rdquo; in tech interviews! Don\u0026rsquo;t those two things directly oppose and contradict each other? But I digress.\nThe thing is, AI replacing developers is straight up impossible, at least in the currently deployed LLM architectures. The idea is nonsensical. I need to write a blog post detailing both why it\u0026rsquo;s impossible and why people continue to shout it from the rooftops, and I think I\u0026rsquo;ll do that next.\nBut right now I want to talk about the second thing - the more interesting and real thing. In case you haven\u0026rsquo;t caught the news, there has been a massive surge of developers cheating in tech interviews by using LLMs to beat LeetCode.\nDevelopers Are Cheating In Interviews Here are just a few of the many recent articles on the subject:\nCNBC: Meet the 21-year-old helping coders use AI to cheat in Google and other tech job interviews\rThe problem has become so prevalent that Pichai suggested during a Google town hall in February that his hiring managers consider returning to in-person job interviews.\nEntrepreneur: Job Seekers Are Getting Increasingly Bold By \u0026lsquo;Cheating\u0026rsquo; in Interviews — and AI Is Making It Worse\r\u0026ldquo;A lot of the efforts to cheat come from the fact that hiring is so broken. So you\u0026rsquo;re just like, \u0026lsquo;Oh, my God, how do I get through? How do I get seen? How to get assessed fairly?\u0026rsquo;\u0026rdquo; Lindsey Zuloaga, the chief data scientist at HireVue, told BI.\nMSN: Is it cheating? AI use during job interviews sparks debate over whether to restrict emerging tools\r\u0026ldquo;A tech leader recently told me they suspect that 80% of their candidates use LLMs on top-of-funnel code tests — despite being explicitly told not to,\u0026rdquo; said Jeff Spector, co-founder and president of Karat, a Seattle startup that helps companies conduct technical interviews.\nHere\u0026rsquo;s the thing: I don\u0026rsquo;t blame \u0026rsquo;em one bit. As Ice-T says: don\u0026rsquo;t hate the player, hate the game - and the game of tech interviewing has been broken for years.\nWe\u0026rsquo;re taking smart people, giving them generic, well-known, completely solved academic algorithm questions for interviews, and then acting shocked when they use an LLM that has literally been trained on those questions to answer them expertly. What else could the outcome have been really?\nLeetCode Interviews Suck How many of us have done a coding interview where the question was \u0026ldquo;write an algorithm to recursively delete a node in a balanced binary search tree\u0026rdquo; but then the job description was like \u0026ldquo;in this HTML-forms-over-database-tables clone-stamping job, you\u0026rsquo;ll be stamping exciting new clones of database tables represented by HTML forms!\u0026rdquo; I sure have; I\u0026rsquo;ve had some crazy interviewing experiences actually:\nI had a hiring manager tell me he needed to confiscate my phone so I didn\u0026rsquo;t cheat. When I refused to hand him my phone, he handed me a pad of paper and a pen (not even a pencil!) and asked me to write a recursive balanced binary search tree insert - on paper - in pen. I\u0026rsquo;d bet you 100 dollarbucks that guy had never written a recursive method in his entire career.\nI had an interview where the \u0026ldquo;super hard\u0026rdquo; tech challenge question was a one-liner: \u0026ldquo;implement Huffman Encoding\u0026rdquo;. I had to freaking look it up (and I told them I was doing so). You know why I had to look it up? Because I\u0026rsquo;ve never used it. No one has ever used it. Of every developer I\u0026rsquo;ve ever met in my entire life, not one of them has ever written a Huffman Encoding algorithm in anything ever. Because it\u0026rsquo;s a solved problem.\nI got asked if I knew what the =\u0026gt; symbol meant. No specific programming language specified. Dude just stood up and drew it on a whiteboard in front of me. I said the meaning depended on the programming language. The guy rolled his eyes at me and walked out of the room. Ironically I got the job.\nAsk yourself this: when was the last time you wrote a binary search tree at work? Or used recursion for a practical reason (like graph traversal)? On that note, when was the last time you wrote a graph? Or used a linked list? (Ignore that last one, crypto crew).\nI\u0026rsquo;ve worked at jobs where we actually had reason to do some of these things, and even then I\u0026rsquo;d say it accounted for less than 1% of our dev time. I remember writing a custom Quicksort with 3-way partition, and still not being able to beat the native QuickSort implementation in .NET because it cheated and invoked kernel level functions under the hood that I couldn\u0026rsquo;t easily or safely access myself. But we didn\u0026rsquo;t ask about this stuff in interviews, because it was such a tiny part of the overall job.\nSo why do we, as an industry and community, continue to ask people interview questions that have nothing to do with the job?\nHow did we reach a point where every technical screener is another bloody LeetCode test asking me to solve prefix sum or sliding window or DFS or BFS or whatever other obscure and mostly unused algorithmic concept?\nI get that there are jobs out there that actually need academic algorithm expertise. For example, data scientists need to understand top k and such. But really, 90% of tech companies are using these LeetCode questions to hire while maybe 10% (if we\u0026rsquo;re being generous in our estimates) are actually doing this academic stuff every day, or even weekly. The rest are just cargo-culting. Like that time in the 2000s when every tech interviewer was asking why manhole covers are round and how many quarters you\u0026rsquo;d need to stack to reach the moon (thanks for that Google, so great).\nThese academic algorithms are pure compsci. They have value. They solve problems. But I\u0026rsquo;m saying these problems are mostly academic - and the practical, non-academic tech teams don\u0026rsquo;t deal in this stuff at all. We all just clone stamp HTML forms over database tables (see above). And sometimes we send that form data other places like queues and other kinds of databases. We do the practical stuff that makes a business go. No executive is ever going to kick open the board room doors in a panic, yelling \u0026ldquo;EVERYONE! WE NEED TO IMPLEMENT A RECURSIVE BINARY SEARCH RIGHT FREAKING NOW OR THIS COMPANY IS BANKRUPT!\u0026rdquo; That reality just doesn\u0026rsquo;t exist.\nLLMs Train on LeetCode Algorithms None of this academic stuff happens at a typical company. But you know where it does happen? Academic papers, books, and work. And getting to the juicier part of this topic, you wanna guess what LLMs are trained on? Academic papers, books, and work! Except when that stuff is copyrighted, because LLM companies totally don\u0026rsquo;t plagiarize things to train their models. In the immortal words of Homer Simpson: \u0026ldquo;oh by the way, I was being sarcastic.\u0026rdquo;\nLLMs also train on social media site data. Like that LeetCode subreddit\rwhere all the LeetCode takers talk to each other about LeetCode problems and solutions.\nSo in many (literal, actual) ways, LLMs have actually been directly trained to beat LeetCode interviews. And eventually people realized that LLMs were trained on these well-known algorithms, and started employing them to beat LeetCode interviews. And the thing is - it works, at least well enough that companies are now saying that they need to move their technical interview processes back on-site in order to thwart this \u0026ldquo;cheating\u0026rdquo;.\nHow Cheating Happens There are a few key conditions that allow candidates to cheat on the tech interviews:\nWhen the interview question is academic or well-known. LLMs are trained on public data sets and written works. Fun dystopian fact: at this point they\u0026rsquo;ve pretty much consumed the collective knowledge of the entire Internet. And so if a question is well-known in the public domain, then it has been discussed a lot on the internet and / or in written works, and that means the LLM has been well-trained on it and can answer is super effectively. When the technical screening is automated and involves no human interaction. Often candidates are given async \u0026ldquo;take home\u0026rdquo; tests to do, with companies reassured that the testing software or platform claims to have cutting-edge anti-cheat capabilities. But the thing is, the cheaters are always one step ahead of the current anti-cheat and writing new methods to thwart it, and so that \u0026ldquo;bigger-mouse, better-mousetrap\u0026rdquo; dance goes on eternally and you can\u0026rsquo;t be sure the candidate didn\u0026rsquo;t cheat. When an interviewer doesn\u0026rsquo;t ask follow-up questions. LLMs are good at producting high-level, generic answers to many questions. But if you ask a candidate to walk you through their code, or even explain it line by line, you\u0026rsquo;ll pretty quickly be able to tell who wrote the code and who had an LLM write it for them. LLMs produce answers without understanding. Programmers produce well-understood answers.\nDo Collaborative Interviews Tech companies don\u0026rsquo;t need to bring technical interviews back on-site. Flying candidates out for in-person sessions is expensive and time-consuming. But relying on automated, academic-style assessments isn\u0026rsquo;t a viable alternative either. LLMs have effectively solved these problems, making them unreliable for evaluating real developer skills.\nThe key to effective technical interviews is balancing structured assessment with human interaction. Interviews should be collaborative, interactive, and designed to simulate real-world problem-solving.\nA solid technical interview process should involve:\nTailored questions that reflect the actual work a candidate will be doing, rather than relying on well-known algorithm puzzles. Live, interactive coding sessions where candidates and interviewers work together, much like they would on the job. Follow-up discussions to ensure candidates truly understand their solutions, rather than relying on generated answers. One of the simplest ways to detect AI generated code is to ask the candidate to explain their solution. No one can fake understanding. Someone who wrote the code themselves will easily walk through their logic. In contrast, candidates who copy from an AI tool often hesitate, struggle with explanations, or fail to make necessary modifications. Take note of how they respond, and trust your gut if it feels like they\u0026rsquo;re cheating - if you believe the news, they probably are.\nIn the new AI era of tools and assistants, companies should embrace structured, human-driven coding interviews. When interviews focus on genuine collaboration and problem-solving, both candidates and hiring teams get better outcomes.\nDavid Haney is the creator of CodeSession\r, a platform that helps companies do collaborative coding interviews.\n","permalink":"https://www.davidhaney.io/the-tech-interview-ai-cheating-epidemic/","tags":["ai","interviews","codesession"],"title":"The Tech Interview AI Cheating Epidemic"},{"categories":["blog"],"content":"Recently in CodeSession\rI encountered the following logged warning:\nNo XML encryptor configured. Key {6848a46f-d0d6-49a6-b035-0f30f5448f9d} may be persisted to storage in unencrypted form.\nThis was totally new to me, so I started Googling. There I learned all about .NET Core\u0026rsquo;s Data Protection\rAPI.\nAfter reading the MS docs, this was still confusing. The documentation states that this API is used for things like .NET Authorization, but CodeSession doesn\u0026rsquo;t use that functionality. So why was this happening?\n.NET Core MVC XSRF Request Validation I eventually figured out that .NET Core MVC uses Data Protection under the hood if you\u0026rsquo;re employing Request Validation code to protect forms\r.\nCode in my Views:\n1 2 3 \u0026lt;form id=\u0026#34;register-form\u0026#34; method=\u0026#34;post\u0026#34; action=\u0026#34;/register\u0026#34; autocomplete=\u0026#34;on\u0026#34;\u0026gt; @Html.AntiForgeryToken() ... Code in my Controllers:\n1 2 [HttpPost(\u0026#34;register\u0026#34;), ValidateAntiForgeryToken] public async Task\u0026lt;ActionResult\u0026gt; Register(RegisterModel model) The reason for the logged warning that I got in the first place is that Data Protection must be configured to persist keys in a shared place when using 2+ instances of an app. From the MS docs:\nThe data protection stack must be configured to work in a server farm\nWhen the Data Protection system is initialized, it applies default settings based on the operational environment. These settings are appropriate for apps running on a single machine. However, there are cases where a developer may want to change the default settings:\nThe app is spread across multiple machines. For compliance reasons. For these scenarios, the Data Protection system offers a rich configuration API.\nAll n instances of your app must all share the same cryptographic keys, in order to produce the same results.\nWhat Happens If You Don\u0026rsquo;t Configure Data Protection When left unconfigured, Data Protection generates and stores a local encryption key based on local machine and environment settings. This is a problem for an app with more than 1 instance behind a load balancer, because each instance of the app will generate its own unique key that is different. And if requests to your app randomly bounce between instances behind a load balancer, the key generated on machine 1 will not match the key generated on machine 2, and you\u0026rsquo;ll get all kinds of fun cryptography related exceptions + your users will get error pages.\nI needed to solve this for CodeSession because run on more than 1 instance. I host in Azure, so once more I went to Google to figure out how to configure the Data Protection API for Azure. Unfortunately it turns out that the documentation is a little out of date, a bit unintuitive, and in some cases straight up wrong. So here\u0026rsquo;s how to configure Data Protection in Azure quickly and correctly.\nData Protection in Azure For \u0026lt;$0.05 A Month 1. Get The NuGet Packages The first step is to install a few NuGet packages to enable configuration:\nAzure.Extensions.AspNetCore.DataProtection.Blobs Azure.Extensions.AspNetCore.DataProtection.Keys 2. Configure Azure Resources Next you need to do most of the work in Azure. We\u0026rsquo;ll need a Key Vault, a Storage Blob Container, and an Azure Managed Identity to get this done.\nKey Vault Create an Azure Key Vault using the cheapest settings possible (or re-use one you already have). You won\u0026rsquo;t need the Certificate functionality that is offered, just the ability to store Keys.\nImportant note: your app will need to be able to access your Key Vault, so if you haven\u0026rsquo;t set up Virtual Networks, you\u0026rsquo;ll need to enable public access. Go to Settings \u0026gt; Networking and set \u0026ldquo;Allow public access from all networks\u0026rdquo; to enable. It sounds scary, but it just exposes an endpoint to the Internet - people still need authenticated accounts to access anything at all. Still, I\u0026rsquo;d recommend setting up a Virtual Network as the most secure option.\nOnce that\u0026rsquo;s provisioned, go to Objects \u0026gt; Keys and click the Generate/Import button. Give it a name that you\u0026rsquo;ll remember (I called mine \u0026ldquo;dataprotection\u0026rdquo;) and all the other defaults are totally fine:\nData Protection Vault Key Press Create. All done with this step.\nStorage Container Create an Azure Storage Account using the cheapest settings available (or re-use one you already have).\nImportant note: your app will need to be able to access your Storage Account Container, so if you haven\u0026rsquo;t set up Virtual Networks, you\u0026rsquo;ll need to enable public access. Go to Security + Networking \u0026gt; Networking and set Public network access to \u0026ldquo;Enabled from all networks\u0026rdquo; to enable. It sounds scary, but it just exposes an endpoint to the Internet - people still need authenticated accounts to access anything at all. Still, I\u0026rsquo;d recommend setting up a Virtual Network as the most secure option.\nNext go to Data Storage \u0026gt; Containers and create a new container where you\u0026rsquo;ll store your encrypted keys. I called mine \u0026ldquo;dataprotection\u0026rdquo; here also:\nData Protection Storage Container Once created, you\u0026rsquo;re all done here.\nAzure RBAC Managed Identity This is perhaps the trickiest part. Finally you need to set up a Managed Identity that can access these new resources from your application.\nFrom Azure, go to Managed Identities. Create a new identity with a useful name - I called mine \u0026ldquo;dataprotection-access\u0026rdquo;.\nData Protection Managed Identities Once provisioned, we\u0026rsquo;ll need to give this identity specific roles. Go to Azure role assignments within the identity and give it:\nKey Vault Crypto User role on your Key Vault Storage Blob Data Contributor role on your Storage Container. These are the least privileged roles needed to make Data Protection work. Once configured your role assignments will look like this:\nData Protection Managed Identity Roles Just one last thing to do in Azure. Your app needs to be able to assume this new Managed Identity, thus gaining the access it provides.\nGo to your app (mine is a Web App), then go to Settings \u0026gt; Identity. Here, click the User assigned tab at the top, click add, then select your new Managed Identity. When configured it\u0026rsquo;ll look like this:\nData Protection App User Assigned Identity OK! You made it! We\u0026rsquo;re done with all the Azure stuff, so finally it\u0026rsquo;s time to write a few lines of code.\n3. Configure Your .NET Core App This is the easy part. In your Program.cs file, add the following code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // Data Protection - used for XSRF form antiforgery tokens, must be hosted in a shared resource (Azure) so that n instances of this app can use it // See: https://learn.microsoft.com/en-us/aspnet/core/security/data-protection/configuration/overview?view=aspnetcore-8.0 // For this to work: // - An Azure Managed Identity must be created with RBAC \u0026#34;Storage Blob Data Contributor\u0026#34; access for the Storage account, and \u0026#34;Key Vault Crypto User\u0026#34; access for the Key Vault // - The AppService hosting this app must have this Managed Identity \u0026#34;User Assigned\u0026#34; via the Identity --\u0026gt; User Assigned Azure view { // Get the Client ID from the Managed Identity Overview tab in Azure var azureCredential = new ManagedIdentityCredential(\u0026lt;managed_identity_client_id_here\u0026gt;); services.AddDataProtection() // Must be same name for all instances .SetApplicationName(\u0026#34;CodeSession\u0026#34;) // Requires AppService to have identity RBAC assigned to blob storage access .PersistKeysToAzureBlobStorage(new Uri(\u0026#34;https://\u0026lt;your_storage_name\u0026gt;.blob.core.windows.net/dataprotection/keys.xml\u0026#34;), azureCredential) // Requires AppService to have identity RBAC assigned to key vault access .ProtectKeysWithAzureKeyVault(new Uri(\u0026#34;https://\u0026lt;your_vault_name\u0026gt;.vault.azure.net/keys/dataprotection/\u0026#34;), azureCredential); } Note that we pointed to a keys.xml blob file in the container that doesn\u0026rsquo;t yet exist. Don\u0026rsquo;t worry - the API will create it for you automatically.\nNext, run your app. If you followed all these steps correctly, everything should now work properly! You can also look for and inspect the newly created keys.xml file in your container within Azure Portal to verify.\nA final note: I recommend putting this configuration logic behind an if (localDev) check / branch once you\u0026rsquo;ve verified that it\u0026rsquo;s working. You don\u0026rsquo;t need to run all this or talk to Azure for local dev purposes. It just needs to work when deployed.\nHopefully this helps other people sort out the Data Protection configuration - it ate up a good 3 or 4 hours of my time last week. Cheers!\n","permalink":"https://www.davidhaney.io/configuring-net-core-data-protection-for-azure/","tags":["netcore","azure","dataprotection"],"title":"Configuring .NET Core Data Protection For Azure"},{"categories":["blog"],"content":"It\u0026rsquo;s amazing how quickly 4 years can go by. But I\u0026rsquo;m back to blogging, and promise to follow this post with more regular entries going forward. In fact, I already have a technical topic scheduled for later this week, and plan to return to Engineering Management topics in the coming weeks as well.\nSo what have I been doing since my last blog post over 4 years ago? LOTS. Let\u0026rsquo;s talk about it.\nMy Time At Stack Overflow I worked at Stack Overflow\rfrom 2014 through late 2023. It has easily been the highlight of my career to-date. I had the fortune and privilege of working with and managing some of the smartest developers in the world. We accomplished so many amazing things together at incredible scale.\nA quick timeline of notable events in the past 4 years:\nJuly 2020: we raised an 85 million dollar Series E to accelerate our SaaS business\r(my org owned Teams, the SaaS) June 2021: Prosus acquired Stack Overflow for 1.8 billion dollars\r(this did not suck for me) Jan 2023: Stack Overflow started pivoting towards AI (eventually named OverflowAI) after fully realizing the level of disruption to our sites and traffic May 2023: Upon return from a leave, I became technical lead on OverflowAI efforts, working closely with Prosus and the C-Suite as well as our Eng teams Oct 2023: Stack Overflow laid of 28% of staff\rafter rapidly doubling the company size and having our big bets disrupted by AI Nov 2023: I departed Stack Overflow (voluntarily with the layoffs) Why Did I Leave Stack? In short: it was simply time for me to go. It made sense (and still does in hindsight). I had been there 9+ years, seen and done a ton, reached my potential at the company, and there wasn\u0026rsquo;t much left for me to do. To be honest, I was becoming bored in some respects. 9 years is a long time.\nWhat I\u0026rsquo;ve realized over my career so far is that I do best in small-to-medium sized companies (think a few hundred employees max). Conversely, I go insane in larger organizations due to the heightened bureaucracy and politics. Core to my self worth is the need to get sh#t done (GSD) to feel productive and happy.\nI have found anecdotally that there is a strong correlation between the size of an organization and its productivity. In general, as an organization grows in size:\nthe development velocity and productivity slows way down the meetings and discussions factor grows exponentially (each new person adds n+1 peer-to-peer communication channels) the more politics and bureaucracy play into accomplishing useful goals (risk tolerance levels, bonus incentives for execs, etc.) There was a time at Stack Overflow where 2 devs worked for 2 or 3 months to produce a great new product feature that was immediately deployed for the community\u0026rsquo;s benefit. We launched many of these projects over the years: our \u0026ldquo;DevKinds\u0026rdquo; ML project that classified Stack users, our Tag Engine, our Bonfire Chat application, and others. I built Stack Snippets in 2014 in just a few months by myself as a new hire\r, and it\u0026rsquo;s still widely used on the site to this day.\nStack was amazing for a long time, but by mid 2022 it had become too large for my tastes and work needs. I was going to well over 35 hours of meetings a week (often triple or even quadruple booked for the same time slots) which left me with maybe 5 hours a week to GSD. It felt to me as if the pace of projects had become glacial; meaningful work was replaced with dozens of meetings with multiple stakeholders (who weren\u0026rsquo;t always aligned), numerous \u0026ldquo;hallway conversations\u0026rdquo; (virtually), repeated status updates, and more Slack pings than anyone could ever possibly read in a work day. That kind of environment isn\u0026rsquo;t what makes me happy, so naturally I had started thinking about moving on.\nTo be clear, I don\u0026rsquo;t think these characteristics are uniquely a Stack Overflow thing or dysfunction. I think that this is the natural outcome of most companies that grow to a larger size, and also why I don\u0026rsquo;t do well in large companies.\nSo when the C-Suite came to me (and others) in late September 2023 asking for names for another large round of layoffs, I countered by discussing how many devs who were excited to be there I could save by leaving in their place. After many conversations with Jody, my boss and CTO (who was terrific about everything), we agreed that it made sense for me to depart. I got a solid exit package - a soft landing of sorts - which suited my purposes at that time in my life. We had literally just moved from Florida to Michigan in Oct 2023 and I really wanted some time off to get settled in the new city and state, as well as do some home renovation projects. The timing could not have been more perfect for me, all things considered.\nStack Overflow still employs many incredibly smart and talented people, and I look back on my time there with a sense of happy nostalgia. It was a helluva good run. I\u0026rsquo;d do it all over again without hesitation.\nWhat Have I Done Since Stack? After announcing my departure from Stack on LinkedIn, I was almost immediately courted by a Series C tech company. They build a really cool nextgen iteration of an existing data processing tech that efficiently uses all CPU cores and is in general a better product.\nI eventually accepted an offer and joined the company in Feb of 2024 - departing shortly thereafter.\nI made a few mistakes in accepting that job, the most important of which was letting my ego lead the conversation and decision making process. I liked being pursued by a company - it was flattering and made me feel important. It made me ignore many red flags:\nOver TWENTY pre-offer interviews and meetings. Not meeting any of my would-be team members during any of those meetings. Obvious morale issues with the folks I did meet during the process. Having the role and job title completely changed mid-interview process to an entirely different function. Being asked to do a homework assignment that wasn\u0026rsquo;t well defined and then presenting the wrong results to the group (partly because I didn\u0026rsquo;t clarify things correctly, and partly because they didn\u0026rsquo;t specify things well). To an objective eye, it was clear that this technical organization was mismanaged and extremely disorganized. But again, my ego led the way. So when I finally started there and the full weight of my mistake hit me, I threw all of my energy into quickly making useful and impactful decisions in my role to positively influence the low morale and productivity on my teams. Unfortunately I was stonewalled at every turn by large company dysfunction and bureaucracy, all while being berated by my boss for not getting useful things done. I felt completely set up for failure, so I pushed the eject button quickly (you should never linger in your mistakes in life - it\u0026rsquo;s not healthy).\nI won\u0026rsquo;t be letting my ego do the driving again anytime soon.\nAfter that short stint, I wanted to take a break from management and refresh my technical skills. After all, it had been over 9 years since I programmed full-time and I believe that the best technical leaders follow the \u0026ldquo;Engineer/Manager Pendulum\u0026rdquo; career path\r. By refining and refreshing my tech skills, I could be a better developer and technical leader going forward.\nI had a recruiter pal ping me asking if I knew of any .NET devs who would be interested in a small FinTech startup. A few discussions and one well-organized interview process later, I joined Cartwheel\rwhich does Accounts Receivable (AR) automation for various staffing organizations.\nI did 6 months or so at Cartwheel and dove head first into learning new things like .NET Core, Docker, Kubernetes, Helm, and other hosting providers like DigitalOcean and CloudFlare. It was a great place to polish my tech skills and a small enough company that I could quickly make a large positive impact. I optimized our CI/CD pipelines to turn 90 minute builds into 5 minutes or less. I also consolidated 16 projects (and accompanying individual Git repos) into a single monorepo project that could be built multiple ways in parallel. This was a huge improvement because the individual repos were all tightly coupled by dependencies on each other, yet independently versioned and released, which put us deeply into \u0026ldquo;DLL hell\u0026rdquo; territory and made debugging in non-local environments absolutely terrible.\nI\u0026rsquo;d probably still be working at Cartwheel today, however I left last week because a project I started 4 years ago is rapidly gaining traction, and I need to dedicate time and effort to seizing the opportunity in front of me right now.\nToday: Full-Time CodeSession CodeSession\ris a project I started in 2020 while working at Stack Overflow. I built it entirely on my own time and personal hardware outside of work. But I built it to solve a problem that we were having with our technical interview process at Stack Overflow.\nAnyone who has interviewed at Stack Overflow over the years knows that we like to do \u0026ldquo;peer programming\u0026rdquo; interviews. We get you in a (virtual) room with a developer or technical manager, present a technical problem that is relevant to the work we do regularly at Stack, and see how you go about solving it.\nWe had been using Google Docs with monospace fonts as an \u0026ldquo;IDE\u0026rdquo; of sorts for collaborative code editing, but this clearly wasn\u0026rsquo;t working super well. Google Docs is a word processor: it had no coding features like auto-indentation or closing of parentheses, it tries to capitalize every new line as a new sentence, and so forth. We were effectively jamming a square peg into a round hole in trying to make a word processor into a coding IDE. As a result it was not a great interviewing experience for us or the candidates.\nWe started to shop around for companies that could offer programming interview tools. We found that ALL of them were A) expensive and 2) just provided a large bank of generic LeetCode-style compsci questions that tested candidates on theoretical \u0026ldquo;clean room\u0026rdquo; programming skills and knowledge. When\u0026rsquo;s the last time that you reversed a linked list at work?\nStack\u0026rsquo;s hiring process deliberately DOESN\u0026rsquo;T do LeetCode-style questions because we found that they don\u0026rsquo;t correlate reliably to a developer\u0026rsquo;s ability to do the job. Reversing linked lists and traversing graphs are definitely compsci skills, but what good is testing devs on these mostly theoretical algorithms and then having them do totally different work in their day to day on the job (mostly in the form of stamping HTML over database tables with CRUD operations with consideration for large scale traffic)?\nGiven we didn\u0026rsquo;t find any good product offerings or options for our use case, I saw an opportunity in the market and went about prototyping a new product.\nI started writing CodeSession (which at the time was called CodeInterview). For once in my life I did things \u0026ldquo;right\u0026rdquo; in building a tech product MVP:\nI did the simplest, dumbest, ugliest thing I could - as fast as I could - to test the market and prove my product value I acknowledged mistakes and fixed them without ego or taking it personally I got real people using it ASAP I constantly solicited and listened to feedback from the devs at Stack as they piloted it For the first 12 or 18 months, basically only Stack used CodeSession. I built new features as devs requested them, and iterated on the core concepts of the product to make things better.\nBut then I began to think: if Stack finds value in this product, I bet other companies do as well! So I opened it up to public use in 2022 with a rebranding to CodeSession (since there was another similar product called CodeInterview already out there, and name collisions are bad for marketing and discoverability).\nIt\u0026rsquo;s been about two years since opening CodeSession up to the public, but today it has 227,586 lines of code written in 4,315 code sessions - and it\u0026rsquo;s growing a ton every single week. I have a handful of paying customers who get value from the product. I\u0026rsquo;m cash-flow positive and bootstrapped.\nToday the IDE experience looks like this:\nCodeSession IDE Experience As the feds cut the prime rate and January inches ever-closer with the promise of a fresh set of annual hiring budgets at tech companies, my window of opportunity becomes strikingly obvious (and limited).\nSo as of last week I\u0026rsquo;ve quit my job at Cartwheel to work on CodeSession full-time for at least the next few months. It\u0026rsquo;s honestly really exciting - and maybe a little bit scary because I am not collecting a consistent paycheck. I don\u0026rsquo;t think this will ever be the next billion dollar tech company, but if I can create and operate my own product that funds my modest lifestyle and goals, that would be pretty great.\nSo that\u0026rsquo;s the bet I\u0026rsquo;m currently on, and I\u0026rsquo;ll be blogging about it as I go. I\u0026rsquo;ll be blogging about more technical .NET topics and Engineering Management concepts as well.\nAnd finally, for those of you who have read this far: a short sales pitch. If you interview developers, why not give CodeSession\ra try? There\u0026rsquo;s a Free Forever plan where you can test it out with no credit card or obligation at all.\n","permalink":"https://www.davidhaney.io/leaving-stack-overflow-and-building-codesession/","tags":["startup","codesession","stackoverflow"],"title":"Leaving Stack Overflow and Building CodeSession"},{"categories":null,"content":"Hi, I\u0026rsquo;m David Haney. I currently live in the Detroit Metropolitan Area of Michigan with my family. We moved here in late 2023. Prior to Michigan, I lived in Florida for 14 years. Prior to that, I was born and raised in Canada.\nI have been programming most of my life. We didn\u0026rsquo;t have a lot of disposable income, but my dad was given a home computer for his day job which was quite the luxury. It was a Macintosh SE/30\rwith an 80 MB HDD. I can still remember all the unique and quirkly sounds it made. Fun fact: to this day, turning on any Apple computer still makes the same warm-toned \u0026ldquo;Happy Mac\u0026rdquo; chime - and it gives me mad nostalgia.\nI was fortunate to be able to play with that machine in the evenings (when my dad wasn\u0026rsquo;t working). I cut my teeth on HyperCard and QBasic, and started a love affair with video games playing such classics as Lode Runner, Apache Strike, Spectre, and Bolo.\nI continued to program throughout my childhood, building websites on GeoCities by teaching myself HTML through the reverse-engineering of other GeoCities pages. I remember HTML being exciting because it turned code into visuals (most stuff I had done before that was command-line output). It was amazing to change some code, reload my browser, and see what changed right away. And a tag based language? Mine blowing to 12 year old me.\nI graduated with a Bachelors degree in Computer Science and went into the workforce full-time around the summer of 2008. It was hard to get a job, because I had no real experience (I was never able to land programming internships during my degree) and it was the middle of the 2008 housing market collapse. Also, I thought I was a LOT more skilled as a programmer than I actually was at the time, so ego didn\u0026rsquo;t help me ace interviews. Eventually I landed at an oil \u0026amp; gas company doing Java work (which is what I learned in my degree also). After a year or so there, I was assigned a .NET project. I quickly fell in love with C# and have been working in it ever since.\nA quick timeline:\nI\u0026rsquo;ve spoken at numerous tech conferences. My most popular talk is about scaling Stack Overflow to 100,000,000 monthly users\r.\nIn 2012 I started this blog to share what I learned in my programming career as I worked. My consistency in posting comes and goes over time, but I always strive to write useful stuff.\nIn 2014 I ended up getting my dream job at Stack Overflow\rwhere I worked for over 9 years. I entered as a Developer and left as the Senior Director of Engineering. It was an incredible experience and time in my career. I still remember my interview with Joel Spolsky like it was yesterday.\nIn 2020 I started building a live programming interview tool in my evenings and weekends to solve a problem we had in our interview process at Stack Overflow. We loved to pair program in our interviews, but we used Google Docs with monospace fonts and this wasn\u0026rsquo;t a particularly great candidate experience. We checked out a lot of popular programming interview companies but they all pretty much did the LeetCode / HackerRank programming question library thing and that\u0026rsquo;s not how we wanted to interview people. So I reimplemented the operational transformation\ralgorithm that is at the heart of real-time collaboration in Google Docs in my pet project, which was then called CodeInterview.\nSince 2022, CodeInterview has been called CodeSession\rand is growing rapidly! It turns out lots of other companies like to interview developers collaboratively too.\nWant to get in touch? You can reach me a few ways:\nLinkedIn: https://www.linkedin.com/in/davidahaney\rGitHub: https://www.github.com/haneytron\rMastodon: https://www.hachyderm.io/@haney\r","permalink":"https://www.davidhaney.io/about/","tags":null,"title":"About David Haney"},{"categories":["blog"],"content":"Well, once again I\u0026rsquo;ve fallen off of the blogging wagon. And once again I am now getting back on said aforementioned wagon. Let\u0026rsquo;s try and make this a more regular thing than once a year or so, shall we?\nI recently built a quick little hobby site / side project with .NET Core MVC. I have a lot of experience with ASP.NET MVC but I was totally new to .NET Core MVC. As a result, I had to teach myself the new ways of doing old things.\nOne such lesson (which took me a few hours to learn by the way, even with what I\u0026rsquo;d call \u0026ldquo;expert\u0026rdquo; Google-fu skills) was how to access appsettings.json settings values.\nLet me walk you through an apples-to-apples comparison of how it\u0026rsquo;s done in both MVC frameworks.\nThe ASP.NET MVC Way - Web.config ASP.NET MVC uses a Web.config file to store your app settings and provides access to the values via the ConfigurationManager class, particularly the ConfigurationManager.AppSettings property.\nHere\u0026rsquo;s an example of an ASP.NET MVC Web.config file that only contains \u0026lt;appSettings\u0026gt;. We\u0026rsquo;ll define two settings here that we will use for both ASP.NET and .NET Core:\n1 2 3 4 5 6 7 \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;appSettings\u0026gt; \u0026lt;add key=\u0026#34;WebDomain\u0026#34; value=\u0026#34;http://localhost\u0026#34;/\u0026gt; \u0026lt;add key=\u0026#34;SomeNumber\u0026#34; value=\u0026#34;10\u0026#34;/\u0026gt; \u0026lt;/appSettings\u0026gt; \u0026lt;/configuration\u0026gt; Accessing these values in code looks something like this:\n1 2 3 4 5 public static void Test() { string webDomain = ConfigurationManager.AppSettings[\u0026#34;WebDomain\u0026#34;]; int someNumber = int.Parse(ConfigurationManager.AppSettings[\u0026#34;SomeNumber\u0026#34;]); } Ugly, right? Not only do we use a static class to access things (good luck testing), but we have to explicitly cast our integer in order to use it in code. Gross!\nThe .NET Core MVC Way - appsettings.json .NET Core MVC, unlike ASP.NET MVC, has no such Web.config file or ConfigurationManager class. Nonetheless, you can still easily read settings from the appsettings.json file.\nAccess to appsettings.json values depends on two new concepts: the IOptions\u0026lt;T\u0026gt; interface and the built-in dependency injection of the .NET Core MVC framework.\nHere are those same two configuration settings from before, but this time located in the appsettings.json file:\n1 2 3 4 5 6 { \u0026#34;MySettings\u0026#34;: { \u0026#34;WebDomain\u0026#34;: \u0026#34;http://localhost\u0026#34;, \u0026#34;SomeNumber\u0026#34;: \u0026#34;10\u0026#34; } } Notice that I nested my settings in a section called MySettings - you can use whatever name you want here, and even define multiple settings under multiple names like so:\n1 2 3 4 5 6 7 8 9 10 { \u0026#34;MySettings\u0026#34;: { \u0026#34;WebDomain\u0026#34;: \u0026#34;http://localhost\u0026#34;, \u0026#34;SomeNumber\u0026#34;: \u0026#34;10\u0026#34; }, \u0026#34;SomeOtherSettings\u0026#34;: { \u0026#34;OtherSetting1\u0026#34;: \u0026#34;blah\u0026#34;, \u0026#34;OtherSetting2\u0026#34;: \u0026#34;-1\u0026#34; } } Accessing these values in code is a little more intimidating at first, but still reasonably simple. First, let\u0026rsquo;s define a class that represents our settings using strongly-typed public properties:\n1 2 3 4 5 6 7 public class MyOptions { public const string SectionName = \u0026#34;MySettings\u0026#34;; public string WebDomain { get; set; } public int SomeNumber { get; set; } } Note that I defined SectionName here as a public const - this is particularly crucial. The value of this string should match the name of your settings section as defined in appsettings.json - in this case \u0026quot;MySettings\u0026quot;.\nNote also that my property names exactly match the settings names in the appsettings.json file - they must be the same for your settings to be successfully bound to your class. If you don\u0026rsquo;t name them the same, your settings class will have default property values.\nNext we need to wire these settings up in dependency injection. We need to tell the dependency injection system how to connect our appsettings.json values to our new class. We do this in the ConfigureServices method of the Startup class:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 public class Startup { public Startup(IConfiguration configuration) { Configuration = configuration; } public IConfiguration Configuration { get; } public void ConfigureServices(IServiceCollection services) { // Add functionality to inject IOptions\u0026lt;T\u0026gt; services.AddOptions(); // Hook our MyOptions class up to the corresponding appsettings section services.Configure\u0026lt;MyOptions\u0026gt;(Configuration.GetSection(MyOptions.SectionName)); } } Now that the DI framework knows what to do, getting our settings is as simple as injecting them into our controller:\n1 2 3 4 5 6 7 8 9 public class MyController : Controller { // Inject the IOptions instance into our controller\u0026#39;s constructor public MyController(IOptions\u0026lt;MyOptions\u0026gt; myOptions) { string webDomain = myOptions.Value.WebDomain; int someNumber = myOptions.Value.SomeNumber; } } And voila - strongly typed settings accessed via code!\nThere are lots of other interesting things that you can do with the options paradigm - for example reloading values automatically when the appsettings.json file is modified. Such advanced features are left for you to Google and learn about if needed for your codebase.\nThanks for reading! Hopefully I\u0026rsquo;ll put out another blog post before November of 2021. ;)\n","permalink":"https://www.davidhaney.io/net-core-mvc-access-appsettings-json/","tags":["dotnet","core","csharp","mvc","json"],"title":".NET Core MVC - How to Access appsettings.json"},{"categories":["blog"],"content":"Sabbatical week one is complete, and I\u0026rsquo;m finally finding some time to blog about it!\nI had a pretty active week. It started with hurricane preparations as Dorian burled toward Florida, however in the end we suffered a very light glancing blow. In short, we got off easy.\nThe Bahamas did not get so lucky, however, and got pretty messed up. I\u0026rsquo;d love for you to take a moment to donate to their recovery here: https://www.bahamas.com/relief\r.\nHere\u0026rsquo;s what I\u0026rsquo;ve been up to outside of hurricanes in week 1!\nMy First Kegged Beer Those who know me know that I enjoy brewing beer at home. I love beer for the taste and don\u0026rsquo;t actually like getting drunk. Ideally, someone will invent some kind of Star Trek-esque Synthohol that has 100% of the flavor and 0% of the side effects. Throw in 0 calories and that would be my dream!\nAnywho, I started off brewing beer back in 2014 or 2015. I brewed extract beers which means you buy malt extract and use that as your base. It\u0026rsquo;s a great way to start brewing!\nPrepping for an extract brew back in 2015 Eventually I progressed to all-grain beers (where you extract the malt yourself, from grain) and have been doing that since about 2015 or 2016 (I can\u0026rsquo;t remember). I usually make 5 gallon batches.\nA couple of home brews fermenting in 5 gallon carboys As many have said before, brewing is 10% fun and 90% cleaning. On that note, one of the worst parts of home brewing is bottling your beer. You must sanitize and clean ~48 bottles (for a 5 gallon batch) + bottlecaps + bottle capper + bottler wand + bottling bucket + tubes, and it sucks.\nA bunch of bottles that need cleaning So, I decided to take up kegging during my sabbatical. Step 1 was acquiring a keg and CO2 canister. These are not cheap, but fortunately in my local friend circle a friend named Kevin moved to NYC a few years ago and left his keg stuff in town. I asked him about buying them used and he generously donated them to me (in need of minor repairs, but the price is right) and wouldn\u0026rsquo;t take any cash. Given that this kind of a setup can run you $250 USD or more, this was a sweet deal. Thanks again, Kevin!\nKeg and CO2 Fermentation Chamber and Kegerator With my keg and CO2 canister in tow, I ordered 2 very cheap and small \u0026ldquo;Magic Chef\u0026rdquo; fridges on labor day sale from Home Depot. My plans were to create a temperature controlled fermentation chamber, and a kegerator system. All-in I had now spent $250. Kegerators pre-made are $600+ so a little DIY is totally worth it! The fridges arrived promptly, and I got to work converting them.\nMy Labor Day Sale fridges Fermentation Chamber Controller It\u0026rsquo;s important to beer to ferment at specific temperatures. I had previously fermented at room temperature but this contributed to off flavors in my beers.\nTo make the temperature controlled fermentation chamber, I needed a controller and a temperature probe. Fortunately, Inkbird makes a unit that supplies both for about $15 but needs hard-wiring. I bought that, a few 3 prong plugs, a cheap plastic enclosure, and a dremel tool, and got to work.\nMy Inkbird temp controller \u0026#43; misc parts First I needed to dremel out holes in the plastic for the Inkbird to mount and the various electrical plugs to also mount.\nMy Inkbird slot Electrical outlets mounted I was unable to snap the electrical outlets in, so I glued them in place using E6000. That stuff rules.\nNext I needed to mount the Inkbird and wire up the outlets. I used 14g wire for this and made sure to properly ground the outlets (don\u0026rsquo;t forget to do this).\nElectrical work done I closed the whole thing up, drilled a hole for the sensor probe, plugged it in, and gave it a test!\nInkbird working! This enables me to plug the control unit into the wall, then plug the fridge into the control unit directly and fish the probe into the fridge. The power cuts on when temperatures rise and cuts off when the desired temperature is reached. Works like a charm!\nFermentation Chamber I had to modify the fridge a little bit to allow a 5 gallon glass carboy to fit. I took out most shelves and trimmed down a few parts with a dremel.\nBefore dremel After dremel And now my carboy fits just fine.\nCarboy in the fridge Kegerator The kegerator was a little easier. I ordered a nice tower tap and got to work drilling a hole in the top of my second fridge.\nTower tap Kegerator hole drilled Next I mounted the tap. A good coring bit is essential.\nKegerator before Kegerator after Brewing the Beer With my kegerator and fermentation chamber done, it was time to brew a beer!\nBrewing! Brewing! Brewing! I made an all-grain hop-forward peach mango wheat ale.\nWith my yeast added (pitched) and my beer in my temperature controlled fermentation chamber, all I needed to do was sit back and let things ferment for about a week.\nTemperature controlled More Next Week Tune in for next week\u0026rsquo;s update when I show you how this beer turned out, and the results of my first few cooking classes.\n","permalink":"https://www.davidhaney.io/sabbatical-week-one/","tags":["career","workplace","stack overflow","culture"],"title":"Sabbatical Week One"},{"categories":["blog"],"content":"One of the most amazing perks of working at Stack Overflow is the sabbatical. After 5 years of FT employment, you are entitled to 20 paid days off (outside of normal vacation) that you can spend however you please. My sabbatical officially begins on Tuesday (as Monday is a holiday). Practically speaking, I am out from Aug 31 - Sept 30, returning to work Oct 1st. This is an amazing opportunity and benefit, and I can\u0026rsquo;t thank Stack Overflow enough for how well they treat employees.\nSabbatical Plans My plans for this time off are pretty simple (and will be blogged here!):\nI\u0026rsquo;m going to unplug from work completely for the entire period. No email, no Slack, nothing. To protect me from myself, I had my AD password scrambed. I\u0026rsquo;ll unscramble it upon my return. I\u0026rsquo;m going to relax. Sleep in a few days. Recharge the batteries. Enjoy September. I\u0026rsquo;m taking cooking classes. Originally they were supposed to start next week, but with hurricane Dorian closing in on Florida as I write this, they cancelled next week\u0026rsquo;s classes. Totally understandable. I will adapt by taking online classes from America\u0026rsquo;s Test Kitchen Online Cooking School\r. Of the online options I investigated, they\u0026rsquo;re both the cheapest and have the largest course variety and offering. So, hopefully it\u0026rsquo;ll go well! I\u0026rsquo;ll be blogging my dishes and experiences, good and bad. I\u0026rsquo;m also brewing some beer. This will involve some technical stuff (I\u0026rsquo;m building a kegerator and a temperature controlled fermentation fridge) and I\u0026rsquo;ll blog about those things as I go as well. Less Technical in September I realize that my blog is typically on the topic of programming and management, so this will be a change for a month. I think it\u0026rsquo;ll be a good one. I\u0026rsquo;ll try to keep the pace to every few days at the least. We\u0026rsquo;ll see if I can stick to that. :)\nStarting With Hurricane Dorian Hurricane Dorian is burling towards us right now. My first few days of sabbatical will be detailing my experience with it. This is not my first hurricane, and it\u0026rsquo;s no longer projected to hit us directly, so it should be an OK time at my house (I\u0026rsquo;m not evacuating). I really hope it just spins off into the ocean today and ends up not hurting anybody. Damn nature, you scary.\nPrepping So, with that said and explained, I\u0026rsquo;ll get back to hurricane prep. Thanks for reading, and I hope you\u0026rsquo;ll enjoy coming along with me on my journey in September.\n","permalink":"https://www.davidhaney.io/sabbatical-begins/","tags":["career","workplace","stack overflow","culture"],"title":"Sabbatical Begins"},{"categories":["blog"],"content":"In this post I\u0026rsquo;d like to show you how I ported an Azure Classic Cloud Service application (which cost me $16 USD a month by the way) to a .NET Core Azure Function, and now host it in Azure for $0 a month! That\u0026rsquo;s right - Azure Functions are both awesome and (usually) free!\nIntroducing realDonaldTron So back in late 2016, when our dear leader was elected president, I decided to have a little fun at his expense. I created a Twitter account\rand generated API keys. I then created an Azure Classic Cloud Service and wrote a quick and dirty Twitter bot. This bot consumes 1000s of the most recent tweets from Donald Trump, cleans up his grammar and punctuation a little (because he can\u0026rsquo;t write words good), generates a Markov Chain\rfrom the Tweets, and then schedules a .NET Timer to generate and publish a new Tweet at the top of every hour.\nHere\u0026rsquo;s what the startup code roughly looked like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 public void Start() { // Twitter API keys var consumerKey = \u0026#34;*********\u0026#34;; var consumerSecret = \u0026#34;*********\u0026#34;; var userAccessToken = \u0026#34;*********\u0026#34;; var userAccessSecret = \u0026#34;*********\u0026#34;; // Set credentials Auth.SetUserCredentials(consumerKey, consumerSecret, userAccessToken, userAccessSecret); // Build the Markhov chain Console.WriteLine(\u0026#34;Building Markhov Chain...\u0026#34;); BuildMarkhovChain(); Console.WriteLine(\u0026#34;Built! {0} root nodes\u0026#34;, _markhovTreeRoot.Children.Count); // Start generating tweets Console.WriteLine(\u0026#34;Starting Tweets...\u0026#34;); Console.WriteLine(); // Configure timers var timeUntilNextHour = GetTimeUntilNextHour(); // GenerateTweet ends by adjusting timer again for next hour, to avoid clock drift _tweetTimer = new Timer(GenerateTweet, null, timeUntilNextHour, Timeout.Infinite); // Rebuild the Markov chain once per day from the newest Tweets _buildMarkhovChainTimer = new Timer(BuildMarkhovChain, null, 24 * 60 * 60 * 1000, 24 * 60 * 60 * 1000); } However, I was getting a little tired of paying $16 a month to host it as an Azure Classic Cloud Service. Given my thrifty nature combined with my interest in building my first .NET Core application, I thought a great first step would be to port realDonaldTron over. And so I did!\nPorting .NET to .NET Core Porting a .NET application over to .NET Core is usually not too terrible. I found the easiest way was to create a new .NET Core solution using Visual Studio, drag and drop my directory structure over to my new directory, install all of the NuGet packages that I needed in my old solution (many will have a netstandard specific version for .NET Core), and then attempt to compile and correct errors, Googling (or Stack Overflowing) for new APIs where old calls no longer existed. I needed to make very few changes to realDonaldTron in order to get him working.\nI was also pleasantly surprised at how simple the new .csproj file structure is. By default, everything in the folder is included in the project except where you specify exceptions. This makes for a pretty clean file. Here\u0026rsquo;s the .NET Core realDonaldTron .csproj file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 \u0026lt;Project Sdk=\u0026#34;Microsoft.NET.Sdk\u0026#34;\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;TargetFramework\u0026gt;netcoreapp2.1\u0026lt;/TargetFramework\u0026gt; \u0026lt;LangVersion\u0026gt;latest\u0026lt;/LangVersion\u0026gt; \u0026lt;AzureFunctionsVersion\u0026gt;v2\u0026lt;/AzureFunctionsVersion\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;PackageReference Include=\u0026#34;Microsoft.NET.Sdk.Functions\u0026#34; Version=\u0026#34;1.0.*\u0026#34; /\u0026gt; \u0026lt;PackageReference Include=\u0026#34;TweetinviAPI\u0026#34; Version=\u0026#34;4.0.0\u0026#34; /\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;ItemGroup\u0026gt; \u0026lt;None Update=\u0026#34;host.json\u0026#34;\u0026gt; \u0026lt;CopyToOutputDirectory\u0026gt;PreserveNewest\u0026lt;/CopyToOutputDirectory\u0026gt; \u0026lt;/None\u0026gt; \u0026lt;None Update=\u0026#34;local.settings.json\u0026#34;\u0026gt; \u0026lt;CopyToOutputDirectory\u0026gt;PreserveNewest\u0026lt;/CopyToOutputDirectory\u0026gt; \u0026lt;CopyToPublishDirectory\u0026gt;Never\u0026lt;/CopyToPublishDirectory\u0026gt; \u0026lt;/None\u0026gt; \u0026lt;/ItemGroup\u0026gt; \u0026lt;/Project\u0026gt; A welcomed change from the million line csproj files of past\u0026hellip; and the Git conflicts when checking in code! So frustrating.\nHosting in Azure: Don\u0026rsquo;t use WebJobs Since I had ported realDonaldTron over as a console application, the right click \u0026ndash;\u0026gt; publish dialog in Visual Studio immediately told me to publish it to Azure as a WebJob in an AppService context. Heck, there\u0026rsquo;s even a free tier - no cost to me! This seemed as if it solved all of my problems, but I soon learned why this was a bad plan.\nWebJobs time out Azure WebJobs are a way to run a service, such as a command line application, within the context of a web application. This web application comes with the built-in IIS 20 minute time-out. This means that if you have a service running as a WebJob, and nobody is hitting the URL at which it has been published (don\u0026rsquo;t worry, Azure generates a cute little web page for you if your WebJob has no web page of its own), then the application pool spins down, taking your service down with it.\nFor me, this manifested as my application sometimes working soon after I published it (if I had published it with 20 minutes or less before the next hour), other times not working at all (if I had published it in minutes 1 - 40 of an hour), and regardless it would never publish another Tweet beyond the first 20 minutes.\nAlways On isn\u0026rsquo;t cheap I Googled my little heart out but could not find much useful documentation on Azure WebJobs. It wasn\u0026rsquo;t until I started exploring the App Service settings in the Azure Portal that I realized I needed to have the Always On feature configured (which disables the IIS timeout). But guess what - you can\u0026rsquo;t turn that on for the lowest two tiers of App Services (free and shared) - you must pay for at least a Basic plan.\nNo Always On for you! And did I mention that Basic plans start at $55 USD per month (assuming Always On and therefore running 24 / 7)?\nI\u0026#39;m not paying that! WebJobs for web sites So while WebJobs are a great option for websites, and App Services can be free or cheap for website hosting (since they will lazy-load whenever someone makes the first request after the 20 minute idle), I realized one important fact:\nWebJobs should NEVER be used for services that need to run continuously and/or always be on.\nAnd with that I began Googling Azure alternatives for command line services\u0026hellip; Enter Azure Functions.\nHosting in Azure: Use Azure Functions Disclaimer: nobody is paying me to talk about Azure or Azure Functions here (though they should). With that said, Azure Functions are awesome.\nPricing Here\u0026rsquo;s the thing about Azure Functions: you pay based on consumption in two ways: total executions, and gigabyte seconds (GB-s). These are defined confusingly:\nFunctions are billed based on observed resource consumption measured in gigabyte seconds (GB-s). Observed resource consumption is calculated by multiplying average memory size in gigabytes by the time in milliseconds it takes to execute the function. Memory used by a function is measured by rounding up to the nearest 128 MB, up to the maximum memory size of 1,536 MB, with execution time calculated by rounding up to the nearest 1 ms. The minimum execution time and memory for a single function execution is 100 ms and 128 mb respectively.\nWhile these pricing calculations are confusing, it\u0026rsquo;s not that hard to decipher them. Basically, a really quick execution of an Azure Function (like a \u0026ldquo;Hello World\u0026rdquo;) will cost:\n(100 ms / 1000 ms) * (128 MB RAM / 1024 MB) = 0.0125 GB-s\nMultiplying that by the cost per GB-s gets you your cost per execution:\n0.0125 GB-s * $0.000016/GB-s = $0.0000002\nWhich is far less than a single penny.\nIn fact, you could execute your \u0026ldquo;Hello, World\u0026rdquo; function 50,000 times before it would cost you $0.01!\nMonthly free grants! But these crazy cost calculations are not even a concern for most of your and my use cases! Azure Functions pricing includes super generous free grants per month: 1 million executions and 400,000 GB-s!\nAzure Functions pricing Basically, you\u0026rsquo;d have to run your application 1,000,001 times or for 400,001 GB-s in a month to even begin to see charges to your Azure account.\nWhat does realDonaldTron cost? realDonaldTron runs at about 30 seconds per execution, once per hour, so my calculation is:\n30 seconds * 24 runs per day * 31 days in a month * 0.125 GB RAM used = 2,790 GB-s per month.\nThis is of course WAY less than the 400,000 GB-s I get for free every month, so realDonaldTron runs for free!\nPretty awesome, right? Basically: you\u0026rsquo;re gonna be hosting all of your Azure Functions for free - JACKPOT. And even if you somehow go over the limits, Azure Functions are stupid cheap.\nA caveat about Azure Function logging Note that if you use any of the Logging features of Azure Functions, you WILL pay a small fee as they are written to Azure Storage. realDonaldTron costs about $0.12 per month in Azure Storage logging - and I happily pay it when compared to the $16 a month of Classic Cloud Service I was running.\nBut enough about costs, let\u0026rsquo;s get to the fun stuff - creating the actual Azure Function!\nCreating an Azure Function in VS Start by creating a new Azure Function project in Visual Studio (be sure to update to the latest Visual Studio version). My screenshots below are from VS Community 2017 (the free edition) which I use on my home (non-work) computer for pet projects:\nCreate an Azure Functions project On the next screen, make sure you pick Azure Functions v2 (.NET Core) at the top! Azure Functions v1 is for .NET Framework and non-Core apps. I also picked Timer trigger because I want realDonaldTron to run every hour, on the hour:\nCreate an Azure Functions project Note that the \u0026ldquo;Schedule\u0026rdquo; here is a modified CRON syntax. I had a hard time finding good resources for CRON scheduling that actually worked in Azure, but this blog post by Armin Reiter\ris an awesome cheat sheet that you should definitely use! Also, don\u0026rsquo;t worry about the schedule in this dialog - you can change it programmatically super easily in a minute or two.\nFinish up by pressing OK and next you\u0026rsquo;ll see this boilerplate code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 using System; using Microsoft.Azure.WebJobs; using Microsoft.Azure.WebJobs.Host; using Microsoft.Extensions.Logging; namespace FunctionApp1 { public static class Function1 { [FunctionName(\u0026#34;Function1\u0026#34;)] public static void Run([TimerTrigger(\u0026#34;0 */5 * * * *\u0026#34;)]TimerInfo myTimer, ILogger log) { log.LogInformation($\u0026#34;C# Timer trigger function executed at: {DateTime.Now}\u0026#34;); } } } Feel free to change the class name and namespace here - the only important thing to name is the [FunctionName] attribute as this is what will appear in the Azure Portal UI. You can also see that your CRON syntax from the creation dialog is within the [TimerTrigger] attribute here, and can be readily modified. The ILogger you see there is the logger that goes to Azure Storage and will cost you a few pennies per month. If you\u0026rsquo;re trying to run truly for free, be sure not to log anything at all.\nIt\u0026rsquo;s also worth noting that Azure Functions support async entry method signatures, so async your heart out!\nHere\u0026rsquo;s what my .NET Core Azure Function Run method for realDonaldTron looks like:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 namespace RealDonaldTronFunc { public static class Function { [FunctionName(\u0026#34;RealDonaldTronFunction\u0026#34;)] public async static Task Run([TimerTrigger(\u0026#34;0 0 * * * *\u0026#34;)]TimerInfo myTimer, ILogger log) { log.LogInformation($\u0026#34;C# Timer trigger function executed at: {DateTime.UtcNow}\u0026#34;); // Set Twitter credentials var consumerKey = \u0026#34;********\u0026#34;; var consumerSecret = \u0026#34;********\u0026#34;; var userAccessToken = \u0026#34;********\u0026#34;; var userAccessSecret = \u0026#34;********\u0026#34;; Auth.SetUserCredentials(consumerKey, consumerSecret, userAccessToken, userAccessSecret); // Build the Markhov chain Console.WriteLine(\u0026#34;Building Markhov Chain...\u0026#34;); BuildMarkhovChain(); Console.WriteLine(\u0026#34;Built! {0} root nodes\u0026#34;, _markhovTreeRoot.Children.Count); // Start generating tweets Console.WriteLine(\u0026#34;Starting Tweets...\u0026#34;); Console.WriteLine(); await GenerateTweet(); log.LogInformation($\u0026#34;C# Timer trigger function completed at: {DateTime.UtcNow}\u0026#34;); } // ... } } You might notice that all of my Timer stuff is gone. That\u0026rsquo;s because Azure Functions natively support timed triggers, and the CRON syntax 0 0 * * * * says \u0026ldquo;run this every hour, on the hour\u0026rdquo; which completely replaces my need for C# Timers!\nNote that unless your function needs to run continuously, you should have it terminate by exiting the Run method as soon as possible. realDonaldTron does not need to remain active - he grabs the latest Tweets and builds a new Markov chain every hour instead. He terminates as soon as the GenerateTweet() function call returns.\nAfter you\u0026rsquo;ve written your function, it\u0026rsquo;s time to publish it to Azure. Simply right click \u0026ndash;\u0026gt; Publish your project, to be met with this screen:\nPublish your Azure Function Press Publish, and on the next screen you\u0026rsquo;ll need to name your function and set up the various resources it\u0026rsquo;ll use. The part to pay attention to here is the Hosting Plan which you\u0026rsquo;ll need to be sure is set to Consumption:\nPublish your Azure Function Publish your Azure Function Follow the steps past this screen and you\u0026rsquo;ll have your Function published to Azure in mere moments! If you want to test it, but don\u0026rsquo;t want to wait for the timed trigger, you can visit the Azure Portal and run it manually:\nTest your Azure Function Here your logging will appear in the debug window below for manual runs - it\u0026rsquo;s quite a nice little tool. If you\u0026rsquo;ve published a function.json (which you will by default), it will not allow you to edit it here which is fine. Just make local changes and re-publish if needed.\nThe end result Thanks to Azure Functions and .NET Core, I now have realDonaldTron running every hour on the hour via CRON scheduling, as an Azure function, well within the monthly free grants provided on Consumption plans. Can you guess on this graph when I converted over to Azure Functions?\nAzure Functions are free, Classic Cloud Services are not Thanks for reading, and enjoy creating your own easy, cheap, and often free Azure Functions!\n","permalink":"https://www.davidhaney.io/net-core-azure-functions-tutorial/","tags":["dotnet","core","csharp","azure","functions"],"title":".NET Core Azure Functions Tutorial"},{"categories":["blog"],"content":"It\u0026rsquo;s been a while since I changed things up, so I decided on a new Twitter handle and blog domain.\nThis site is now hosted at davidhaney.io\rand called \u0026ldquo;David Haney\u0026rdquo; which in my opinion is a much better (and more descriptive and maybe even more egotistical) domain than haneycodes.net\rand the old name of \u0026ldquo;Haney Codes .NET\u0026rdquo;\nDon\u0026rsquo;t worry - haneycodes.net deep links will redirect properly for years to come, so you won\u0026rsquo;t miss anything at all. I\u0026rsquo;ve told Google\u0026rsquo;s search engine all about the change too, so links should work properly in results there as well.\nI\u0026rsquo;ve also changed my Twitter handle! You can now find me at @haneytron\rwhich again seems like a better handle to me than @haneycodes (which I had used until now).\nChange is scary, but I feel these are good ones.\nSee you in the next blog post!\n","permalink":"https://www.davidhaney.io/new-blog-name-and-domain/","tags":["blog","changes"],"title":"New Blog Name and Domain"},{"categories":["blog"],"content":"A few weeks back I wrote this tweet:\nI did a presentation / speaking thing last week that got me thinking: would it be useful to blog about how to build a resume as someone new to tech, from the perspective of a hiring manager? I might whip that up today or tomorrow if people like the idea.\n\u0026mdash; David Haney hachyderm.io/@haney (@haneytron) July 9, 2018 43 likes later, it\u0026rsquo;s clear to me that this topic is in-demand. So, let\u0026rsquo;s skip over the pleasantries and talk about how you can create a great technical résumé as a junior developer, from the perspective of me (a hiring manager).\nSo let\u0026rsquo;s dive right in! I\u0026rsquo;m going to add photos of my junior résumé here as we go as well, to give you a nice visual of what I am talking about.\nNote that this article is from the perspective of a US hiring manager. International customs, norms, and laws may vary.\nBefore the résumé You might think that a résumé is enough to get you hired as a junior, but often you\u0026rsquo;d be wrong. I want to see a cover letter that stands out. Write a few paragraphs to me about my company. Tailor it so that I can\u0026rsquo;t copy-paste \u0026ldquo;Acme Inc.\u0026rdquo; in over my company\u0026rsquo;s name with it still making complete sense. Show me that you want to work here. Tell me what you can offer, and why you love this company. This will immediately put you ahead of the pack.\nA good cover letter goes a long way: I can assure you I\u0026rsquo;ll read it. But up next is the actual review of your skills and experience, and you need to nail that also.\nGet the first few lines right Let\u0026rsquo;s talk shop: I need the basics, and I need them now. Because you likely have little or no professional work experience, I am going to feel uncertain about you. Hiring a good junior is like throwing darts in the dark: sometimes you hit the bullseye, but often you miss entirely. Standing out in the dark is as easy as shining some light on yourself.\nI don\u0026rsquo;t have time to read everybody\u0026rsquo;s entire résumé, so you\u0026rsquo;re going to put the super relevant stuff right up top, in front of my face, so that I can\u0026rsquo;t possibly overlook it while skimming your application. GitHub repo linked at the bottom? Too bad for you, I didn\u0026rsquo;t read that far. Relevant project work on page 3? Good luck getting anyone to read it. You need to show me why I should keep reading this résumé before I lose interest. Remember that I\u0026rsquo;ve likely never met you before and know nothing about you. So the first few lines of your résumé are your selling points.\nStart with:\nYour full name Your phone number Your email address (don\u0026rsquo;t use an unprofessional one) Your mailing address (or at least state and zip / postal) Your current or sought after title, such as \u0026ldquo;Junior Developer\u0026rdquo; Relevant employment visas / status if required for the position Links to relevant artifacts like GitHub repos, LinkedIn profile, a blog, or Stack Overflow profile! It should end up looking something like this:\nThe first few lines of the résumé Those few lines just gave me a ton of information that will help me decide if I want to interview you. I can use your zip code or address to assess eligibility of employment to start (I might not be able to hire internationally for example). It also helps me figure out what timezone you live in so that I don\u0026rsquo;t call you at 6am asking for an interview. Showing me the job title smack dab front and center tells me that you read my job posting and are applying for a relevant position (vs spamming every job opening you can find, and yes people do this). And the most important part is that you minimized my uncertainty about you by providing evidence (in the form of public artifacts) that give me some insight into your technical abilities and previous experience. Those Stack Overflow and GitHub links do wonders, folks.\nIf you can quickly make me feel very certain about you, especially compared to your competition who also applied for the job, then congratulations: you\u0026rsquo;re getting an interview.\nNo \u0026ldquo;objective\u0026rdquo; statements As a hiring manager, it\u0026rsquo;s clear to me that your objective is to get a job that pays you either the same or more than you currently make, with vacation and benefits to-boot. It\u0026rsquo;s also clear to me that this job should test your skills and help you learn and grow if I want you to stick around and stay interested in the work. You don\u0026rsquo;t need to spell that out for me: I\u0026rsquo;ll figure it out in the interview(s) regardless.\nWriting a paragraph about your professional objective(s) is the most common mistake that I see juniors make. It\u0026rsquo;s tacky, superfluous, and boring. It also screams \u0026ldquo;I\u0026rsquo;M BRAND NEW TO THIS!\u0026rdquo; which to me isn\u0026rsquo;t a deal breaker, but might be for other hiring managers. In hundreds of résumés I\u0026rsquo;ve never read a single objective that caught my eye or was at all interesting. Skip it entirely.\nWhat goes next? In order, here\u0026rsquo;s what I generally care about:\nSkills I want to see technical skills and soft skills. Can you talk to computers? Can you talk to people? Show me. Contrary to popular belief, programming is a people job. If you are the best code-slinger in the world, but you are a jerk or egotist to everybody that you work with, I will still fire you (or simply not hire you if I can detect it during the interviews).\nTechnical skills are fairly common in our industry; soft skills less so. Stand out by showing me that you have compassion for people and get the big picture; that programming is not just code. It will get my attention.\nTechnical and soft skills Work experience Show me any relevant work history. Also show me any irrelevant history that allows you to demonstrate common work skills. Worked a crazy customer service job where you balanced 3 tasks at once all the time? Great! Write about it in the way that is focused around multitasking, being efficient, and getting things done on time and budget. I\u0026rsquo;ll eat that right up.\nTry to avoid gaps in this section. If you have them, include what you did during the gap as part of the experience. It could be that you were attending school, a bootcamp, or even taking care of other family members. As discussed earlier, try to reduce uncertainty by giving lots of information.\nAlso try to avoid going further back than your most recent 3 experiences. Past that most managers really don\u0026rsquo;t care, and it just takes up a ton of space and makes for more reading.\nWork experience Project work Whether you taught yourself, graduated from a bootcamp, or went to college, it\u0026rsquo;s likely that you\u0026rsquo;ve completed a programming project or two with a group. Tell me about these recent projects. Most importantly: tell me about your role in the projects. Did you do the bulk of the work while others slacked? Did you coordinate assignments and deliverables? Were you the major front-end or back-end programmer? Tell me about a difficult programming challenge that came up in the project and how you solved it. All of these things give me insight into your talent and abilities.\nSimilar to work experience, try to keep this section to your 3 most recent projects max.\nProject work Education Some employers value this more than others. I\u0026rsquo;m personally not terribly concerned with how or where you got your education. I\u0026rsquo;m largely concerned with figuring out whether or not you can do the job.\nInclude your education, whether relevant or irrelevant. Grades don\u0026rsquo;t usually matter to me, but on the other hand a 4.0 on a merit scholarship is going to get my attention in a big way. It never hurts to write down what brought you here, be it college, bootcamp, or autodidact.\nEducation Hobbies \u0026amp; human stuff If I\u0026rsquo;ve read this far I am liking what I\u0026rsquo;ve seen. And we all know that work isn\u0026rsquo;t life (or at least in a healthy balance it shouldn\u0026rsquo;t be). So make my job as an interviewer easier by giving me some interesting talking points that I can use to break the ice! Tell me about your pets, your beer brewing hobby, how much you like RC cars, your model train set, your entire series 1 Pokemon card collection, anything at all! Just keep it safe for work.\nAlso tell me about any local user groups or events you have recently attended, or other milestones in your life that are work-relevant.\nHobbies \u0026amp; human stuff This section is a bit like playing the lottery: sometimes you\u0026rsquo;ll luck out and an interviewer will share your hobby, which will endear them to you a bit more. Plus they\u0026rsquo;re just fun to talk about with candidates when getting to know the person!\nPutting it all together Here\u0026rsquo;s what the whole thing looks like when it\u0026rsquo;s all said and done:\nThe entire résumé Try to keep your cover letter and résumé to 2 or 3 pages max. The example one which I\u0026rsquo;ve created is 2.5 pages before cover letter, and I\u0026rsquo;d say that should be about the maximum length. I once received a 23 page résumé dating back to 1988 and you can bet I didn\u0026rsquo;t even bother reading it.\nRemember: quality is way more important than quantity. Sell yourself in a few words but don\u0026rsquo;t sell yourself short. And don\u0026rsquo;t be afraid to brag about yourself a little - be confident! Tell me why you\u0026rsquo;re awesome and hopefully I\u0026rsquo;ll see why also and end up hiring you. :)\n","permalink":"https://www.davidhaney.io/writing-a-great-junior-tech-resume/","tags":["career","hiring","management","workplace","interviews"],"title":"Writing a Great Junior Tech Résumé"},{"categories":["blog"],"content":"Well I failed badly in my mission to blog every week of 2017. I guess life and stuff just got in the way in the end. I\u0026rsquo;ll try to be more consistent in the second half of 2018.\nAnyway, I bring some news: I have a new blog engine, and we are hiring at Stack Overflow!\nNew blog engine I was previously using WordPress but had many issues and concerns with it. One of the biggest concerns was that the site was not mobile friendly, and making it responsive was next to impossible.\nAs a result, I began to explore new blogging platforms. I was particularly interested in the static content generators, as they solve a lot of technical issues for mostly static sites (such as caching).\nI tried Jekyll but when I realized how ridiculous it was trying to set up a simple blog I passed on it:\nCurrent status: installing Jekyll on ruby on Ubuntu on Windows Subsystem for Linux on Windows 10, all so I can move my blog over to Jekyll and start anew. Abstractions? What abstractions?\n\u0026mdash; David Haney hachyderm.io/@haney (@haneytron) June 6, 2018 That got old fast. Thankfully some of my coworkers and friends - Julia Silge\rand Matt Sherman\r- put me onto Hugo\r.\nAlthough I\u0026rsquo;m a die-hard .NET guy, I have to hand it to the people behind both Go and Hugo. This CMS is excellent. Porting my blog from WordPress was a breeze; it took me a few hours over 2 or 3 weeknights at most to complete. I was even able to import all of my old blog post comments over to the new Disqus comments which I am now using.\nAs you read this, my blog is statically generated and hosted on GitHub Pages. No, really, check it out\r! I am super impressed by how flexible and easy both Hugo and GitHub Pages are, and by being able to write my blog with Markdown, committing it, and having it instantly updated. Good stuff.\nI highly recommend that you check out Hugo\rand GitHub Pages\rfor your blogging needs as well.\nOkay, enough fawning over Hugo, onto another interesting topic.\nStack Overflow is hiring! I have worked here for almost 4 years now, and can say with a straight face that this is the best job that I\u0026rsquo;ve ever had.\nWe\u0026rsquo;re hiring a bunch of technical positions right now, including an Engineering Manager, a VP of Engineering, and many Full-Stack Web Developers\r!\nWe\u0026#39;re hiring a VP of Engineering at @stackoverflow. This is an amazing opportunity to manage and grow a top-notch engineering team and work on a site that reaches 50+ million people a month! https://t.co/4DXEjwhT5U\n\u0026mdash; David Fullerton (@df07) May 30, 2018 If any of these listings sound interesting to you, I\u0026rsquo;d encourage you to apply directly. If you have questions or would like to discuss the positions before applying, my Twitter DMs are open\rso give me a shout - I\u0026rsquo;d love to hear from you.\nI hope you\u0026rsquo;ll consider working with us. We have awesome benefits, very low turnover, great culture, and terrific people. Best of all: you can work from anywhere in the world.\nBlogging more often Well that\u0026rsquo;s all the updates for now. I will endeavor to blog more consistently in the next half of 2018. Sorry for the long wait on this post, and for dropping the ball. Thanks for reading!\n","permalink":"https://www.davidhaney.io/new-blog-and-hiring-needs/","tags":["career","hiring","management","open source"],"title":"New Blog \u0026 Hiring Needs"},{"categories":["blog"],"content":"This post is for those of you who hire developers, and also junior developers who want to be hired. Let’s talk about how developers are just like individual stocks in the stock market. Time for a little role-playing: you’re now a stock market investor.\nAs a financial advisor, your company has given you $2,000,000 USD to invest in the stock market. It’s made very clear that the future of the company depends on the return on investment (herein called ROI) – “gains” – that your investments bring to the company. Your decisions will have a major impact on the company’s future. Given that kind of pressure, what’s your investment strategy for success? Begin by reviewing the kinds of stocks available to invest in.\nLet’s Review Some Stocks You take a look at stock #1. It has been on the market for a decade, has nearly consistently yielded high returns (with references you can investigate and check into), and is very reputable. Putting a good chunk of your money here is probably a reasonable call, since this stock is vetted and has historically provided value over time. It’s unlikely to suddenly drop to no value, and if you see it going south you can bail out before you lose it all.\nStock #2 is the interesting one. It’s brand new to the market. You can find no history on it, no performance trends, no reputation, nothing. It’s a total wildcard that has a reasonably low price tag – about 1/3 of stock #1. Its value could skyrocket resulting in incredible ROI. However, it could also end up being a dud, resulting in losing it all. You have little information to go on: this stock is truly a gamble. Do you invest heavily in it?\nGiven the two options above, what’s the smart move? In general, putting all of your money into a single thing is very risky, so you’re likely to diversify your portfolio a bit. It doesn’t make a ton of sense to invest heavily into stock #2 because it’s a major gamble, but there’s some room for potential and it might pay off. So why not put 80% into different stocks that fit the archetype of stock #1 and the other 20% in stocks that fit #2’s profile? That would make for a smart investment with some near-guaranteed returns, including some investment into gambles with high potential.\nStocks On Market == Devs On Market Stock #1 is a senior developer with a proven track record and solid reputation. Stock #2 is a junior developer.\nHiring nothing but juniors is a recipe for high volatility and potential disaster, for reasons that become obvious when given the stock market analogy above. Hiring nothing but seniors is one way to get reasonable gains, however you miss out on significant potential to hire an incredible up-and-coming junior if you never hire any at all. A good strategy incorporates both, with seniors afforded time to mentor the juniors and develop their skills.\nGetting Hired As A Junior As a junior developer, the less artifacts that you can point to and show to companies, the more of a risk you are for them to take. You can mitigate some of this risk with a good interview, but if that interview doesn’t include coding tests which you ace then it might not be enough to get you in the door.\nA junior should strive to create artifacts that reduce the risk of hiring them. These could take virtually any format, and given that everybody is different and we are not all afforded the same privileges and opportunities, one should strive to create artifacts that suit their situation. Single parent with 2 children and little free time? Put a few hours each week into an open source project (or contribute to other open source projects). You’ll be amazed how quickly that adds up. Unemployed with tons of free time? Create a project that shows off your skills and stretches your knowledge, which in turn causes you to learn. Struggling with the whole “I need a job to get the experience to get a job” thing? I’ve been there myself, and while my situation was surely not identical to yours, I found that investing some time into reading books and writing small applications to demonstrate my skills did wonders for potential employers.\nThe point is that all developers are going to sell themselves as hard as they can to a potential employer. To an employer, they may all look similar. Do what you can to stand out and reduce uncertainty by creating evidence of your potential and abilities, and show that to them instead. Talk is cheap, action speaks louder than words.\n","permalink":"https://www.davidhaney.io/on-hiring-developers-are-like-stocks/","tags":["career","interviews","management","open source","salary","workplace"],"title":"On Hiring: Developers Are Like Stocks"},{"categories":["blog"],"content":"In case you missed the big news in the industry this week, a GitLab employee accidentally deleted a ton of production data\rand took their platform down for hours. It was only when everything was on fire and they were in deep trouble that they turned to their backup systems… only to find that none of them actually worked.\nBackup Prod Data Regularly Not exactly a groundbreaking statement, right? Everybody knows this. If there was a “working in corporate IT 101” manual it would have a chapter on this concept. It’s common sense.\nEven still, a lot of people and companies – like GitLab – tend to “set and forget” their backups. They probably created their backup mechanism years ago, tested it at the time, confirmed that it worked, and then scheduled it to run every night at 1am EST or something. Then, since it was out of sight and out of mind, they promptly forgot about it and moved on to other things. After all, they never had a need to check on it right? Nothing had broken down. Until yesterday.\nA Guide To Good Backup Process The secret to ensuring that your backup process is effective and functional is to integrate it into your daily work. One of the best ways to do this is to use it to set up a new dev’s local environment. Have them configure and install the IDE and related tools, and then have them pull down the most recent backup and restore from it to set up their local database. What’s that, you say? It has PII and sensitive data? You’re probably right, which is why your backup process should, as appropriate, create 2 copies: 1 that strips the data (for local dev env) and 1 that doesn’t (for prod restore).\nGreat, so you’ve confirmed that your backups work for a local environment, but what about production? The next step in a good process is simple too: artificially destroy your production environment regularly. Set up fail-over tests at off hours (and compensate your amazing site reliability / IT team appropriately for conducting these tests in off hours too). I recommend once per quarter as a starting point: at 2am on Sunday drop your production database (but don’t delete it, just take it offline so you can bring it back if you find out that your backup system isn’t working). Let your staff work to restore a recent backup and bring the site back online. Announce the outage in advance to your users, and update people on social media or via email when it begins and ends.\nThere is much to be learned and gained from this intrusive and destructive process. For one, you will force your dev team to create a good “the site is down” experience since your customers will otherwise see infinitely spinning web pages or terrible error dumps. Another is that you can time the average outage and thus discern how long you’ll be down if your production database ever actually takes a spill. Finally, your disaster recovery staff will be fresh on their skills and able to fix your real outages quickly and predictably. There are many tangible and hidden benefits derived from just a few hours of planned outage per year.\nGitLab Did One Thing Right The final step in your solid, functional backup process which you test quarterly and use to spin up new dev hires is to document the hell out of everything. When you do these planned outages, have the disaster recovery staff document, step by step, the actions taken to fix it. When you have real live outages, document those too and share the knowledge with the public.\nGitLab got this part right, and are being heralded as a great example and learning experience in the industry instead of spited for mysterious downtimes and no communication. I promise you that this week, many disaster recovery people are doing extra backup tests that they wouldn’t have thought to do otherwise – all as a direct result of the GitLab incident. Making your disasters and their recoveries public creates goodwill in the community, provides a learning experience, and shows people that you can be trusted.\nGitLab took a bad situation and created the best possible outcome, both for themselves and the entire community. For that they should be thanked, not mocked. After all, we are all human and we all make mistakes. Knowing this, you’ll be really glad that you practice making mistakes every quarter when your production database actually goes down in flames.\n","permalink":"https://www.davidhaney.io/gitlab-data-loss-a-discussion/","tags":["attitude","methodology","mistakes","process"],"title":"GitLab Data Loss: A Discussion"},{"categories":["blog"],"content":"In part 2 of my series on dev team interactions, I’d like to talk about conducting good code reviews. Most dev teams will find themselves in a situation where code reviews are necessary, and in my experience many do them very poorly. I’ve even worked in companies that had such a negative code review culture that people left the review sessions upset, even considering quitting. With a few easy adjustments, you can quickly learn to conduct excellent and positive code reviews with your team.\nThe Ground Rules A code review is a process. Like any good process, clear rules need to be established and followed to ensure a consistent experience. Here are mine:\nAttack the code, never the person. Criticizing code is OK, but people are not code. It’s never OK to criticize the person and make them feel bad. Focus strictly on the code output and never make it personal. Don’t laugh or make negative jokes. A person who has their work on display – often on a projector in front of others – is feeling self-conscious as it is. Don’t snicker at their work. Avoid joking about their decisions. I assure you they are trying to do the right thing. Set a strict time limit as a means of focus. Make the code review 15 minutes, 30 minutes, or even 1 hour. Stick to this schedule. This forces you to prioritize the important stuff and ensures (intentionally) that you can’t review absolutely everything the person has done. Don’t take unlimited time to scrutinize every single line of code written. Ever had someone comb through your code line by line, making commentary as they go? “And how did that make you feel?” Thank the person for sharing their code with you. A code review is an intimidating, scary thing – especially for developers on the junior side of the spectrum. Set the tone correctly by being appreciative of their time and sharing. Make it known that they are valued and you appreciate their work up front. This will help them feel relaxed and learn to enjoy code reviews, which in turn will cause them to want to share more of their code willingly in the future as well. The above rules serve only as an example from my personal experience. You should create rules that work for your company and work culture. Use real and practical standards that matter to your team, not just theoretical ideologies that someone on your team read in a book. Always remember: a process is only as good as the people that follow it, so try to be consistent with whatever rules you decide on. And if they aren’t working well in practice, change them up!\nSeek Understanding, Not Power The general goal of your code review attendees should be to seek understanding, not explanations. Avoid an us-them conflict or standoff. This can be easily accomplished with a subtle shift in communication approach. Rather then asking aggressive questions that demand the reviewee explain the code to your team such as “why would you do it that way?” or “how could that possibly work?” you need to ask questions to promote understanding the code instead. Once your team and the reviewee correctly understand the code being reviewed, you can proceed to discuss a different approach without conflict or hurt feelings.\nWhen you see some code that makes you think “what the hell were you smoking?” (explain) you should instead ask “Can you tell me why you wrote the method this way?” (understand). These statements are similar but the former gives the power to you while the latter empowers the code reviewee to share their knowledge and thought process in a non-defensive way. This approach to questioning takes a bit of practice, but is very powerful. Some other examples of how you’d change common code review aggressive thoughts and statements from explaining to understanding:\n“This class name is wrong” becomes “This class name doesn’t conform to our standards, was that intentional?” You’ll probably find that they say it wasn’t, and agree to fix it on their own volition. “I see a bug in your code!” becomes “I think there’s a null reference exception on line 28 for variable X, do you see how it happens?” “This is terrible” becomes nothing – keep your mouth shut. There’s no value in such a statement other than to make the person feel bad. If the code truly is terrible, express these concerns to the person (and maybe their manager) privately for further investigation and resolution. People are not robots, and will never conform perfectly to your shop’s coding standards. That’s OK. Pick your battles, and call out only the major violations. Let the little things (like naming and spacing) slide. If the dev is hitting 90% of the standards, the rest of the team can pick up on the 10% deviation without much worry or effort. That is to say, the code won’t be that different than what they expect to see.\nIf you seek understanding consistently, you’ll find that the person drops their defenses and ego, and instead feels encouraged by your positive approach and attitude. They’ll even start pointing out and suggesting fixes to their code themselves – right on the spot – which is the golden sign of trust and confidence. The best kind of code reviews are the ones that a person points out issues themselves to fix, rather than the team having to do it for them.\nAvoid Opinion Wars When reviewing code, there will be two general categories of issue:\nObjective, fact-based issues Subjective, opinion-based issues Focus explicitly on objective issues, and disregard all subjective issues. An objective issue is an exception or oversight in conforming to well-defined coding standards. A subjective issue is you not liking the way the person solved the problem, or feeling that what they did is not what you would have done. You’re right because you’re different people. Striving to make others conform to your thought process as the one true standard is egotistical, destructive, and stressful. In the long run you will be unhappy because there will always be a gap between others and yourself. This is natural so go with the flow. Allow members of your team to be individuals and write code with their own flair and flavor. It’s OK, it will be OK, and you will be OK. I promise.\nBe Humane Code is for computers, but programmers are humans. Be kind to each other and always remember that the thing on the other end of the code review is a real live person with feelings and emotions. Treat them with respect in what you say and do. Recognize that they are valuable and thank them for their hard work. Together you will create a great team that others will love being a part of!\n","permalink":"https://www.davidhaney.io/dev-team-interactions-conducting-good-code-reviews/","tags":["attitude","behavior","culture","process","workplace"],"title":"Dev Team Interactions: Conducting Good Code Reviews"},{"categories":["blog"],"content":"As a developer working for a company, you probably work on a team. The interactions on these teams are sometimes pleasant, and other times hostile. What’s interesting to me is that a lot of the time, a hostile interaction could have been a pleasant one if only approached differently. Hostile teams are created by the actions of the people on them, not by the situations they encounter. One such hostile action is blame.\nBlame Blame is assigned externally – it comes from other people. You cannot control blame when it is directed at you. Blame tends to surface under tense circumstances, such as when the build has broken or a project has failed. It often rears its ugly head in the form of pithy and simplistic statements. “Jane broke the build again!” or “this is Mohinder’s fault!”\nBlame is often facilitated – if not encouraged – by a poor workplace culture. When the patterns and practices of the management team demand that heads roll when mistakes are made, blame is the obvious solution. It allows you shift the burden of responsibility to someone else, forcing them to defend themselves from an attack while you retreat to safety.\nBlame Never Helps Anything Blame may solve the immediate problem of your head being on the proverbial chopping block, but it never solves the actual problem at hand. If anything it tends to distract from the issue as the discussion changes from “what went wrong” to “who caused this?” A challenging situation becomes even more complicated as strong negative feelings start to emerge.\nAnother effect of blame is that people feel alienated and attacked. Team members disengage, afraid to speak up or make changes because they may be attacked for doing so. In this way it damages the innovation and progression of your people, product, and entire company. Blame holds your entire organization hostage.\nIt’s not hard to see that a workplace which embodies blame culture is really the root problem in-and-of itself.\nAccountability Unlike blame, accountability is assigned internally – it comes from within. It is a positive force of empowerment that requires courage and compassion. Accountability allows people to take control over themselves and their own actions. It destroys “fault” culture while building trust and communication channels with other team members. Statements like “that was my fault team, sorry about that” are welcomed and encouraged, but not necessary.\nAccountability can only thrive if the workplace culture is tolerant and accepting. In a blame culture, accountability cannot exist. The mere act of owning up to a mistake assigns the blame to you, and scrutiny and / or punishment is sure to follow. A good work culture is intolerant of blame, instead focusing on people learning from their mistakes and growing as professionals.\nA Blameless Workplace My personal take on accountability as a manager is that “everybody makes mistakes, and as long as you make new and interesting mistakes – rather than repeating them – you have nothing to worry about.”\nMistakes are normal; we all make them regularly. A workplace culture that understands this fact and incorporates it into professional and personal development is one that succeeds with vibrant and engaged teams. Allow people to make mistakes, and follow-up only if they do not learn and grow from those mistakes. Shut down any and all blame conversations immediately. Communicate the difference between assigning blame to someone for their mistakes, and taking ownership of those mistakes themselves. Foster a workplace culture that is blameless.\nAt Stack Overflow, we do this via a tool that we call the Wheel of Blame. When something goes wrong, anyone in a company chat room can say “not my fault” to spin the wheel. The wheel arbitrarily picks a person from those present in the room and blames them. With blame quickly and definitively assigned, we then move on to fixing the problem.\nThe Wheel of Blame in action. Creating Accountability Despite the Wheel assigning blame – which clearly goes against the blameless culture which I described earlier – this tool is very powerful. It’s a clear and obvious signal to seasoned and new employees alike that we don’t care about blame. When someone is randomly assigned, it’s almost always the case that the problem wasn’t actually their fault. More often than not they had nothing at all to do with the current situation. An ironic and hilarious side effect of the Wheel is that when it does randomly assign the blame to the person who is accountable to the issue, it results in uncontrollable laughter and joking. This alone makes the Wheel worth using: it lightens the mood and communicates our rules around blame in a tribal way (there is no policy written anywhere).\nTo shift a culture from blame to accountability, begin by shutting down blame discussions. Explain that blame is not helpful, and that the team needs to immediately focus on resolving the issue at hand. Once the problem is solved, consider doing a retrospective which explains what went wrong and who was involved. Circulate this retrospective to any and all interested parties with the clear intention of spreading information, not assigning blame. This allows people to learn and grow from what happened. Continuing this pattern will slowly shift the team away from blame. The trick here is that you don’t have to work on accountability: it will naturally follow when people stop fearing blame.\nI encourage you to work towards a culture of accountability on your development team, starting today. Sometimes this transformation can be very difficult to accomplish; particularly when the blame culture starts at the executive level. Nonetheless, do what you can to change the behavior in the part of the company that you influence. Your peers and reports will thank you for it.\n","permalink":"https://www.davidhaney.io/dev-team-interactions-accountability-blame/","tags":["attitude","career","culture","workplace"],"title":"Dev Team Interactions: Accountability \u0026 Blame"},{"categories":["blog"],"content":"If you’re working on an application built using ASP.NET MVC, you’re hopefully aware of the OutputCacheAttribute\rattribute which can be used to statically cache your dynamic web pages. By adding this attribute to a controller or action method, the output of the method(s) will be stored in memory. For example, if your action method renders a view, then the view page will be cached in memory. This cached view page is then available to the application for all subsequent requests (or until the item expires out of the cache), which can retrieve it from the memory rather than redoing the work to re-create the result again. This is the essence of caching: trading memory for performance.\nThe OutputCacheAttribute is a really powerful way to improve performance in your MVC application, but isn’t always the most practical. Because it caches the entire page as raw HTML, it circumvents a large part of the MVC pipeline and thus also skips the code that runs to generate the page. This means that if your view has dynamic content that comes from session or ViewData, such as displaying the currently logged in user’s name in the top bar, or the current time of day, or the resulting view of an invalid form post which tells your user to correct their input errors, you’ll quickly discover the error of your ways when you try to cache that page. When David accesses the logged in page for the first time and caches it, everybody else who logs in will be called David on the page. And if David fills out your empty form and presses submit, only to cache the resulting input validation error page, then everybody will see David’s completed form when they have errors too – maybe even including sensitive data like his username, password, or even his credit card information. I think that most of us have seen this kind of (often humorous) caching error before. It’s scary stuff, nonetheless.\nA great way to balance the benefits of output caching with the dynamic content and features that the modern ASP.NET MVC web application offers is to create a custom caching attribute. This attribute can cache the ActionResult instead of the raw HTML of the page, and in doing so will allow you to cache all of the work that is done to generate the ActionResult (be it ViewResult or otherwise). By executing within the MVC pipeline, this custom caching attribute will not interrupt or short-circuit the MVC pipeline. This allows for things like SessionState or ViewData to vary per cached request! It’s not quite as efficient as the true OutputCacheAttribute, but my custom ActionResultCacheAttribute is an excellent tradeoff between performance and dynamic data:\n/// \u0026lt;summary\u0026gt;\r/// Caches the result of an action method.\r/// NOTE: you'll need refs to System.Web.Mvc and System.Runtime.Caching\r/// \u0026lt;/summary\u0026gt;\r[AttributeUsage(AttributeTargets.Method | AttributeTargets.Class, AllowMultiple = false, Inherited = false)]\rpublic class ActionResultCacheAttribute : ActionFilterAttribute\r{\rprivate static readonly Dictionary\u0026lt;string, string[]\u0026gt; _varyByParamsSplitCache = new Dictionary\u0026lt;string, string[]\u0026gt;();\rprivate static readonly ReaderWriterLockSlim _lock = new ReaderWriterLockSlim();\rprivate static readonly MemoryCache _cache = new MemoryCache(\u0026quot;ActionResultCacheAttribute\u0026quot;);\r/// \u0026lt;summary\u0026gt;\r/// The comma separated parameters to vary the caching by.\r/// \u0026lt;/summary\u0026gt;\rpublic string VaryByParam { get; set; }\r/// \u0026lt;summary\u0026gt;\r/// The sliding expiration, in seconds.\r/// \u0026lt;/summary\u0026gt;\rpublic int SlidingExpiration { get; set; }\r/// \u0026lt;summary\u0026gt;\r/// The duration to cache before expiration, in seconds.\r/// \u0026lt;/summary\u0026gt;\rpublic int Duration { get; set; }\r/// \u0026lt;summary\u0026gt;\r/// Occurs when an action is executing.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;param name=\u0026quot;filterContext\u0026quot;\u0026gt;The filter context.\u0026lt;/param\u0026gt;\rpublic override void OnActionExecuting(ActionExecutingContext filterContext)\r{\r// Create the cache key\rvar cacheKey = CreateCacheKey(filterContext.RouteData.Values, filterContext.ActionParameters);\r// Try and get the action method result from cache\rvar result = _cache.Get(cacheKey) as ActionResult;\rif (result != null)\r{\r// Set the result\rfilterContext.Result = result;\rreturn;\r}\r// Store to HttpContext Items\rfilterContext.HttpContext.Items[\u0026quot;__actionresultcacheattribute_cachekey\u0026quot;] = cacheKey;\r}\r/// \u0026lt;summary\u0026gt;\r/// Occurs when an action has executed.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;param name=\u0026quot;filterContext\u0026quot;\u0026gt;The filter context.\u0026lt;/param\u0026gt;\rpublic override void OnActionExecuted(ActionExecutedContext filterContext)\r{\r// Don't cache errors\rif (filterContext.Exception != null)\r{\rreturn;\r}\r// Get the cache key from HttpContext Items\rvar cacheKey = filterContext.HttpContext.Items[\u0026quot;__actionresultcacheattribute_cachekey\u0026quot;] as string;\rif (string.IsNullOrWhiteSpace(cacheKey))\r{\rreturn;\r}\r// Cache the result of the action method\rif (SlidingExpiration != 0)\r{\r_cache.Add(cacheKey, filterContext.Result, TimeSpan.FromSeconds(SlidingExpiration));\rreturn;\r}\rif (Duration != 0)\r{\r_cache.Add(cacheKey, filterContext.Result, DateTime.UtcNow.AddSeconds(Duration));\rreturn;\r}\r// Default to 1 hour\r_cache.Add(cacheKey, filterContext.Result, DateTime.UtcNow.AddSeconds(60 * 60));\r}\r/// \u0026lt;summary\u0026gt;\r/// Creates the cache key.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;param name=\u0026quot;routeValues\u0026quot;\u0026gt;The route values.\u0026lt;/param\u0026gt;\r/// \u0026lt;returns\u0026gt;The cache key.\u0026lt;/returns\u0026gt;\rprivate string CreateCacheKey(RouteValueDictionary routeValues, IDictionary\u0026lt;string, object\u0026gt; actionParameters)\r{\r// Create the cache key prefix as the controller and action method\rvar sb = new StringBuilder(routeValues[\u0026quot;controller\u0026quot;].ToString());\rsb.Append(\u0026quot;_\u0026quot;).Append(routeValues[\u0026quot;action\u0026quot;].ToString());\rif (string.IsNullOrWhiteSpace(VaryByParam))\r{\rreturn sb.ToString();\r}\r// Append the cache key from the vary by parameters\robject varyByParamObject = null;\rstring[] varyByParamsSplit = null;\rbool gotValue = false;\r_lock.EnterReadLock();\rtry\r{\rgotValue = _varyByParamsSplitCache.TryGetValue(VaryByParam, out varyByParamsSplit);\r}\rfinally\r{\r_lock.ExitReadLock();\r}\rif (!gotValue)\r{\r_lock.EnterWriteLock();\rtry\r{\rvaryByParamsSplit = VaryByParam.Split(new[] { ',', ' ' }, StringSplitOptions.RemoveEmptyEntries);\r_varyByParamsSplitCache[VaryByParam] = varyByParamsSplit;\r}\rfinally\r{\r_lock.ExitWriteLock();\r}\r}\rforeach (var varyByParam in varyByParamsSplit)\r{\r// Skip invalid parameters\rif (!actionParameters.TryGetValue(varyByParam, out varyByParamObject))\r{\rcontinue;\r}\r// Sometimes a parameter will be null\rif (varyByParamObject == null)\r{\rcontinue;\r}\rsb.Append(\u0026quot;_\u0026quot;).Append(varyByParamObject.ToString());\r}\rreturn sb.ToString();\r}\r}\rYou can use this method on a controller to affect all action methods:\n[ActionResultCache(Duration = 60 * 60 * 24)]\rpublic class HomeController : Controller\r{\rpublic async Task\u0026lt;ActionResult\u0026gt; TermsOfService()\r{\rreturn View();\r}\r}\rOr just apply it to individual action methods:\n[ActionResultCache(Duration = 60 * 60 * 24)]\rpublic async Task\u0026lt;ActionResult\u0026gt; TermsOfService()\r{\rreturn View();\r}\rYou can also use it with the VaryByParam property to vary the cached result by the parameter(s) of the action method:\n[ActionResultCache(Duration = 60 * 60 * 24, VaryByParam = \u0026quot;username\u0026quot;)]\rpublic async Task\u0026lt;ActionResult\u0026gt; ViewUser(string username)\r{\rvar model = new UserModel\r{\rUsername = username,\r...\r};\rreturn View(model);\r}\rThe main benefit of this custom caching attribute is that your session state and all global action filter attributes, etc. still run in the MVC pipeline as they would normally. The only code cached and skipped over is the method body of the action method.\nPlease use and enjoy! Feedback welcomed in the comments.\n","permalink":"https://www.davidhaney.io/custom-asp-net-mvc-action-result-cache-attribute/","tags":["dotnet","caching","mvc","performance","programming"],"title":"Custom ASP.NET MVC Action Result Cache Attribute"},{"categories":["blog"],"content":"Well, I’ve utterly failed to blog at regular intervals, writing only\rthree\rposts\rin 2016. Ouch. To be fair, one of those posts is insanely famous (the one about NPM and left-pad.js), but still, I’ve really let my readers – and myself – down.\nSo, I resolve to write a blog post every single week of 2017, starting today. This will probably mean that I write slightly shorter posts, and maybe even multi-part series posts. My traditional style has been “come upon something that is really bothering me or is really tricky, and proceed to blog about it in great detail writing thousands of words for all to benefit from” which doesn’t really scale well. Instead I plan to take the approach of “write about a new or interesting topic each week, and see what people like and what they don’t like” which will hopefully be better.\nThis week’s post is about compassion, especially in the field of programming. Let’s use an example that is both recent and practical, if a tad emotional.\nMy dog died 3 days ago our dog Ruby died. She was 16 and lived a good, long life. Having said that, her death was unexpected and rather painful to go through. My wife and I were up with Ruby all night comforting her as she slowly passed away from sudden heart failure. The last breath she exhaled will forever be seared into my brain. It feels like it’s all I can think about right now.\nMy wife and I have been grieving for the past 72 hours in our own ways. This has surfaced as mostly a mix of depression, tears, quick but sad chats about the things we miss about Ruby, and sorrow every time we look somewhere in the house for her only to find that she is no longer there. It sucks.\nOut of sight, out of mind What’s important to realize is that I am not unique or special in this circumstance. Animals die all of the time, and you don’t feel sad about them because you never met them. They are out of sight, out of mind. Yet they are out there, and the passing of good dogs and cats – as well as other households pets – is happening every single day. Even as you read this.\nThese events occur in the scope of individual’s lives. There is no national news story about the passing of Ruby to make us all aware of it and help us all feel the effects of it. There never will be. This event is confined to the lives of me, my wife, and our extended family. We suffer through it silently (other than my writing about it and a few social media posts).\nSilent suffering This silent suffering is the point that I want to drive home. Others out there are silently suffering too. It might be the death of a family pet, or it might be something else entirely. It’s not a contest of who has it the worst. I want you to simply realize that crappy things are happening to people, even if they are not in your domain and thus you are not aware.\nOdds are that someone in your life or workplace is suffering with something that troubles them. Right now for me it’s the passing of my dog. For a close friend of mine it’s stress about work and finding the next permanent job. For another person I know it’s about being a brown man in America and how scared they feel about the future. We all battle demons every single day.\nYou work with people With this in mind, I ask you to be compassionate everywhere, but especially at work. You work with people, not cogs of some machine (even if management may think of them in that way). These people have real feelings and some of them are surely fighting battles you’ve never even fathomed.\nYour responsibility as a good member of the IT field – and as a good citizen of this planet – is to demonstrate compassion. Be nice to everyone you work with until you are given a reason not to be. Consider that they might be having a bad day if you feel they’re lashing out or being an asshole, and forgive them. Try to sympathize with what frustrates them. Work to understand them beyond their role at the company and their job output.\nWe are all in this together. Now more than every we need to be compassionate, respectful, kind, and considerate to others. It is my opinion that our industry has a long history of being hostile to certain people and demographics – such as women – and we must each work individually to change that.\nMy last request to you: please think hard about what you’ve read here as you begin your first week of work in 2017. I know I will.\n","permalink":"https://www.davidhaney.io/our-industry-needs-compassion/","tags":["attitude","behavior","culture","workplace"],"title":"Our Industry Needs Compassion"},{"categories":["blog"],"content":"On Plumbers Picture this situation: you woke up this morning to find that there’s no water coming through your valves and taps. No sink water. No shower water. Having no plumbing experience, you call around for a plumber.\nPlumber #1 Plumber #1, let’s call him Mario, tells you he can’t be bothered to come check out your issue because it’s minor and he’s very important and too busy for it. You explain that you really need a plumber, and he explains he’ll do it for 1.5x what everybody else costs, and only if you have lunch and coffee ready for him when he arrives. You have no water, keep in mind, so making coffee is an extra special effort.\nPlumber #2 Plumber #2, let’s call him Luigi, agrees to show up and assess the damage. He walks in your door, doesn’t bother to shake your hand or introduce himself, and immediately gets to work. A few minutes of inspection later and Luigi is telling you that all of your plumbing is wrong. None of the pipes are plumbed how he’d plumb them, so they’re wrong. He needs to rip out the whole plumbing system and do it all again from scratch, which will cost you thousands of dollars. He has a reputation for being skilled: his past references love him, but this is going to cost a lot of money and take a lot of time. It’ll be at least a week before you have running water again.\nPlumber #3 Plumber #3, let’s call her Peach, asks you some detailed questions about the problem over the phone. “How long have you lived in the home?” “Have you had any prior plumbing issues?” “Has this happened before?” “Do you have any other sources of water?” Based on this analysis, she tells you she has a good idea of what the issue is and agrees to come over later that day, since she’s in the area anyway. She shows up, immediately discovers that the pesky neighbour kids shut off your water main, turns it back on, and then gives you some advice: “it’s not something you need to do right now, but you should upgrade to a locked main switch that operates with a key so that this won’t happen again! Call me if you want something like that installed.”\nNow, which one would you hire?\nBack to Programming Let’s take this [terrible] analogy back to programming. Plumber #1 is a rock star developer who demands special treatment and pampering. Plumber #2 has a huge ego, a serious case of not-invented-here syndrome\r(they didn’t plumb the house so it’s wrong!), and might also be a rock star. Plumber #3 is pragmatic, friendly, sensible, analytical, and balances cost of time and money with urgency. For me personally, #3 is the only one I’d happily hire.\nIt seems so obvious that egos and not-invented-here cost a lot of money and do a lot of damage in that analogy. It’s less evident in the workplace. Some companies foster the ego and demand to hire rock stars, believing that the “ten ex” developer will outperform others they could hire. What they don’t realize is that the cost of this hire almost certainly outweighs the benefits.\nThere’s no place for ego in programming. An egotistical programmer does incredible amounts of damage to those around them. They do cultural damage to your company, emotional damage to their peers, and financial damage to your budgets. Those rewrites aren’t cheap!\nDon’t be a rock star. Rock stars are cold, hard to approach, notoriously unfriendly and self-absorbed, make no time for others and teaching them (because everything’s about ME!), and are frequently dramatic and rude (to get in the “news” – after all, any publicity is good publicity).\nI don’t know about you, but I want to work with Peach. I prefer a humble, thoughtful, talented peer that can teach me while maintaining respect and a relationship of trust. That’s the kind of workplace that I get up every morning excited to go to. Peach is probably not perfect, and has her flaws, but the lack of an ego and attitude means she’s willing to learn and grow – and that’s the key to success in this industry.\nIntrospection If this analogy hit a little close to home, it’s time for a little introspection. The beauty of the human condition is that we are always on a path, growing and changing. Who you are today is not who you are forever. Think about which plumber you resemble above, and then adjust. Next time you’re about to say “that’s all wrong!” or “we have to write it ourselves!”, pause and think: there’s probably a decent a reason that the code exists in its current state. Is now really the time to undertake a very expensive rewrite / greenfield project? Probably not.\n","permalink":"https://www.davidhaney.io/lets-talk-about-rock-stars-egos/","tags":["attitude","behavior","culture","ego","workplace"],"title":"Let's Talk About Rock Stars \u0026 Egos"},{"categories":["blog"],"content":"Intro Okay developers, time to have a serious talk. As you are probably already aware, this week React, Babel, and a bunch of other high-profile packages on NPM broke. The reason they broke is rather astounding:\nA simple NPM package called left-pad\rthat was a dependency of their code.\nleft-pad, at the time of writing this, has 11 stars on GitHub\r. The entire package is 11 simple lines that implement a basic left-pad string function\r. In case those links ever die, here is the entire code of the left-pad package:\nmodule.exports = leftpad;\rfunction leftpad (str, len, ch) {\rstr = String(str);\rvar i = -1;\rif (!ch \u0026amp;\u0026amp; ch !== 0) ch = ' ';\rlen = len - str.length;\rwhile (++i \u0026lt; len) {\rstr = ch + str;\r}\rreturn str;\r}\rWhat concerns me here is that so many packages and projects took on a dependency for a simple left padding string function, rather than their developers taking 2 minutes to write such a basic function themselves.\nAs a result of learning about the left-pad disaster, I started investigating the NPM ecosystem. Here are some of the things that I observed:\nThere’s a package called isArray\rthat has 880,000 downloads a day, and 18 million downloads in February of 2016. It has 72 dependent NPM packages. Here’s its entire 1 line of code:\nreturn toString.call(arr) == '[object Array]';\nThere’s a package called is-positive-integer\r(GitHub\r) that is 4 lines long\rand as of yesterday required 3 dependencies to use. The author has since refactored it to require 0 dependencies, but I have to wonder why it wasn’t that way in the first place.\nA fresh install of the Babel\rpackage includes 41,000 files\nA blank jspm/npm-based app template now starts with 28,000+ files\nAll of this leads me to wonder…\nHave We Forgotten How To Program? On what possible plane of existence is this a better solution to past problems? How are hundreds of dependencies and 28,000 files for a blank project template anything but overly complicated and insane?\nI get the impression that the NPM ecosystem participants have created a penchant for micro-packages. Rather than write any functions or code, it seems that they prefer to depend on something that someone else has written. It feels to me as if the entire job of an NPM-participating developer is writing the smallest amount of code possible to string existing library calls together in order to create something new that functions uniquely for their personal or business need.\nFunctions Are Not Packages Functions are too small to make into a package and dependency. Pure functions don’t have cohesion; they are random snippets of code and nothing more. Who really wants a “cosine” dependency? We’d all really like a “trigonometry” dependency instead which encompasses many “tricky” functions that we don’t want to have to write ourselves. This is much more akin to how .NET and other frameworks create a “core” library of basic functionality. Such a library is vetted by the creators of the language and pretty much guaranteed to be correct and bug-free.\nThird Party Problems There’s absolutely no guarantee that what someone else has written is correct, or even works well. Even if correct, is it the most optimal solution possible? At least when you write the code yourself, you can easily modify it to fix bugs and improve its efficiency. Not that there should be many bugs in 1 line functions.\nSecond, even if the package’s logic is correct, I can’t help but be amazed by the fact that developers are taking on dependencies for single line functions that they should be able to write with their eyes closed. In my opinion, if you cannot write a left-pad, is-positive-integer, or isArray function in 5 minutes flat (including the time you spend Googling), then you don’t actually know how to code. Any of these would make a great code screening interview question to determine whether or not a candidate can code.\nFinally, stringing APIs together and calling it programming doesn’t make it programming. It’s some crazy form of dependency hacking that involves the cloud, over-engineering things, and complexity far beyond what’s actually needed to create great applications.\nWhat’s worse is that if any of your code (or the 3rd party dependency code) has a bug or breaks, you won’t know how to debug or fix it if you don’t know how to program.\nStrive For Few Dependencies Every package that you use adds yet another dependency to your project. Dependencies, by their very name, are things you need in order for your code to function. The more dependencies you take on, the more points of failure you have. Not to mention the more chance for error: have you vetted any of the programmers who have written these functions that you depend on daily?\nTake on a dependency for any complex functionality that would take a lot of time, money, and/or debugging to write yourself. Things like a database access layer (ORM) or caching client should be dependencies because they’re complicated and the risk of the dependency is well worth the savings and efficiency.\nBut, for the love of all that is programming, write your own bloody basic programming functions. Taking on dependencies for these one-liners is just nuts. Don’t believe me? Just ask the React team how well their week has been going, and whether they wish they had written those 11 lines for left-padding a string themselves.\nFollow David Haney on Twitter at @haneytron\rUpdated Jan 13 2017 with some minor grammar and sentence structure changes.\n","permalink":"https://www.davidhaney.io/npm-left-pad-have-we-forgotten-how-to-program/","tags":["left-pad","npm","packages","programming"],"title":"NPM \u0026 left-pad: Have We Forgotten How To Program?"},{"categories":["blog"],"content":"Are Developers Good Negotiators? Developers come from all walks of life, and have many unique interests, passions, and hobbies. Often the only thing that developers have in common is their love for programming. It follows that some are good negotiators; others get the double digit percentage finance rate at the dealership when they go in to buy that new car.\nHow Does Your Company Determine Compensation? When you hire developers, how do you decide on their salary? Do you allow for negotiations to take place? Is there a strategy in place where you offer a low value, expecting the candidate to counter with a higher number? Are you pleased when they don’t counter, and you get good talent for cheap?\nThe thing is, developers are the linchpin in your tech company. They make or break your products – quite literally, in fact. They’re worth a lot of money. You should be paying them what they’re worth as one of many strategies to keep them happy. If you’re low-balling developers with a salary strategy that rewards negotiation skills, you’re probably underpaying them while overpaying the developers who are good negotiators (but maybe not amazing coders).\nYour underpaid talent might not feel comfortable asking for a raise to the income that they are worth. Can you guess what happens then? They leave your company for another that values them correctly. The result is that your department has high turnover, lots of churn, and high costs around replacing the fleeting talent.\nStack Ranking: Upsetting Developers Everywhere One of the most common ways to compensate employees is by employing a stack ranking system\r. There are varying approaches to stack ranking, but a typical implementation of a stack rank system is as follows:\nA company employs a ranking system, often a scale of 1 to 5, to assess employees. These numbers often come with generic and impersonal descriptions. The scores themselves are bucketed with percentages that limit the number of employees that may receive a given grade: 1: Exceptional (10% of employees) 2: Highly Effective (20%) 3: Consistently Strong (40%) 4: Partially Effective (20%) 5: Not Effective (10%) Managers are given a fixed $X in raises to hand out for annual reviews. The $X budget for raises is distributed per the grading, with the 1’s getting the biggest chunks, and 5’s often getting nothing. Note that this is how stack ranking is used to control budget costs. You always know exactly how much money is being given out in raises. The 4’s are warned that they’re on the way to being fired if they don’t shape up. Typically, the 5’s get fired or put on a performance plan. This system – also known by the endearing terms “rank and yank,” “forced distribution,” and “grading on a curve” – is popular because it control costs, both in terms of annual raises and also under-performing employees. It serves as a system that forces the bottom 10% (or whatever the bucket is set at) out of your company regularly. This is not a bad thing in-and-of itself, assuming that the replacements hired are any better. Of course, this is where one of the major problems becomes evident.\nWhy Stack Ranking Sucks Here’s the thing: someone always has to be a 5. This system is built on the false assumption that there’s always someone who is Not Effective on your team. It institutionalizes the idea of mandatory mediocrity.\nIt is easy to see how ridiculous this concept is when you apply it to objects instead of people. For example, let’s review 5 cars with stack ranking: a Ferrari, a Lamborghini, a Maserati, a Porsche, and Rolls Royce. Which one is the exceptional one? Which two are under-performing and mediocre? In this all-star set, they’re all great, but stack ranking demands that one is worse than the others by a large margin.\nWhat happens if you employ good hiring practices and recruit a team of 10 amazing developers? What if they’re all 2’s and 3’s, but you have to give out 4’s and 5’s? You end up having a difficult annual review where you find yourself apologizing and telling your developer how great you think they are. Because you really do think that. But your words are hollow, and their raise and review are the actions that speak louder; at this company they are thought of as mediocre, because someone has to be.\nHave you ever had a review where the actions of your manager didn’t match their words? You’re being told what an all-star and amazing player you are on the team, how important and awesome you are, and how everything you touch turns to gold, but your review says “you’re average” and that big fat 3 rating is searing itself into your brain. You’re wondering “if my boss thinks I’m so great, why is my rating average?” That’s what stack ranking gets you. This review probably upset you, and now you’re contemplating your options. Not a great outcome for you or the company.\nStack ranking also stifles the desire of your developers to try new things, take on new roles with more responsibility, and take risks to grow their careers. This is because a 1 or 2 player won’t want to take on the risk of joining a new team, or getting a new boss, who might rank them as a 3 or lower compared to their more seasoned colleagues. Indeed, the smart play is to stay right where they are, and reap the benefits of being on the good end of this ridiculous bell curve.\nMy biggest concern with stack ranking is the fact that compensation is relative. Your assessed performance depends entirely upon the performance of your peers, as subjectively assessed by your manager. You might have been a 1 if it weren’t for that person who happened to claim it. Now the best you can be is a 2. But if they didn’t work there, perhaps you’d be a 1. Doesn’t seem fair (or even rational), does it?\nMy Personal Experience With Stack Ranking I have had a few jobs in the past that employed stack ranking. At one of them, developers picked 3 projects to be assessed on, and then both they and their manager ranked their performance on the projects from 1 to 5. One of the projects that I picked was something that I had built from scratch with a team of 4 people. The thing we built had made the company millions of dollars that year, and I was the lead on it. Naturally, I gave myself a 1 on it. My manager gave me a 3.\nI asked him how I could possibly be average at the thing which I created, was the most experienced with at the company, and which had led to millions in additional revenue. His reply was that I was awesome and not to worry too much about the grade. It was very confusing; what he said didn’t match what he wrote.\nThe review continued, and I ended up being given a 3 out of 5. I got a small raise. This was all conveyed to me as my boss happily told me that I was awesome, to keep up the great work, and that to keep it between me and him but I got the biggest raise on the team.\nThe idea of having the biggest raise made me feel less wronged… Right up until I found out that it was a lie. My team and I were at lunch a few days later when one person bragged that he got the biggest raise on the team. Another immediately said “what? I was told that I did.” I began to laugh as I realized what my manager had done. He had told us all that we received the biggest raise, and to keep it to ourselves. Perhaps as damage control for the pain that the mediocre grades had inflicted, but unfortunately for him we talked to each other. The jig was up, and now most of us were madder than we would have been had he said nothing to us at all.\nA side note: leaving that job was one of my favorite resignations. When I quit, my boss was distraught, but I said (paraphrased) “hey [boss], don’t worry about it! I’m a 3 out of 5, so you should have no issues hiring any other average developer to take up the work I was doing.” The look on his face told me all I needed to know: he had now realized for himself why stack ranking sucks.\nHow to Properly Review \u0026amp; Compensate Developers The key to happy developers is fair compensation. Fair compensation is all about transparency. At Stack Overflow, we have a transparent system for assessing employee skills and compensation, which is lovingly called Be More Awesome. There is no magic in employee compensation here, and all developers know exactly what they’re getting paid, why, and what they could get paid in the future. There are no negotiations, no bell curves, and no quotas.\nBe More Awesome (BMA) is pretty simple. It is meant to measure skills; we use performance as evidence of skill. A person may rate highly on their BMA for a skill, even if they haven’t used it in the previous year. There are 4 possible grades that can be awarded for each skill:\nB: Could be more awesome. This is a good thing to work on over the next year — and your manager will help. This can also be applied for new employees who haven’t been in the company long enough to demonstrate that they deserve a higher grade. A: Does as expected, at our high Stack standards. Completely, utterly able to accomplish what is needed. A+: Does more than your team expects, even at our high level. Exceptional and noticeable skills. A+++: Widely recognized level of amazingness. Does and teaches. When people think of this skill, they think of you (or would if they knew you). This will be rare, even on our amazing team. You might notice that there is no grade below a B. We don’t have C’s or lower because we believe that everyone that works here is awesome. If one of our developers were doing C, D, or F level work, we would already be working closely with them to correct it – prior to reviews.\nIt’s also not uncommon for people who are earlier in their programming career to receive a lot a B’s. This doesn’t mean they’re doing poorly at all. It just means they’re closer to the beginning of their career than the middle and there’s a lot of opportunity for them to grow.\nThe actual skills that we assess each year change, but the 2016 BMA chart for developers currently looks like this:\n2016 Developer BMA There are descriptions that explain what each of these categories are, and they are available for all employees to review at any time.\nOnce a developer is assessed on a BMA, their letter grades get converted into a numeric score by using a formula that is also published internally for all developers to review. This formula outputs a numeric score between 0.00 and 5.99 (with 5.99 being the best grade), which is then rounded down to the nearest whole number. In short, a developer can receive a score of 0, 1, 2, 3, 4, or 5.\nNext we assess the years of programming experience. This is a value that falls in a range from 0 to 25. Naturally, this goes up by 1 at every annual review.\nThe score and years of experience are then looked up on a chart that has years of experience on the X axis and score on the Y axis, and details salary amounts at each cell. It’s worth redundantly noting that this chart is published internally also and can be reviewed by all developers at any time. Unsurprisingly, the cell that your score and experience points to is exactly what you get paid.\nMake Compensation Transparent There are no secrets or magic in our compensation system. All aspects of it are published internally for all developers to review at any time. They also get input into the changes to the BMA skills each year, well in advance of their annual review. They know the formula that we use to calculate salary. Most importantly, their compensation doesn’t depend on the performance of anyone else. Everybody can be a 5 in our system and everybody can be a 0.\nAbove all else, our system is fair and evaluates individual performance, not team performance. If you want happy developers and low turnover, I highly encourage you to try adopting such a system yourself. If your company is unwilling to do so, perhaps evaluate why. Are there secrets and magic in the compensation system that you don’t want your employees to know about? Why do you value these hidden metrics? Do your employees feel valued?\nA happy developer is a productive developer, and while a fair system does not allow you to easily control salary costs in terms of budget (because everybody can be a 5), it does help increase job satisfaction, lower turnover, and maintain a relationship of trust with managers. And as I’ve written about before\r, if you don’t have the trust of your employees, you will fail.\nEDIT (7/27/2016): We have published a salary calculator based on our internally transparent compensation numbers! Take a look\r.\nPS – hate stack ranking but love Stack Overflow? Come and work with us!\rFollow David Haney on Twitter at @haneytron\r","permalink":"https://www.davidhaney.io/developer-compensation-stack-overflow-doesnt-stack-rank/","tags":["compensation","salary","stack overflow","transparency"],"title":"Developer Compensation: Stack Overflow Doesn't Stack Rank"},{"categories":["blog"],"content":"Background I’m going to discuss an important topic that affects everybody in tech: diversity.\nNo, this won’t be some preachy post about how diversity is great and how you should be a better human being. Rather, I’m going to tell you about the things I’ve experienced working on diversity – particularly the interesting events of the last few days that happened internally at Stack Overflow.\nIt’s no secret that the tech industry is not that diverse. It’s mostly dominated by white males, with a few women and minorities making appearances. Those who do enter the industry as a minority often feel marginalized and excluded.\nOf about 40 engineering employees at Stack Overflow, maybe 8 are not white men. We can clearly do better.\nAs you may have learned in my last post\r, earlier this year I was promoted to Engineering Manager at Stack Overflow (formerly Stack Exchange (formerly Stack Overflow)). One of my first projects was to work on diversity at our company. As a part of this goal, I was given the opportunity to create a new diversity page to showcase our efforts.\nThere’s a volunteer group that we have internally called the Diversity \u0026amp; Inclusion Panel (DIP). This group works to make Stack Overflow a more open, diverse, and welcoming environment.\nThere were some awkward feelings that came with taking the lead on the diversity page project. For one, I was a straight white male running a project about diversity, which might raise some eyebrows or make people feel weird/jaded. To mitigate those feelings, and also because I value their opinions, I took the approach of including the DIP in the creation and review of the page. My rationale was that by including a group of self-selected folks who are passionate about diversity, we could reach a solid page design that wasn’t biased by building it myself. To this end the DIP were consulted multiple times and ultimately had the final say on the copy of the page.\nDiversity Page Goes Live After a few months of iterating with designers and the DIP, last week we rolled out the new page. Here’s what it looked like in all its glory (please don’t critique it in the comments – if you do, you missed the point of this post):\nOur Shiny New Diversity Page I was excited that we had made progress on such an important and sensitive topic. I announced the roll-out to the DIP group with little feedback or response, which at the time I didn’t think much of. I also broadcast it on Twitter, with very few others from Stack Overflow doing as such themselves. Again, I thought nothing of the lack of engagement.\nA few days go by with the new page rolled out where things are mostly business-as-usual from my perspective. And then, last weekend, something really interesting happened.\nThe Blog Post An employee at our company wrote a blog post titled “Discussing diversity terrifies me” on their personal blog. They have since taken the post down – a decision they made themselves. The post described this person’s feelings about our new page and how it made them feel uncomfortable in ways that they couldn’t fully explain. This person also explained how they felt terrified to discuss diversity with anyone, ever – even their best friends. This was because of prior experiences at prior companies where the discussion went poorly.\nI came across the post on Saturday night at about 9pm. It did not call out me or the page as doing something wrong; it was a way for this person to express some thoughts that were very important to them, but that they otherwise felt they could not bring up.\nI Felt Wronged Upon reading it, I was really pissed off. I couldn’t believe that this person, given numerous opportunities to shape the page as part of the DIP, had remained silent during the project. I couldn’t believe that they didn’t come to me or anyone else at work, and instead took to talking about their issues on their blog, in public, which at the time I felt served to embarrass me in front of the company. After all, they were given tons of opportunities to talk about it internally, so why didn’t they? I felt slighted and wronged. I selfishly felt that this was all a personal attack on my work. I wasn’t happy.\nThe Company-Wide Revelation On Monday afternoon I had a private conversation with the person who wrote the blog post. We had a really constructive chat about how the person felt, and ways to improve upon the new page and make it more sincere and reflective of our true values and current workplace diversity. It ended on a positive note. Later Monday evening, I sent out an anonymous survey to solicit broad feedback from folks on how to improve the page. The plan was to iterate to a better version ASAP. Satisfied with trying to improve the situation, I went to bed.\nOn Tuesday I found myself in a few conversations about our new page. The people engaging me were mostly straight white males (not unlike myself) who all said something similar: “talking about diversity scares me to death” and “it feels like one wrong word and I’m fired.” These conversations continued until about 1pm, when a forward-thinking co-worker realized that we all needed to get on the same page. They brought the diversity page controversy up in our company-wide chat room for literally anyone and everyone to discuss. Many people jumped in and what followed, for lack of better description, was quite an epic conversation. It involved a lot of people from a lot of departments – both people who were visibly diverse and those who were not – and even included 4 executives. The conversation rolled on for a few hours and culminated in a few important outcomes:\nThe person who wrote the blog post felt very uncomfortable talking about diversity at all, with anyone Their blog post gave a voice to many others who felt the same way but were afraid to speak up The diversity page was making people within Stack Overflow uncomfortable As a result of the company-wide chat, the new diversity page was taken offline.\nI Felt Even More Wronged Conveniently, all of this discussion happened while I was at the doctor’s office for a check-up. As a result of the timing I didn’t get to participate at all. When I returned from the doctor and read the chat transcript, I became very, very upset and angry. I felt betrayed by the company and wronged by almost everybody who participated in the conversation. In hindsight, I think that I probably would have said some things that I’d now regret had I been around to participate in the chat. Ignorance is sometimes bliss.\nI was boiling over. Many unfair, selfish, and angry thoughts crossed my mind as I read the chat transcript. I thought things like “f*** it, I’m never touching another diversity project ever again” and “how could these people do this to me?” and even, for a brief moment, “what am I doing working somewhere where I’m undermined and treated unfairly?”\nI called it quits a few hours early on Tuesday and had a few drinks of the alcoholic nature. I was fuming, and ranting to my wife, and otherwise upset about how I had been so wronged by the people that I was trying to help. I took some ibuprofen (for the impending hangover and headache) and passed out, unsure of what Wednesday would bring.\nWorth noting, I don’t recommend self-medicating with alcohol when you’re upset – it’s a bad strategy.\nThis Wasn’t About Me At All I awoke Wednesday morning with a clear mind and new perspective on the events of the past few days. I realized some really important things:\nAbsolutely, literally, none of this was about me. I was being selfish and making it about how I felt, when in reality it was about how we all felt. We had created a diversity page that did a poor job of accomplishing our goals on diversity. Taking the page offline was the right thing to do, because it was making a lot of people in our company uncomfortable. If it had this effect internally, it surely had a similar effect externally. In this case, the page existing was perhaps worse than having no page at all. The person who wrote the blog post was completely in the right, and arguably did the right thing. The alternative was that if they hadn’t spoken up on their blog, nobody may have. This would leave everyone silently feeling uncomfortable. The person who wrote the blog post felt uncomfortable speaking up about diversity at work. They talked about it on their personal blog to keep it outside of the company. This was an important revelation as it showed me that we have a serious issue at our company: people feel as if they don’t always have the right to speak up about difficult subjects like diversity. One person described it as “a manager’s word being law and difficult to contest.” Nobody had been malicious at all. We were all trying to do the right thing, assumed the best of intentions in each other, and were simply expressing our honest feelings on the topic. Feelings are always valid. Diversity At Stack Overflow Through all of this I realized some of why diversity is such a hard topic for people to discuss: no matter what position you are in, you probably feel like you’re not entitled to participate in the conversation. When nobody feels like they’re able to talk, silence soon becomes apathy.\nOn the topic of diversity issues, apathy rules all. It’s a safe and easy play to do nothing at all. Taking a stance means meeting resistance and having conversations where everyone feels invalidated. Unfortunately, doing nothing enables and furthers the issue.\nOne of the mistakes that I made was not considering the emotional response of the new diversity page that we created. How the page makes people feel is the only measure of success. The people in our company felt that the page was insincere and made them uncomfortable.\nHere’s the truth: despite what the page said, we’re not great at diversity. We have about 40 engineering team members, and only 4 of them are women. At most, 8 or 9 total are of a visible minority. We certainly have a diverse team in the sense that we’re located all over the world, but that isn’t how everybody defines diversity, and maybe isn’t even the most important form of diversity since those geographically diverse people aren’t necessarily marginalized.\nIt’s really easy to feel as if your company sucks at diversity when any discussion is met with frustration and hurt feelings. However, I am really proud of the fact that at Stack Overflow we are able to have these conversations. We had a company-wide discussion on how the page made everybody feel, and even though that discussion got extremely heated and many people felt many strong emotions, not one person made it personal. There were 4 executives in the conversation and nobody quit or got fired.\nI’m thankful that one person had the courage to write a blog post and speak up about their overarching fear of discussing diversity. What scares me as a manager is the idea that people might feel as if there’s no channel to talk about things that make them uncomfortable. If people don’t express their concerns, they suffer silently. It was the actions of one person that saved us all from a much worse outcome.\nUltimately, we made a company diversity page that’s like every other company’s diversity page, at a company that prides itself in not being like any other company. We’ve identified that people at Stack Overflow sometimes feel like they are unable to speak up on difficult topics – both inside and outside of the company – and that’s something that we need to work on.\nWhat’s Next As I said earlier, the easy and safe play is to do nothing. We could leave things in their current state of no longer having a diversity page, but it would also be the wrong thing to do.\nWhat we must do now is continue the conversation. We need to talk honestly about diversity and discuss the feelings that we have around it. We need to channel those feelings into a better diversity page that accurately reflects our company and how we feel on the subject. We need to admit that diversity is hard, but that we’re working on it. Most importantly, we need to assume the best intentions of each other and make progress together in all of our diversity initiatives.\nI hope that in reading this, you feel inspired to start or continue the conversation about diversity in your workplace. Like many companies, we have a long way to go. It’s challenging, but as a co-worker said, “the hardest conversations are often the ones most worth having.”\n","permalink":"https://www.davidhaney.io/diversity-is-really-freaking-hard/","tags":["conflict","culture","diversity","mistakes","workplace"],"title":"Diversity Is Really Freaking Hard"},{"categories":["blog"],"content":"In February of 2015, I was promoted to Engineering Manager at Stack Overflow. This has made a lot of people very angry and been widely regarded as a bad move\r.\nThere are tons of things I’ve learned so far, some of which I’ve learned the hard way. There’s also a world of difference between managing code, and managing people who code. Your day to day work routine changes completely. You define success differently. You feel a little bit like you just rebooted your career and are starting over at the bottom of the skills ladder. It’s intimidating.\nI’m going to discuss my experiences and insights over the last 6 months as a new manager. This one goes out to all of the developers out there who wonder what it’s like to be a manager, or are considering taking the leap into the realm of Pointy Haired Boss\r.\nTrust is the most important thing To be a successful manager, you must earn the trust of your team members. People won’t work for someone that they can’t trust. Combine this with the fact that a large reason that people leave their jobs is their boss\r(even above money, benefits, commute, or vacation), and it’s clear that this is your biggest priority as a manager.\nTrust is earned, not given. A manager can’t talk people into trusting them; you have to show them that you can be trusted. In my experience, the time it takes to earn trust varies from employee to employee. I had the advantage of having already worked with my employees for almost a year, so some people trusted me on day 1 as a manager based on prior interactions. Others took a few weeks or even months to come around. The important thing to realize is that you can’t rush developing trust. Patience is key.\nTo earn your team’s trust, you must demonstrate good professional and personal character:\nHold everything told to you by employees in confidence. Don’t gossip or talk about employee issues with other employees. Have an “open door” policy: make it clear that people can talk to you about anything, at any time that you’re free. Be a person of your word. Say what you mean, and do what you say. Don’t promise anything that you can’t deliver on. It’s better to say “I’ll see what I can do, but no promises” about a raise, promotion, or bonus than to say “sure we can work that out! No problem!” and then not deliver. Never lie to anyone. Being a liar in the eyes of your employees is the fastest path to failure. They will not only distrust you, but actually start avoiding you as well. Be direct with your people about things that you need to address, especially when it’s uncomfortable to do so. Don’t dodge subjects. Don’t avoid elephants in the room. These problems grow and only get worse if left unresolved. Never be passive-aggressive. Don’t gossip about people. Don’t talk about person A to person B behind their back. It’s important to have an outlet to vent, but make that outlet your spouse or family in complete confidence – not your employees. Praise publicly, punish privately. Never alienate someone in front of the team. Be as objective and fair as possible at all times. Don’t give anyone special treatment or status. All of these things (and more) are essential characteristics of a good manager.\nCode scales, humans don’t The great thing about code is that when written properly, it scales very well. The same code can easily handle both the 1 user case and the 100,000 users case if it’s efficient and optimized. As a developer this is what we aim for, and it’s easy to measure.\nA manager, however, doesn’t scale. The work is done on humans, not computers, and human interactions don’t scale. Just imagine trying to reach consensus on a subject in a meeting of 20 people. It’s much easier to do in a room of 10 or fewer.\nNo manager can effectively manage 20 direct reports. In fact, people experienced in the field feel that 4-10 employees\ris the maximum. It’s important to recognize that you can’t get stretched too thin; this will cause you to pay too little attention to your employees and make them feel resentful and unimportant. It will also prevent you from getting any work done. If your team grows too large, split them up and get another manager on board to share the work.\nMake people a priority As a developer, people were an interruption and distraction to my work. As a manager, people are my priority. You must always prioritize people. Not just your people, either. It’s not uncommon for people from other teams to want to talk to you for an outside perspective – make those conversations a priority as well. This means being flexible: if they want to call you and chat at 5pm on a Friday, try and make it happen. If they work in another time zone and want to meet at 7am on a Tuesday, get up early and do it.\nAnother important aspect of prioritizing people is having a regular 1 on 1 with each of your employees. The 1 on 1 is your employee’s safe place to talk. They will tell you about what’s going on, bring up personal or professional issues, or give you feedback on things that you can do better. I have a recurring 45 minute meeting with each of my employees every 3 weeks. They have been incredibly valuable to me so far.\nMake an effort to be organized and use your time effectively. Do everything that you can to avoid rescheduling, being late for, or missing a 1 on 1. If you do this regularly, your culture and morale will suffer greatly. Nothing says “you’re not important” more clearly than missing or rescheduling a 1 on 1 for no good reason.\nEmbrace uncomfortable conversations As a developer I sometimes found myself gossiping about, or venting to, other developers about people and policies that were annoying me at work. As a manager, your job is to help people address their problems with each other directly. Encourage people to approach the other party to address their concerns, rather than gossip or vent to co-workers. Be sure to lead by example and practice this consistently yourself.\nWhen someone comes to you with an issue they’re having with someone else, get both parties into a private room with you. Mediate a constructive conversation where they directly discuss their feelings and resolve their concerns. These situations can be awkward and uncomfortable, but you must embrace them and make them a part of your culture. Do this consistently until people start doing it without involving you at all. Make sure that you don’t let problems go unresolved; they only get worse with time.\nSuccess is really hard to measure The great thing about being a developer is that you usually have a specific problem to solve. “The application needs to do this new thing,” “Fix this bug,” and so on are well scoped problems. When you finally complete the work you get the satisfaction of having accomplished something. You might even get some accolades in a team meeting or town hall.\nManagement is not a race but a long-distance marathon. You will not be crossing any measurable finish lines anytime soon. As a manager, your work is in affecting the often-intangible aspects of human beings: their thoughts, feelings, soft skills, and overall growth. These types of progress are on-going and arguably never-ending. We are never truly finished growing, learning, and bettering ourselves.\nInstead of “fix this bug,” a manager’s task is “help Person improve their interpersonal communication style” or “grow Person’s soft skills so that they can be promoted in a year” or even “figure out a way to help Person get to work on time more often because they keep sleeping in.” None of these are break-fix problems, and thus it’s rare to feel like you’ve succeeded or finished something you’re working on. I find that feelings of success come from the feedback of others: an employee thanking me for some great advice or feedback, someone telling me they’re noticing a positive change on my team, or even a “keep up the good work!” from someone outside of your team. Nothing feels better than knowing you’re making a positive impact in the day to day of your people, but it again requires patience to see results.\nNobody sees you do work As a developer, your work is done overtly. You’re a better developer for being outspoken about what you’re working on, and creating transparency so that anyone – from other developers to the CEO – can see what you’re working on and what’s getting accomplished. It’s easy to show that you’re doing some real work.\nAs a manager, almost all of your work is done behind the scenes. As a result, being a manager becomes this “out of sight, out of mind” thing where some people will perceive you as doing very little work. This is because when I talk to an employee about their communication style and issues they’re having interacting with others, I do it privately. It’s not as if I can send a status report e-mail to the team that says “I had a chat with Person about their communication issues, high fives everyone!” As a result, nobody other than the person I talked with directly knows that the interaction happened. Thus, to others, it might appear that I never addressed the issue.\nThe way that a manager shows that they’re doing work is in observable team member improvements. It’s a slow process (like many things management), but it will become obvious that you’re doing good work when the team starts to notice that Person is interacting in generally more positive ways, or on time for work more often. As your people grow and develop, your team will become stronger and more effective; people may remark on your team’s positive changes as well.\nUltimately, your success is the success of others If you’re in it for the praise, you’re gonna have a bad time. Management is about trust, having direct and sometimes difficult conversations, and doing your work behind the scenes but in direct and fair ways. It’s a very empowering, challenging, and rewarding job that I’m enjoying immensely.\nOne of my favourite quotes from Futurama was pretty much written for managers, so I’ll leave you with this:\n“When you do things right, people won’t be sure you’ve done anything at all.”\n","permalink":"https://www.davidhaney.io/developer-turned-manager/","tags":["career","lessons","management","mistakes","promotion"],"title":"Developer Turned Manager"},{"categories":["blog"],"content":"I was working on my fireplace this past weekend. Specifically I had just finished ripping down the old surface to the red brick, and then preparing the brick surface with a layer of thinset for tiling. I spent all of Saturday cutting tiles and then placing them on the fireplace surround and hearth. Even with help it took 11 hours to do, and about 8 hours of it was measuring and cutting tiles.\nWhile I was doing this work, which is just mindless enough that your mind wanders but requires just enough attention that it doesn’t wander freely, I began to recite a common trades mantra. Measure twice, cut once.\nThis quip – a practical saying – saturates the construction industry. Whether you’re a DIYer like me, or a professional tradesperson, it’s important to measure everything twice and do the work once. This saves you a lot of pain and time down the road, since you can double check your angles and distances and get everything right the first time.\nThe reason that this practice is important is as simple as considering a tile. Let’s say that I need a 3/4″ width tile, but I measure incorrectly and cut it to 1/2″. There’s no way for me to turn that 1/2″ piece back into a 3/4″ piece, so I just wasted that tile. I need to toss it out (if it can’t be used elsewhere) and cut a new tile to the correct measurement. In short, measuring twice saves you time and money.\nAs I stood above my trusty wet saw, cutting tile, after tile, after tile, my mind began to wander into the realm of programming. I began to realize something interesting. In my opinion, many IT departments have a policy of measuring twice and cutting once, with the supposed benefit of cost and time savings. One might even call this sort of approach waterfall or agile, where estimates are gathered in detail (measured) long before the work is done (cut).\nI believe that this is a fallacy that ironically leads to even more work. Every single developer that I’ve ever met in my career, including myself, cannot accurately estimate anything. We sometimes get close, because we can relate the task at hand to a similar task we accomplished previously, but in general I find that a new task is very much an unknown and the time spent to gather an estimate is pointless since it’s wrong anyway. By measuring twice and cutting once, we waste a ton of time.\nI believe that developers should measure once, quickly, for a rough estimate, and then cut. The reason that I believe this is due to a fundamental difference between programming and other kinds of work that is managed with processes and estimates.\nCode is not a tile or piece of wood. It is a highly flexible, malleable, mutable, digital thing. If a developer cuts a feature short, they can add on to it later, expanding it seamlessly to the required size. If they overestimate a feature’s length, they can easily chop off the excess and move on to the next feature. There is no significant cost in quick, roughly estimated measurements for programming work.\nImmediately your team will regain a ton of time in which they can do their development work. They won’t have to attend hours of planning meetings or requirements gathering sessions. They will just work to get things done as fast and accurately as they can.\nThe only tradeoff is a lack of estimates that management-types can cite and depend on. I would challenge that any estimates derived are very commonly wrong and useless regardless. More-so, if you do not trust your developers to do the right thing and use their time effectively, why do you keep them employed?\nTo me, a lot of the process models around development that are popular (waterfall, agile) are derived from the measure twice, cut once methodology. This approach is super practical to physical goods since inaccurate measurements are expensive, but this does not apply to development work. These meetings to gather estimates in the hopes of controlling costs ironically bloat budgets and help to deliver less code and extend goal dates and deadlines. You take people that are hired to code, and tie them up in meetings where they have to try and justify what they’re going to code by the hour. They don’t know how long it will take, but they will have a better idea after a few hours of coding – if you’d just give them a few hours of no meetings to code.\nIf you’re working on tiling your fireplace, measure twice and cut once. If you’re working on code, take a rough guess at the measurement and get to work!\n","permalink":"https://www.davidhaney.io/developers-shouldnt-measure-twice-cut-once/","tags":["culture","iteration","methodology","mistakes","process"],"title":"Developers Shouldn't Measure Twice, Cut Once"},{"categories":["blog"],"content":"Today an article was brought to my attention. One that, at the time of writing this post, had hit the front page of various sites (including Hacker News) and had been shared over 2,600 times. The article is On Secretly Terrible Engineers\r, which is a criticism of the tech industry and the mentality which it holds towards hiring both new and experienced developers/engineers.\nSpoiler: I strongly disagree with most of this article. If you aren’t open to debates and discussion, quit reading here and return to your normal activities.\nNote: any and all bolding or emphasis will be entirely my own and not present in the original article. If you see bold, know that I bolded it.\nIf you’re still with me, I’d like to tackle this article inline because educated context, which is what I feel the article lacks in the first place, makes the world go ’round. Let’s begin with the credentials and qualifications from which the author speaks.\nI am not unbiased here, having gone through this process myself. I started programming in second grade. I wrote tens of thousands of lines of code in high school, programming games and my own web server. I got a Mathematical and Computational Science degree from Stanford and continued coding. I should have been a software developer, but after a series of interviews, I realized the field was never for me. So much hostility, so little love.\nI want to make my first criticism the most important: the author has never been a professional programmer drawing a paycheck. Yet, the entire article is about how the world of professional programming works, and how it’s broken. I liken this to me getting a law degree and then, after a few interviews at firms that happened to have mediocre or bad hiring practices, writing an op-ed about how the legal industry is broken and law firms and their lawyers are all cold, heartless entities that are not welcoming to job applicants.\nThe fact that the author earned a Computer Science degree from Stanford is impressive. Unfortunately, as many people who program full-time have realized, most theoretical knowledge gained in school translates very poorly to real world programming. I would venture out and say that the author probably was not a very good programmer when they were fresh out of school, looking for that first job. I say that because for the most part, none of us were. I certainly thought I knew how to program when I graduated, but in reality I was a terrible programmer – arguably even “net negative” (creating more problems than I solved) due to my ego and blind confidence.\nThey lurk, unnoticed in the great halls of engineering that are the office strips along Highway 101. “Programmers” not programmers, people who have cheated, stolen, and lied their way through engineering careers without anyone realizing they can’t code. They are among us, incompetent Cylons secretly plotting to undermine us at a crucial time.\nWhat is this “us” that the author speaks of? There’s a reason I put the author’s credentials (which are at the bottom of the original article) at the top of this one. The author does not work in this field. There is no “us” – the author should have said “you” but knew it would be accusatory instead of an empathic opener that developers and engineers could relate to.\nSecretly terrible engineers (STEs) are everywhere, and they may be on your very team as we speak.\nThere is only one way to stop this scourge, one interview to defeat them all. Well, more like a dozen interviews with white boards, but that doesn’t sound nearly as cool. But I digress. One interview to rat these jackals out, to prove just once that no matter how much you did in the past, you will be discovered as the Person Who Doesn’t Know The Big-O Of Trie Insertion.\nThe interviewer, preparing for this moment for years while waiting for git pushes, stands up and stabs his finger at the interviewee. “I’ve got you!”\nI would not hire a candidate based on a pop-quiz style test. A company that I used to work at did this, and it sucked. We ended up hiring a bunch of the “secretly terrible engineers” or STEs as the author puts it. This is because a pop-quiz test can be studied for; it tests your ability to memorize and repeat, not your ability to comprehend. People would fool us into believing that they were skilled developers by reciting the correct answers to us. Within a few weeks of working at the company, they had usually outed themselves as terrible. Needing basic instructions on for-loops, source control, or even that pesky Trie Insertion that you seem to mock as if it were not a part of the daily job for many of us (ever built a type-ahead or autocomplete system?). It’s O(n) where n is the length of the key being inserted, by the way. I didn’t have to look that up because it isn’t trivia knowledge. It was derived easily from my practical understanding of data structures and algorithms.\nThe interviewer is not preparing for years to out the candidate, and certainly does not make a theatrical show of it. This is because for every 200 candidates that we (proverbial) interview, 199 can’t code\r. It gets really depressing and morale-depriving to constantly be rejecting candidates. We find no joy in it. It may feel like you were “outed” or a scene was made because you were unable to pass the interview, and this is very emotional for and personal to you, but on our end it’s just disappointing. We really liked your personality and attitude, and we’re frustrated that we cannot hire you and work with you. The only thing that stands between you and the job is core competency, which you lack.\nOr, at least, I guess that is how this moment is supposed to go, since it never seems to actually happen.\nHow would the author know? They don’t work in the field and they likely don’t hire developers. If they did, I think they’d have written an entirely different article. The author’s evidence so far is entirely anecdotal and surely suffers from sample bias.\nBut there is also a darker vein running through these articles, of how to see through the posers and fakes, of how to test engineering skills so that you don’t hire that STE. In this view, the world is swimming in pure engineering mediocrity, and only extraordinarily careful interviewing will allow you to distinguish between fraud and genius.\nIt’s a paranoid fantasy, a As don’t hire Bs bullshit lie.\nI agree with the first paragraph. It sucks, but it’s important to carefully screen candidates and hire the right developer. A bad hire costs more than you’d probably ever guess.\rIt is true in my subjective experience that careful interviewing is the key to distinguishing between a fraud and a genius. Pop-quiz style interviews don’t weed frauds out. Only an interview that tests basic coding proficiency can weed them out.\nAs for the paranoid fantasy bit, I’d ask again how the author knows this, based on their lack of experience in the field. Also, what a sensational, angry comment!\nThe reality is, few professions seem so openly hostile to their current members as software engineering. There is always this lingering caution when interviewing a new candidate that somehow this individual has gotten through every interview process and team review without anyone realizing the incompetence before them. Founders swap stories of “that one guy” who somehow managed to work on infrastructure at Facebook, but was a complete idiot.\nI agree. The industry is cold and unforgiving to most candidates. I consider myself a very talented developer, but that didn’t stop me from going through 6 interviews, 4 of which involved coding in order to land a job at Stack Exchange. I did it willingly because I knew that anyone that I would work with also passed those tests and is a competent developer. In reality, my coworkers are all brilliant (or geniuses, as the author says). I’m lucky to be on such a talented team. This is why these practical code tests are important: they ensure that your hires are skilled and that you get the best bang for your buck (the “10x developer” as the author puts it).\nAs for “that one guy,” he exists. As a start-up grows, they experience growing pains. Sometimes this is a rapid in-flux of hires to accommodate some scale issues or immediate need. Part of the growing pains is learning to weed out the fraud “developers” also, and so this person tends to get in before the interview process is solidified. Once hired, it is very hard to get rid of an employee (and it costs a lot too), so that one guy at Facebook slipped by. He got in as Facebook experienced growing pains, and slipped by the not-yet-solidified interviewing process. Most of us have worked with at least one of these people recently if not multiple times throughout our careers.\nI’ve always been curious what all these people have been up to in the Valley. Where do they go all day? What do they do? If we are so surrounded by lackluster talent, how do we build all of these companies that seem to be taking over the world? Are STEs secretly burrowing owls who transform into humans during engineering interviews?\nIn all my years immersed in the tech industry, I have never once heard a firm talk about the idiots lurking in their own offices. They always seem to be elsewhere. For everyone.\nIf you made 7 bad hires and 3 good (or “10x” as the author stated) hires, you’ll likely get the work done. The 3 good developers probably won’t love their jobs (because they’re picking up the slack of the other 7, and in doing so hiding the 7 from the peering eyes of management). Your company culture will also suck, but the work will get done.\nNo firm is going to willingly talk negatively about their employees, especially when they’re seeking or have received funding and extra especially to someone who writes for TechCrunch. That would be asking for trouble. But hey, the author’s experience can dictate their entire view of the industry if they’d like it to.\nDespite the worst talent crunch that Silicon Valley has ever experienced, we still regularly throw away huge groups of talent for not perfectly answering the latest hip algorithm question. “What do you think of the latest RB-tree research,” your interlocutor asks. “What?” “Buzz! Fail. Or should I say Fizz? Dammit I lost track,” you barely hear as security walks you in shame out of the office.\nHere we agree: this is a terrible hiring practice. Pop-quiz interviews are pointless as they don’t accomplish anything and don’t filter out bad candidates. If this is an experience that the author has had, it becomes clearer to me why they wrote this article.\nYet in engineering, we expect people to do live engineering on a white board under stressful interview conditions because, well, because that is what we have always done. Most programmers need StackOverflow, Google search, or Dash in order to be effective, yet you get to an interview and are expected to spontaneously remember the positional arguments for some esoteric function. And we keep doing this even with people who have years of experience in the field!\nAs prior discussed, these types of interviews are terrible. They accomplish nothing other than a round of FizzBuzzword Bingo, and make the candidate feel stupid or bad. I deplore these companies. Read my prior blog posts for better ways to hire good candidates.\nYet, we don’t make the same assumptions in Silicon Valley. You can work at Facebook or Google for years, and still start over from scratch with FizzBuzz when you start searching for another job. That is complete paranoia. Nerds are frankly very nervous about status, which Paul Graham argues comes straight from high school. I am not as convinced by such pop sociology, but there is something in the culture that is making us seek out and destroy those losers on our group projects that can’t carry their own weight.\nThere are definitely elitist engineers; most of them are misguided. The industry is certainly full of egos. I’ve met enterprise architects that couldn’t architect their way out of a cardbox board, and junior developers who think that they are experts who know everything. It’s definitely a problem, and makes the industry seem harsh to newcomers.\nNo, but we only realize this when we consider the real context of how engineering happens today. We still act in interviews as if every engineer works independently, when in fact teams greatly affect the performance of every contributor. We act as if engineers should have the entirety of Python’s standard library memorized, when in reality we all use the API reference docs. Take an engineer and remove their team, their search engine, and StackOverflow, and yes, they might look completely incompetent. That’s a fault of the interview, not them.\nActually, that’s the fault of the candidate. If you ever interview with Stack Overflow, we will test your coding skills in a way that prevents you from looking up the answer or depending on libraries. This is not to make the challenge extra hard; it’s quite the opposite. We believe that libraries and helper functions abstract away the true complexity and importance of code. Many developers can sling code from libraries all day (think LINQ or jQuery), but the ones that understand the computational complexity of what they’re doing under the hood are the ones that I want to work with.\nIf a candidate cannot solve a relatively simple (or even moderate) programming problem without the help of documentation and libraries, then they do not know how to code.\nWe need to move beyond the algorithm bravado to engage more fundamentally with the craft. If people are wired for engineering logic and have programmed in some capacity in the past, they almost certainly can get up to speed in any other part of the field. Let them learn, or even better, help them learn.\nAgreed, fully.\nNo one ever offered me a book. No one even offered advice, or suggestions on what was interesting in the field or what was not. No one ever said, “Here is how we are going to bring your skills to the next level and ensure you will be quickly productive on our team.” The only answer I ever got was, “We expect every employee to be ready on day one.” What a scary proposition! Even McDonalds doesn’t expect its burger flippers to be ready from day one.\nThis is the saddest part of the article to me, as it reveals the true experiences of the author.\nI believe that empowering developers is how you bring out the best in them and get things done. This involves mentoring in positive ways: offering and suggesting books (countless devs have loaned me books over the years), offering advice (also countless), and suggestions and talk of what’s hot in the field. It makes me angry at the industry that the author had this experience, and I agree fully that it’s bullshit and needs fixing.\nAs ambassadors to future developers and engineers, we should be more welcoming and willing to teach. If the candidate has the right attitude and aptitude, they can learn – quickly. I leave you with this: while I disagree with the author on many of their points, this one is at the crux of the problems in the IT industry.\n","permalink":"https://www.davidhaney.io/on-secretly-terrible-engineers-a-rebuttal/","tags":["analysis","commentary","culture","rebuttal"],"title":"On Secretly Terrible Engineers - A Rebuttal"},{"categories":["blog"],"content":"A Job Listing Let’s say you were walking down a street one day and noticed an ad for help wanted. It is posted in the window of a bakery. It reads:\nHELP WANTED:\nNeed a baker for FT work. Must be familiar with modern baking methods such as ovens, barbecuing, and deep fryers. 5+ years experience with the Super 6 commercial baking oven required (aside: came out in 2014). Nice to haves include experience with butcher’s blocks, chopping meat, and making candles.\nGiven that most of us understand the high-level job of a baker pretty well, it’s easy to see how totally ridiculous this job listing is. Let’s break it down:\nNeed a baker for FT work.\nDoesn’t say too much, but it makes sense and describes your need accurately.\nMust be familiar with modern baking methods such as ovens, barbecuing, and deep fryers.\nMost bakers would be familiar with ovens, but no baker in their right mind would ever “bake” with a barbecue or deep fryer. At this point we begin to question whether or not this bakery has any idea of what it’s doing at all.\n5+ years experience with the Super 6 commercial oven required (aside: came out 2 years ago).\nWell, they want me to have 5 years of experience (or more) with an oven that has existed for only 2 years? That’s just silly. Now I think that they’re sort of out to lunch and don’t really want to work here.\nNice to haves include experience with butcher’s blocks, chopping meat, and making candles.\nA baker is a very specialized position. There’s rarely going to be one who has specialized in not just baking but also butchery and candlestick making (catch my sly joke there?). A place asking for all 3 is really asking for 1 person to do 3 jobs for the price of 1 job. This isn’t fair and further dissuades the candidate.\nRidiculous Expectations This job posting sounds funny because it’s plain to see just how insanely off-kilter it is. However, recruiters are out there right now posting tech job listings that are just as ludicrous. The problem is that they haven’t bothered to learn much about the field that their job is focused around. So, they end up sounding as silly as the bakery owner does above.\nRecruiters: this is how silly you sound to developers when you post a listing full of jargon that you don’t understand. Most of us don’t want to ever work on both C# and Java at the same time, and the odds of finding both an iOS and Android pro in one human being are slim-to-none.\nThe Recruiting Edge All that is required for a recruiter to gain a significant competitive advantage over their peers is a little knowledge. Knowledge that could be learned in weeks. Knowledge of a standard tech stack, and the ability to Google technical acronyms like MVC and EF when they show up. If you just took a little time to make sure that your job listing was not a walking contradiction – full of statements that counter each other, technologies that don’t work together, and demanding 5+ years of experience on things that have existed for less than 2 years – you’d have a huge leg up on your peers.\nDevelopers are rational, logical people. We will engage in the jobs that make sense. If your listing is irrational or illogical, we will avoid you like the plague. We will also make fun of your job listing to our friends, especially when you spammed e-mailed us with it directly.\nSo, if you’re a recruiter reading this, and you want to get ahead, it’s very simple. Learn the tech that your listing is based around at a high level (in my opinion a basic proficiency of your profession), or failing that, consult a developer to make sure that your listing doesn’t sound crazy!\n","permalink":"https://www.davidhaney.io/the-recruiting-competitive-advantage/","tags":["attitude","career","recruiters","workplace"],"title":"The Recruiting Competitive Advantage"},{"categories":["blog"],"content":"Intro We’re just two days from a brand new year and yet the primary measurement of a developer’s skill seems to be the same as it was 20 years ago. The most important classification to most companies is job title, as I talked about in great detail in my last post\r. The job title is acquired via working for a veritable slough of credentialist companies whose HR departments break it down very simply:\nYou’re a Junior Developer if you have 0-2 years of professional experience You’re a Developer if you have 2-5 years of professional experience You’re a Senior Developer if you have 5+ years of professional experience You’re a Team Lead, Architect, or Manager once someone promotes you from Senior Developer The problem with this sort of classification is obvious. The truth is that one person has 10 years of development experience, and the person standing next to her has 1 year of development experience repeated 10 times. The people that end up in the latter category tend to be “company [wo]men” – the ones who stick with a job forever and never try something new.\nProper Job Titles I am not one to believe that rockstar developers are valuable. As Scott Hanselman writes\r, rockstar developers do much more harm than good. I do, however, believe that the proper classification of developer skill lies in my paraphrasing of Nate Waddoups’ description:\nA Junior Developer creates complex solutions to simple problems. You see these people across all companies and in all kinds of positions: from Junior Developer to CIO. They take a one-off batch service that processes requests and make it into a 40 class, 2000 line monster that involves a ServiceFactoryFactory and a ServiceDecorator. A Developer creates simple solutions to simple problems. They’ve gotten past the pattern-itis and realize (for the most part) when to use design patterns, and when to just write simple code that works. These people tend to write the batch service in 100 lines and it works correctly 99% of the time (and is easy to maintain too). A Senior Developer creates simple solutions to complex problems. These people have begun to truly master the art of software development and can whip up a highly scalable e-commerce app in fewer assemblies, classes, and lines than most of us would have used to create “Hello World” as a Junior Developer. They would be incredible role models if only they’d drop the ego. A Truly Exceptional Developer is a rare thing. They don’t just make complex problems simple – they make complex problems disappear entirely. These people wield code like I imagine ancient Samurai wielded the blade: with great precision and skill. They dropped their ego years ago. In doing so, they empowered themselves to continue to learn from others which ultimately caused them to excel far beyond their Senior Developer peers. They are incredible role models and all around must-haves on any high performance team. They love to teach as much as they love to learn. Notice that I never gave a time frame to any of these levels of skill. I don’t believe that 1 Truly Exceptional Developer can be faster than 10 Senior Developers. I do believe that in the long run they save you 10x your time and money. They write extremely maintainable code which has few bugs and even fewer scaling issues. They can quickly learn and do anything asked of them. They are truly versatile and efficient.\nMake \u0026lsquo;Em Write Code What would be required for companies to finally start hiring based on skill as opposed to credentialism? The reality is that some already do. To hire skill takes nothing more than ignoring the number of years of experience written on paper and simply administering a few code tests to a candidate that appears to have the desired personality traits. These tests should be moderately difficult and focus on a problem that has many answers – only a few of which are good.\nAn Example: FizzBuzz A great example (though well known at this point) is the FizzBuzz test:\nWrite a program that counts from 1 to 100. If the current count is evenly divisible by 3, output “Fizz”. If the current count is evenly divisible by 5, output “Buzz”. If the current count is evenly divisible by both 3 and 5, output “FizzBuzz”. Otherwise, just output the current count.\nThis task is fairly simple but has many solutions – only a few of which are good. In tasking a candidate to complete FizzBuzz, their skill (or lack thereof) will show itself immediately:\nJunior The Junior Developer will write a whole ton of if statements, possibly even hard-coding all of the multiples. This section of code is probably in its own class, injected via the constructor and provided by the DivisorCounterFactory. Also, there will be as many lines of comments as there are code.\n// Count to 100\rfor (int i = 1; i \u0026lt;= 100; i++)\r{\rif (i == 15 || i == 30 || i == 45 || i == 60...)\r{\r// Write \u0026quot;FizzBuzz\u0026quot; to the console\rConsole.WriteLine(\u0026quot;FizzBuzz\u0026quot;);\r}\relse if (i == 3 || i == 6 || i == 9 || i == 12...)\r{\r// Write \u0026quot;Fizz\u0026quot; to the console\rConsole.WriteLine(\u0026quot;Fizz\u0026quot;);\r}\relse if (i == 5 || i == 10 || i == 15 || i == 20...)\r{\r// Write \u0026quot;Buzz\u0026quot; to the console\rConsole.WriteLine(\u0026quot;Buzz\u0026quot;);\r}\relse\r{\r// Write current count to the console\rConsole.WriteLine(i);\r}\r}\rThis is not a very scalable way to solve the problem.\nDeveloper The Developer will whip together a few if statements using modulus to solve the problem.\nfor (int i = 1; i \u0026lt;= 100; i++)\r{\rif (i % 3 == 0 \u0026amp;\u0026amp; i % 5 ==0)\r{\rConsole.WriteLine(\u0026quot;FizzBuzz\u0026quot;);\r}\relse if (i % 3 == 0)\r{\rConsole.WriteLine(\u0026quot;Fizz\u0026quot;);\r}\relse if (i % 5 == 0)\r{\rConsole.WriteLine(\u0026quot;Buzz\u0026quot;);\r}\relse\r{\rConsole.WriteLine(i);\r}\r}\rDecent solution, fairly simple and by-the-book answer.\nSenior The Senior Developer will realize that they can combine the statements.\nfor (int i = 1; i \u0026lt;= 100; i++)\r{\rstring result = \u0026quot;\u0026quot;;\rif (i % 3 == 0)\r{\rresult += \u0026quot;Fizz\u0026quot;;\r}\rif (i % 5 == 0)\r{\rresult += \u0026quot;Buzz\u0026quot;;\r}\rConsole.WriteLine(result.Length \u0026gt; 0 ? result : i.ToString());\r}\rClean, simple code that takes up maybe 10 lines when you get rid of my Allman style code indentation\r. Then, if the ego remains, they’ll ask you why you make developers jump through interview hoops, and possibly berate you for wasting their precious time and skills on such a remedial task.\nExceptional The Truly Exceptional Developer, having never before heard of the FizzBuzz problem, will write the Senior Developer solution in a matter of 2 or 3 minutes. They’ll then discuss the ways in which it could possibly be improved. Things like how a case statement might technically be faster than an if statement in some cases due to omitting an op code at the assembly level, or how to optimize appending immutable strings together in order to avoid creating additional objects which would later have to be garbage collected. They’ll thoroughly enjoy the problem and be enthusiastic in the discussion.\nCode Testing Works A 30 minute code test immediately gives you more insight into the candidate’s skill than a whole day of discussions ever could. You can proceed to hiring with confidence (perhaps after a few more code tests administered by different people in order to observe the candidate’s ability to work with others). You can also reject a candidate with confidence. No matter how good of a salesperson they are, they will not be able to talk their way past the code test.\nIn summary, my unsolicited advice to you is this: if you want to work with good developers, just reject any interview process or job offer that doesn’t involve a code test. A lack of code testing means that all manners of developer will slip through that company’s hiring process (including those at the very low end of the bell curve) and that they will be your coworkers.\nVia this hiring approach, you’ll meet Junior Developers with 25 years of experience, and Truly Exceptional Developers fresh out of school – and everything else in between.\nDoesn’t it seem obvious that we should be tested on our coding skills, given that coding is literally the primary task that we are hired to do for 40+ hours a week? Do you think that a hospital would hire a surgeon without testing their skills? Would the NFL or NHL recruit a player without testing their skills (or at least seeing their skills in action)?\nIn all professions, the top tiers test their candidates for practical skills. This filters out the unskilled, resulting in awesome teams of incredibly skilled people. These are the teams we want to be on. These are the companies we want to work for. These are the environments in which we learn and become Truly Exceptional Developers.\nPS – I hope that you had a terrific Christmas and will have a Happy New Year!\n","permalink":"https://www.davidhaney.io/on-credentialism-in-software-development/","tags":["career","credentialism","culture","interviews","workplace"],"title":"On Credentialism In Software Development"},{"categories":["blog"],"content":"Intro I’ve had a good career so far. I began working full-time as a programmer in 2008. At that time my title was Junior Developer. I had a decent boss and cool co-workers, and I cut my teeth on Java and .NET. It was a good experience. After 2 years at that gig, I felt that it was time to move on.\nI contacted recruiters, and one eventually found me a promotion: Systems Analyst. It came with a decent pay bump and so forth, as well as the luxury of dropping “Junior” from my job title. As this was a good deal all around, I took the offer.\nA few more years went by and I found myself once again looking for “the next step” in my career. Lo-and-behold my current company had appreciated my skill as a Systems Analyst and wanted to bring me on as a Developer. Before I knew it I was a .NET Developer which, as my other job changes had previously, came with a pay raise and better perks. Things were going great.\nI did 3 years total with this company, during which I was promoted to .NET Development Team Lead – a terrific title in the realm of programming. I ran a small team of 4-5 people and things were going pretty well.\nEventually, having maxed out my skills and knowledge at this company, I moved on. I became a .NET Architect with appropriate pay and benefits. It was a reasonable gig.\nRecruiters Use Your Title This is where I’ll introduce a sideline fact: during my entire career to this point, recruiters (both in-house and third-party) had been cold-calling and contacting me with jobs that matched my current title (with more contact as I got further in my career). As a .NET Development Team Lead, I was offered jobs that ranged from Senior Developer to Team Lead to Architect. As a .NET Architect, I was offered titles from Team Lead to Architect to Manager. So far things made sense.\nAbout 5 months ago I took a job with a company that I love, Stack Exchange. Things are going great so far, and I’ve never been treated better in my entire development career. My pay and benefits are top-notch, my job perks are incredible, and my boss is a genuinely competent, solid human being – as are all of my co-workers. It is the current high-bar of my career track record.\nWhere it becomes interesting is in my job title. At Stack Exchange I am a Web Developer. That’s the whole title. Not Manager, or Team Lead, or Architect, but Web Developer. The reason for this is that Stack Exchange is not a company that cares for credentialism. We hire smart people who gets things done (their words, not mine) and developers here handle projects from start to finish. This means that we sometimes act as a Manager of others, sometimes a Team Lead of others, and often an Architect as we discuss and debate the best way to solve a particular problem or design a particular system.\nIt was not until I got this job that I realized how broken the current recruiting industry is. Just 6 months ago I was receiving cold calls and offers to interview for Team Lead, Architect, and Manager jobs (with expected pay). Can you guess what jobs they contact me with these days?\nJunior Developer and Intermediate Developer. Complete with massive pay and benefits cut. Offering these positions to me is a complete waste of both my time and the recruiter’s.\nIt’s pretty obvious why this happens: recruiters are usually not contacting me personally based on researching my career and goals. They are contacting me as 1 of 10-50 others that they will contact via cold-call or e-mail on any given day. A shotgun approach of “fire it off and see what hits” as it were.\nI am certainly not egotistical enough to be offended by being offered titles and pay well below my current skill set. What does bother me, however, is the fact that I cannot convince most recruiters of anything otherwise. I took it upon myself to explain to one recruiter that Web Developer at Stack Exchange is about the equivalent (in my opinion) of a Team Lead or Architect at many other companies. Their response was that if that were true, it would be my title here. I was dumbfounded.\nRelativity Hurts Given that there are no current standards for developer titles, it is plain to see that a Developer at Company A could be anything from a Junior Developer to CIO at Company B. It’s a crap shoot. Especially when you consider the fact that some companies inflate the titles of their employees as a “free” form of making them feel valued (as opposed to, say, paying them properly).\nI don’t know what the solution is to the recruiting industry’s problem, but it does make me realize the value of Stack Overflow Careers. We match candidates with jobs based on experience, interests, and open source projects, not via titles and credentialism. We explicitly ban companies that spam candidates with a job offering. This means that every single message written to you as a candidate is personalized and written by hand by the recruiter. Very few recruiting companies can claim similarly. We continue to evolve this product, and I hope that one day it sets a new standard for recruiting where people don’t blindly call you up with jobs that match your current title.\nThis lack of standards is a difficult problem to solve. I don’t pretend that I have the answers. It does show me precisely how you end up working with incompetent people though.\nA Broken System John Doe is an incompetent developer who works for a small dev shop that gives him low pay but a big title such as Senior Developer. A recruiter eventually offers this developer jobs of a similar title but that have the appropriate pay. John jumps at the giant raise and heads to the next company. What happens next is something we’ve surely all seen a few times… John can’t hack it, John gets 3-6 months in and gets let go. John was cheated by the system of credentialism. Who can blame him directly for grabbing the highest value offering he can get? We all would.\nThis broken system wastes time, tons of money, and screws up company culture. Nobody likes working with John as a peer when he knows 1/10th of what they do, does even less work, and gets paid the same or better. That causes your good developers to run for the hills, and your workplace culture to suck as another John fills the now open role of a good dev.\nThe current state of the recruiting industry is fairly toxic. Most recruiters will put anyone anywhere to make a buck and meet their draw or quota.\nThe question becomes: how do we, with Stack Overflow Careers, completely dismantle this system and rebuild it properly? Time will tell, and I’m excited to see the results.\n","permalink":"https://www.davidhaney.io/the-trouble-with-job-titles/","tags":["career","credentialism","culture","interviews","stack overflow","workplace"],"title":"The Trouble With Job Titles"},{"categories":["blog"],"content":"Intro Like many of you, today I watched the Apple media event in which they announced both the iPhone 6 and Apple Watch. I’m not going to talk about the watch, but instead about the phone.\nFor years Apple has been a true cachet brand. They are a luxury item that is sought after for status and image. I don’t blame anyone for owning an iPhone: they’re reasonably sexy and you get to show off the Apple branding. Good on you.\nMy smartphone adventure started with an iPhone 3 in 2008. It was an amazing, groundbreaking piece of hardware back then. Android was nowhere near it and nothing else came close. I was able to put my iPod Nano in my dresser drawer and use the phone for both music and talk/text. It was amazingly innovative and nothing at the time compared.\nThis morning I was very excited to see Apple finally innovate again under the direction of Tim Cook, after years of what seems like stagnation on the mobile front. Instead, I was disappointed. I’m going to explain why in terms of comparable hardware and cost, not in terms of opinion (because they’re cheap and everybody has one).\nLet\u0026rsquo;s Compare Let’s compare the bleeding-edge Apple iPhone 6 and 6+ offering with my Nexus 5 - a phone that is nearly 1 year old and was released on Oct 31 2013. I highlight the winner in bold for each spec:\nNexus 5 iPhone 6 / 6+ Processor 2.26 GHz quad-core Krait 400 \u0026ldquo;Snapdragon\u0026rdquo; A8 chip specs not released, benchmarked at 1.4 GHz dual-core RAM 2 GB Benchmarked at 1 GB Battery 2300 mAH 6: 1810 mAH. 6+: 2900 mAH Screen Quality 1920 x 1080, 445 pixels per inch 6: 1334 x 750, 326 pixels per inch. 6+: 1920 x 1080, 401 pixels per inch Screen Size 5\u0026quot; 6: 4.7\u0026quot;. 6+: 5.5\u0026quot; Storage 16 GB 128 GB Fingerprint Sensor Nope Yup LTE Bands 6 20 Front Facing Camera 1.3 MP 3.2 MP Rear Facing Camera 8 MP 8 MP NFC, Bluetooth Yup Yup Wireless 802.11ac 802.11ac Screen Polarization Sucky Sucky Price $349 for 16 GB no contract, unlocked 6: $649 for 16 GB no contract, unlocked. 6+: $749 for 16 GB no contract, unlocked. Thoughts: Apple has some better specs, but they tend to be with respect to the cheaper hardware aspects (LTE bands which are redundant anyway, storage space, front facing camera). The iPhone 6 standard model has a really poor display resolution, surprisingly, since Apple has made strides in display technology with their retina MacBook Pros and iPads. The battery on the standard iPhone 6 is also poor compared to the Nexus 5, though the 6+ gets a much larger battery with its larger size. In terms of performance (CPU/RAM), Nexus 5 dominates.\nConclusion: Apple cheaped out on this phone, big time. It BARELY competes technically with the Nexus 5, an Android phone released almost 1 year ago. I cannot believe that for the cost of 1 iPhone 6+ I can buy 2 Nexus 5’s. It truly must be a cachet phone if people are willing to pay the price for such mediocre hardware.\n","permalink":"https://www.davidhaney.io/iphone-6-style-over-substance/","tags":["analysis","android","iphone"],"title":"iPhone 6: Style Over Substance"},{"categories":["blog"],"content":"Intro Almost exactly 1 month ago today I found myself on a video call with Joel Spolsky. It feels insane to write that, even now, as it was a banner moment in my career. For me it was the equivalent of meeting a movie star who I had idolized since I was old enough to know what movies were. There had always been this Joel Spolsky guy throughout my career that I regularly read about and whose opinions on software development agreed with mine, and suddenly I was talking with him face to face. It was awesome.\nReaching this conversation was not the easiest thing I’ve done in my life. It took a few weeks and in all honesty it was a bit trying to find time to have so many interviews. How many interviews, you ask? Prior to Joel I talked with 5 or 6 other amazing people (Marc Gravell and Nicholas Larsen included). Somehow I managed to impress each of them enough to reach the end boss: Joel Spolsky.\nThe conversation lasted about an hour. It felt like 5 minutes to me, probably because of the excitement. Joel and I discussed the pros and cons of various software methodologies, mistakes each of us had made in our careers, some of the challenges of Dache (my open source software), and a few other topics. Then he said something awesome: “We’d love for you to come and work with us at Stack Exchange!” So much adrenaline hit me at this moment that I could have lifted a car with my bare hands. It was surreal.\nA few days from now I officially start working with Stack Exchange. I feel very fortunate and excited for the opportunity. So far the Stack Exchange team have proven themselves an insanely skilled and professional organization that treat employees as human beings instead of expendable resources. I’m already loving the culture and interactions with my coworkers.\nDisclaimer First and foremost, the commentary here consists of my views alone and not those of Stack Exchange or any other entity. This post is merely to reflect upon the interview process and discuss the aspects and traits of my career and knowledge which I feel helped me get the job. This is not a tell-all or any sort of shortcut or easy way out. If you want a job at Stack Exchange, you will have to endure the same technical and soft skills challenges that I did – the details of which I will NOT be disclosing. 🙂\nSo, with that disclaimer out of the way, here are my thoughts on how I got a job at Stack Exchange, and how you can too:\nEgo Can Kill Ego is the mind killer, so kill the ego. Most developers that I’ve met (including some prior versions of myself) have massive egos. Egos so big that the room can barely hold them. Egos that even put the illustrious Kanye West’s attitude to shame. This is natural given that we spend all day creating things from scratch (which is a god-like quality) that often generate significant revenue for companies. We start to feel very powerful and even fawned over. We learn the entirety of the software and hardware vertical of our current job’s domain, and then make a Superman-like flying leap to conclude that we know EVERYTHING about ALL software and hardware.\nThinking you know everything is the easiest way to suck as a programmer. If you believe that you know everything, you stop trying to learn new things (since you already know them, duh). So, while you’ve mastered ASP.NET MVC 3 at your current gig, 4 and 5 came out… the catch is that your company never upgraded because it’s too risky, and so you never learned them (or cared to). Now a year or two later you’re so far behind the current development stack that you can’t even see it with binoculars. And did I mention those little things called Ruby and PHP and even Java that you’ve never written a single line of? And how about MongoDB, Couchbase, Azure, EC2, and the literally thousands of other platforms and programming languages? I should hope that by now you realize that you know a little bit about a handful of languages and hardware configurations amidst a sea of thousands… By percentage, you’ve conquered maybe 0.1% of all that there is to know about development. So have I, and that’s OK.\nDon\u0026rsquo;t Be A Rock Star Don’t be a rock star developer. This is something that I did for a few years and it only hurt my career. Many companies employ a strategy of intentionally furthering the developer ego in order to make them feel valuable (often without handing out appropriate compensation). Being a rock star sounds cool, but really it’s a nasty strategy that can cultivate incredibly destructive developer attitudes. Companies seek out and hire rock stars, and rock stars have a sense of entitlement. They develop huge egos and run their mouths at meetings, interrupting and talking over others. They seem to love circular logic (they’re right because you’re wrong because they’re right). They feel that all other team members exist simply to serve their every whim… and everyone loathes working with them.\nThe thing is – while you may actually be a 1 in 1 million bona fide rock star developer – nobody cares. Talk is cheap and in my experience people will say nearly anything and everything to portray an image of who they want you to believe they are. If you are really good at what you do, telling people doesn’t do anything other than make them despise you… Hearing about how amazing your proprietary code is gets annoying – especially when it’s the 5th time this week you’ve said it. A good developer doesn’t need to brag about how good they are: their work speaks louder than any boastful words could. People around them will naturally do the bragging for them. Hallway conversations to the tune of “wow, he’s really smart” or “she knocked that out in hours when we thought it’d take days” will be fostered, and that isn’t a bad thing. Let people talk about you all they’d like, but maintain a sense of humility and reality. You might be the best developer on your team or even at your company, but you are still a human being and this is still a job. Nobody has been hired to serve your ego (even if their job is to serve you). Being a good developer doesn’t make you better than anyone as a person; it just makes you successful in your career. Never lose sight of the fact that thousands of other developers are great at their jobs too. What separates you from the pack is being a great developer AND modest. It is a very rare combination in my experience, and the complete package that many companies are striving to hire. So, while it’s cool that you’re the very best developer that there ever was, stop believing your own hype and telling everyone who will listen. They don’t care, and you shouldn’t either.\nCan\u0026rsquo;t Know Everything Know that you know enough to know that you don’t know enough. Know what you do know, and know what you don’t know, and never be afraid to say “I don’t know” when it’s the truth. A developer who isn’t afraid to say “I don’t know” in front of a room full of people is a rare gem. By being honest you create trust and credibility. You also foster positive relationships with your peers and company. Nobody will remember that you’ve never heard of Angular.js or Couchbase, though they’ll always appreciate that you didn’t waste their time or money by pretending that you did. You can’t trust a developer who doesn’t know what they don’t know.\nThe \u0026ldquo;Basics\u0026rdquo; Count Know your data structures and algorithms. High level programming languages such as C# abstract so much away from the modern developer that many of us have no idea what’s actually happening “under the hood” when it executes. It’s cool that you can sling LINQ everywhere, but do you know the computational complexity of what you’ve done? Do you know what a hash table is and why it is useful? Could you sort a list of things efficiently? Can you think of a scenario where a stack is the best option? Note that you don’t need to memorize things like sorting algorithms (hell, I couldn’t if I tried), but a working knowledge of data structures such as trees, hash buckets, lists, queues, and stacks combined with the rudimentary knowledge of things like sorting, searching, and caching is a very valuable skillset. It’s the difference between a good programmer and a great one. Anyone can write C#, but only those who understand even the low level operations of each deliberate method call will write good, clean, efficient code. You owe yourself a fundamental understanding of how data structures like stacks and heaps work, as well as by-value vs by-reference memory addressing. These core concepts apply to ALL programming languages. Too many developers ignore the complexity of their algorithms and just call pre-made methods without understanding the implications. Educate yourself on data structures and algorithms and suddenly you’ll be ahead of the pack.\nKnow why your code works and why your code doesn’t work. Have you seen this image circulating on sites like Reddit?\nI hate this image Despite being funny, the popularity of this image pisses me off. It claims that the essence of programming is having no freaking clue why your code does or does not run. I feel that this is unacceptable. A great developer strives for the WHY in every single thing that they do, not just HOW to quick-fix it. Code doesn’t compile? WHY? Race condition across threads? WHY? In asking “why” you further your knowledge and expand your skillset with the functional, rational “how” which allows you to become a better programmer. Most great programmers don’t repeat the same mistakes over and over, though they of course make mistakes… They just make new and interesting ones!\nI remember the days of slinging shoddy code and then copy-pasting lines from blogs and sites like Stack Overflow until my code seemed to work (though I wasn’t sure if or why it did). Those days are long behind me. When my code doesn’t compile, 99% of the time I immediately know how to fix the issue. When my code has a bug, I usually know exactly how to track it down and resolve it, and in doing so I often learn how to avoid it in the future. Having no idea why your code works is like being a lawyer who has no idea why their client is not guilty: fairly useless, overpaid, and an amusing spectacle at times. Make sure that you know why your code works and why it doesn’t. In my opinion this is a basic competency of being a professional developer. It’s OK to create bugs and make mistakes – it’s not OK to make the same mistakes repeatedly.\nWrapping It Up At this point I feel that I’ve done a reasonable job of representing my skillset and core competencies. These are the things I showed to the Stack Exchange team in my interviews. I didn’t necessarily have the exact answer (or even most efficient answer) to each of their technical questions, but I was modest and never afraid to ask for help or say “I don’t know” when I got stuck. My answers involved efficient data structures and algorithms, and I was able to explain why the data structure was the best choice in solving the problem. When I created bugs I was mostly able to identify them and indicate how to fix them. In doing all of this, I demonstrated competency and confidence as a developer and fortunately ended up with my dream job.\nIf this kind of workplace sounds awesome to you, apply today!\n","permalink":"https://www.davidhaney.io/how-i-got-a-job-at-stack-exchange/","tags":["career","ego","interviews","mistakes","stack overflow","transparency","workplace"],"title":"How I Got A Job At Stack Exchange"},{"categories":["blog"],"content":"The Problem I think that most devs would agree when I state that the definition of success in the corporate world of development places less emphasis on “good” code and more emphasis on “working” code. Working code is code that can be released to production on or before the deadline, regardless of performance or even bugs in most cases. As a developer, you ultimately feel as if you’ve failed when you toil for nights on end to meet steep deadlines and churn out crappy code. As a business, however, you’ve succeeded when you hit the deadline. My experience tells me that the typical metric upon which development teams are measured is often not quality of code or unit tests or even performance, but instead ability to meet deadlines and deliver solutions to clients. You’ve failed when you do not meet the deadlines and thus piss off the clients/customers. Your job has become a veritable boolean result with the outcomes of true and false. Deadline met? True. Deadline missed? False.\nDoesn’t it feel awful to be measured in such a binary way? All or nothing, success or failure, deliver or delay. These are the only outcomes according to the people who write and sign your paychecks.\nThe Conflict Why does this happen? A little introspective thought brings light to the subject, at least for me. The reason for these types of metrics becomes obvious when you consider their source. You work for a company who pays you (often with I.T. seen as a cost-center or “money pit”) to accomplish things which the company can then sell to clients. You’re an expensive tool by which they accomplish their means. Though these companies often see software as a profit source, they see the means by which they get the software as an expense and cost. Kind of strange, really.\nThe problem begins at the very core of the organization; the structure of the company is the starting point for guaranteed failure. In my experience, the dichotomy that forms in most companies is “I.T.” versus “The Business” in a bout-to-knock-the-other-guy-out. The minute you create this relationship of opposing fronts, you’ve already guaranteed development failure. With competing and contrasting goals (the business wants to sell stuff ASAP while I.T. wants to build stuff properly which takes longer) it is not possible for trust to exist within the organization. The Business will not believe a word that I.T. says when it comes to estimates, deadlines, or things that need to happen to ensure stability of the product in the future (technical debt). I.T. will not trust The Business to make rational decisions when it comes to features, development timelines and ensuring product quality. The result is a boxing match where each side is trying to force the other into compliance. Now you have conflict. Conflict dismantles good companies.\nThe Measurables The Business is used to tracking their sales teams by metrics like “how many calls did you make/receive today?” and “how many sales did you make?” and “did you make X sales by Y arbitrary date?” where Y could be the end of each month. These are things that they understand, and thus like to control. Ask your favourite sales person for their opinion on the metrics by which their success is measured, and I am confident you’ll find that most will sum it up as “the faster I sell things, and the more things I sell, the more successful I am.” This makes sense from an empirical, see-the-figures-on-paper-on-my-desk-in-my-executive-office point of view, but I bet that the sales person in this case is not loving their life. A constant push to sell more, make more money, and do more. Any success in the future just raises the bar for the success which must follow. It’s a losing scenario. Eventually, they either get promoted out of the trenches of sales or they move to another company, resetting the bar which has been set too high. This buys them another year or two of raising that bar, until they ultimately repeat the process again.\nSales people who are put under the gun in such situations often resort to employing any tactic that they can to reach their goals… One of these strategies is saying anything at all to sign the client up. “Sure, the software can create rainbows and unicorns, just sign on the dotted line!” they say. It’s unfortunate, because customers who are hooked into these contracts tend to be very unhappy with the product when they find out that the software does not, in fact, create rainbows or unicorns. Or even a colour wheel and horses. It doesn’t even come close.\nIn the above case, The Business fails to measure the things that, long-term, make you the most money: client satisfaction and relationships. A good sales person (they definitely exist) is one that keeps the client happy with rational discussions and promises, and who is very transparent about what can and cannot be done and why. A great sales person is one the client loves so much that they’ll keep using your product, even when a better product exists, simply because they fear losing the relationship. This client is a client for life (or at least a long while) and makes you a lot of money. But how do you measure “happiness” and “relationships” long term? It’s a hard problem. Dating sites have been trying to solve it for over a decade. The Business will likely not dedicate the time and resource to do so themselves. So, they measure phone calls, sales, and other crappy metrics to ensure that the sales team are doing their job.\nHere’s where we get back to the topic: developers and failure. The Business, who in most cases pays I.T. to create things to sell, employs these same arbitrary measurements when grading development teams. They often only know how to see success as a measured outcome of facts, and so they create the only measurements that they can empirically apply: features and deadlines. Does the dev team build all of the features and hit the deadline? Great. Do they not? Not great. These measurements themselves are acceptable (even good), but the combination of them (lots of features on short deadlines) is the problem.\nThe Money Talks Where it gets tricky is in the realization that “show me the money” is how business ultimately tends to run. The sales people very overtly make the money, so they are seen as successful and important people in the company. The dev team also makes money, but is perceived to cost money, and they are seen as a cost-center that must be carefully weighed and measured to avoid excessive spending. What this leads to is an unhealthy practice of allowing sales people the freedom to employ any tactics necessary to land sales and make the money. In a business such as The Business as described, your life as a developer begins to suck.\nTo close the deal, the sales person will often promise the client almost anything about the software that you develop. They may promise new feature X by the end of the month, they may even promise 10 new features by the middle of the month. Whatever makes the client sign on. Then, the client says let’s rock and your quality of life drops sharply.\nThe very next thing that happens is The Business casually tries to confirm what seems obvious and even mandatory to them. “So your team will have these 10 things ready to go by the 15th, right?” they say. “This is a million dollar client, and it would be horrible if we lost them because you couldn’t deliver!” and now the pressure is on to do the nearly-impossible in virtually no time.\nThe dev team might try to politely push back and say that this is practically impossible, but The Business sees the dollars on the dotted line and will not listen. Flip the kill switch. Forego the QA time, all developers must focus on all of these features, day and night, so that the deadline can be met. Why? Because that’s how the team is measured. If the team doesn’t hit that deadline, they’ve failed and the million dollar deal is lost with the dev team seemingly at fault. Developers don’t want to work extra? Order in pizzas and promise them time-in-lieu as soon as the deadline is over with. Note that they will likely never actually see this time-in-lieu because right after this deadline will be the next one, with similar outlandish expectations and even tighter deadlines. And after that, another one. And another one. And the cycle will probably never end.\nThe Mad Production Dash So, as the developer, you develop it as fast as you can. The code starts to resemble Frankenstein as you tack on bits and pieces to make it work ASAP. You subdue your ego and uneasiness about the quality of code by commenting // HACK: undo this crap later everywhere. Somehow that makes you feel better as it creates the slight glimmer of hope that eventually you’ll have enough time to come back and undo this monstrous pile of garbage. But you never will get that time, because the next deal is coming down the pipe. And so the code becomes worse. Your development effort completes 1-2 days before the arbitrary sales deadline, and after your QA team flips their lids on having 48 hours to test 1000+ hours of work, they do “critical path testing” to make sure it at least does something correctly and certify it as “good enough.”\nThe team releases to production early in the morning of the deadline day, and though it takes 5 hours because there are 17 untested things to fix on-the-fly (and realistically they have no option to abort the release or roll back because the consequences will be dire), they eventually shove the hacked up code out the door and declare it done. The Business shows their appreciation in the form of a short, impersonal e-mail that doesn’t name any person of achievement specifically. The development team is feeling underappreciated and pissed off.\nWhat does the future hold for such a company? The code will probably spiral into bug-filled oblivion until it can’t do anything correctly or in any reasonable amount of time. Despite the weeks and months during which the development team pleaded with the business for time to clean up the technical debt, they are brushed off because taking time off of features loses clients and thus money. Then, as it starts to come crashing down in production, they suddenly beg the developers for a quick fix. “Do whatever it is that needs to be done!” they plead as they see their sales going down the drain. And now, because it is on fire and burning to the ground, the dev team is finally given a moment to pay back some of the technical debt that has been accrued during this vicious cycle. Repeat.\nThe Solution When a dev team has no say in the deadlines of the work they must do, they will usually fail. And when they are set up for failure from the start, they will likely get tired of being blamed for the problems without ever being given the time to devise the solutions. This leads to bad work culture, high turnover, and low productivity.\nThe way to guarantee dev team success is obvious at this point. It’s really as simple as trust between I.T. and The Business. They must keep each other in the loop as stakeholders. The Business has no product without I.T. and I.T. has no job without The Business’s clients. It’s a mutually beneficial relationship and it should be treated as such, rather than mutually parasitic.\nA good company’s sales team will often consult with I.T. prior to promising any dates and deadlines when unknowns are involved. It is practical to ask the people responsible for a task how long it will take them to complete a task. This is much like how you might ask a waitress how long it will take for the food to arrive or a painter how many days they need to paint your home. This is a positive and productive discussion. Hallway conversations should become commonplace: “Hey dev team, I’ve got a client who wants to sign on but not until we build X, how long will that take?” The reply is as easy as “We’ll discuss it as a team and send you an estimate with some assumptions to confirm with the client” and just like that there’s a great working relationship that practically guarantees success. The team knows what work is coming, and also knows how long they have to complete it.\nThe Correct Measurements If a dev team continues to fail in an environment where trust exists, then that team is likely not competent. They either cannot estimate correctly or cannot deliver within their own estimates. Sometimes devs suck at estimating because they’ve been making estimates under the oppressive sales gun for so long that they’ve effectively forgotten how to give themselves a fair amount of time. The blame for this remains entirely on the dev team, and they (or The Business) must repair the situation quickly and effectively to maintain the mutually beneficial relationship based on trust. As The Business owes I.T. input into the deadlines, I.T. carries the burden of being fair, accurate, and responsible with those deadlines.\nAssuming that The Business now has a competent, skilled dev team, the question turns to the customers. If the customers do not like the estimates given to them, this may cost the company sales. Perhaps the customer wanted the impossible and The Business is giving them a dose of reality. Perhaps The Business does not want such a needy customer and they’re in a situation to be able to afford to tell them no thanks. Perhaps The Business realizes that the client’s request is reasonable but the timeframe of the estimate feels a bit long. In that case they can ask I.T. why. If the answer is not sufficient and justifiable, then perhaps the dev team is still not competent. No dev team should be let loose without checks and measures on productivity, but those metrics should be reasonable.\nUltimately, if you want to guarantee the failure of a development team, simply promise features to clients and customers without ever asking for (or trusting) the input of the team that is actually going to build those features. It’s just like telling the waitress that your food must be on the table in 10 minutes, without first asking the cooks how long it takes to safely and properly make it.\nIf this situation sounds familiar, try talking with The Business about it. Try to help them see it from your point of view. Ask them “how successful would you be if I demanded that you sell 20 new clients by Friday?” and perhaps some light bulbs will start to go on. Ultimately, we as developers often know nothing about sales and have no business dictating their measurable work expectations. They similarly have no business dictating ours, but a relationship of trust can be built to allow us all to work together and accomplish great things.\n","permalink":"https://www.davidhaney.io/how-to-guarantee-dev-team-failure/","tags":["conflict","culture","ego","mistakes","process"],"title":"How To Guarantee Dev Team Failure"},{"categories":["blog"],"content":"Intro Node.js – it has rapidly become the “new hotness” in the tech start-up realm. With each passing day, the fan base of Node lovers grows larger, spreading their rhetoric like a religion. How do you spot a Node.js user? Don’t worry, they’ll let you know.\nOne day you’re at a regular user group meeting, sipping soda and talking with some colleagues, when the subject turns to Node. “Have you guys tried Node.js?” asks one of the people in your group. “It’s all the rage. All of the cool kids in Silicon Valley are using it!” “What does it do?” you ask, only to be bombarded with a sales pitch worthy of the best of used car lots. “Oh, it’s amazing!” they reply, sipping their diet coke and shuffling their hipster fedora and backpack with MacBook Pro in it (or something like that), “It’s server side JavaScript. It runs on a single thread and it can do 100,000 web requests a second!” They glance at the group for the oohs and ahhs, but most people just stare back with amazement in their eyes. Then, your hipster Node-loving friend drops the words that start wars: “It’s way better than .NET” – and just like that, your group is hooked. They go home, download the Node.js tools, write “Hello World”, and suddenly they’re on their way to the next user group meeting to talk about how great Node is.\nOkay, so I might be exaggerating the appearance and demeanour of your average Node lover a little (read: a lot, almost entirely in fact). However, I have had this exact scenario happen repeatedly over the last six months, with ever-increasing intensity and frequency. Node users love Node. They want you to love Node. They’re excited about it.\nHaving given it some thought, why wouldn’t Node.js developers be excited? Want to fire up a “Hello World” web server in Node? It’s trivial:\n// Load the http module to create an http server.\rvar http = require('http');\r// Configure our HTTP server to respond with Hello World to all requests.\rvar server = http.createServer(function (request, response) {\rresponse.writeHead(200, {\u0026quot;Content-Type\u0026quot;: \u0026quot;text/plain\u0026quot;});\rresponse.end(\u0026quot;Hello World\\n\u0026quot;);\r});\r// Listen on port 8000, IP defaults to 127.0.0.1\rserver.listen(8000);\rWant to do the same thing in .NET? Be prepared to learn about IIS, the Machine.config, the Web.config, the Process Model, how Global.asax works, either ASP.NET MVC or WebForms (huge paradigms in themselves), and how Visual Studio works. Don’t forget to learn how to create a solution, at least one web project, and how to deploy the application to IIS. Oh, and one little detail: go ahead and learn C# while you’re at it. All of that’s pretty much just as easy and intuitive as Node.js, right?\nComplexity Matters .NET is incredibly complicated. Node.js is incredibly simple. On the merits of that fact alone it’s no wonder that these .NET developers and fresh-out-of-college kids who have already dabbled in JavaScript are transferring these skills to the server side and standing up web servers in literally 5 minutes and 5 lines of code. How can you deny the sexiness of that? The bragging rights it gives you? The ease and comfort of a language you’re already familiar with?\nThis, in my opinion, is why Node.js is becoming huge. It has simplified and streamlined the development process and made programming very accessible to almost everyone (or at least anyone who has ever played with JavaScript).\nHowever, to those who sell Node.js as single-threaded, and to those who sell Node.js as having significantly better performance than .NET, I say this: you are wrong.\nMisunderstandings With simplicity comes misunderstanding and the concept of “Leaky Abstractions.” As my good friend and colleague Thomas B. said to me during dinner last week: Node.js is opinionated. It has an opinion on how you should do things, and it forces you to do them a certain way.\nNode.js is not single-threaded, though many Node developers in my experience believe it to be. Node’s creator believes that a single-threaded listener delegating I/O-bound work to a thread pool is the key to a highly available application. As a result, Node.js forces you into this paradigm of event-based asynchronous execution of I/O operations via a thread pool.\nNode.js has a single thread listening for connections. All of the code which you as the Node developer write is executed on this single thread. This single thread is all that is exposed to you. As soon as a connection is received, Node’s listening thread executes your coded event on the same listener thread. This event either does quick, non-CPU intensive work (like returning static content to a client), or long-running I/O bound operations (like reading data from a database). In the case of the former, the listener thread does in fact block for the duration of the request, but the request happens so quickly that the delay is trivial. In the case of the latter, Node uses V8 and libuv (which it is built upon) to delegate the I/O work to a thread from an underlying pool of native C++ threads. The single listening thread kicks off the work to an I/O worker thread with a callback that says “tell me when you’re done” and immediately returns to listening for the next connection. It is thus plain to see that Node.js is indeed multi-threaded, though this functionality is not directly exposed to the Node developer.\nAn important note regarding Node.js is that any CPU-intensive code which you write will block the entire system and make your application scale poorly or become entirely unresponsive. As a result, you would not want to use Node.js when you need to write an application that will do CPU-intensive work such as performing calculations or creating reports.\nThis is how a single thread can handle multiple requests at once; receiving a request and either serving static/simple content or delegating it to an I/O thread from a thread pool are both very cheap and quick operations. When the thread pool thread that is doing the long-running I/O work signals to the single listener thread that the work is done, the listener thread picks up the response and sends it back to the user; this is another very cheap operation. The core idea is that the single listener thread never blocks: it only does fast, cheap processing or delegation of requests to other threads and the serving of responses to clients. The diagram below (taken from Stack Overflow) explains this visually:\nNode.js Processing Model This is a very good, scalable, highly-available way to write code; Node.js nailed their approach and developers benefit from it. However, as of .NET 4.5, you can easily create this exact paradigm/pattern in your .NET applications. The difference is that .NET does not force you to do so.\nOWIN \u0026amp; A Better .NET With the introduction of a very tidy wrapper around asynchronous programming in .NET 4.5 (async/await keywords), Microsoft made asynchronous, event-based programming quite a bit easier and more intuitive. And with recent conformance by Microsoft to the jointly-created OWIN specification, the web pipeline of .NET has become much simpler.\nIn fact, you can now write the “Hello World” asynchronous web server in .NET in about as few lines as Node.js! In this example, I host a web server in a console application which is terminated when a key is pressed:\n/// \u0026lt;summary\u0026gt;\r/// A simple program to show off an OWIN self-hosted web app.\r/// \u0026lt;/summary\u0026gt;\rpublic class Program\r{\r/// \u0026lt;summary\u0026gt;\r/// The entry point for the console application.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;param name=\u0026quot;args\u0026quot;\u0026gt;The args to the console application. Ignored.\u0026lt;/param\u0026gt;\rstatic void Main(string[] args)\r{\r// Start OWIN host\rusing (WebApp.Start\u0026lt;Startup\u0026gt;(url: \u0026quot;http://localhost:8000\u0026quot;))\r{\r// Runs until a key is pressed\rConsole.ReadKey();\r}\r}\r/// \u0026lt;summary\u0026gt;\r/// This code configures the OWIN web app. The Startup class is specified as a /// type parameter in the WebApp.Start method.\r/// \u0026lt;/summary\u0026gt;\rprivate class Startup\r{\r/// \u0026lt;summary\u0026gt;\r/// Configures the web app.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;param name=\u0026quot;app\u0026quot;\u0026gt;The app builder.\u0026lt;/param\u0026gt;\rpublic void Configuration(IAppBuilder app)\r{\r// We ignore any rules here and just return the same response for any request\rapp.Run(context =\u0026gt;\r{\rcontext.Response.ContentType = \u0026quot;text/plain\u0026quot;;\rreturn context.Response.WriteAsync(\u0026quot;Hello World\\n\u0026quot;);\r} );\r}\r}\r}\rOne of the big positives of Node.js is that you opt-in to complexity. You start very simply and add on functionality as you need it. I’m a big fan of this approach and I feel that this is where Node really shines. Nothing bothers me more than starting an “Empty MVC 4 Web Application” from template in Visual Studio only to have it install about 15 Nuget packages, one of which is Entity Framework. Great, I fired up a blank application and already my ORM has been decided for me. Who said I even needed one in the first place?!\nThe above OWIN-based approach to hosting a web app in .NET allows you to benefit from Node’s simplistic approach. I have started out simply, and can now add Dapper if I need an ORM, Newtonsoft.Json if I need to serialize to and from JSON, Unity if I care about dependency injection, etc. It’s a nice, clean slate upon which I can build any framework that I desire.\nOWIN .NET vs Node.js The OWIN approach in .NET is very comparable to Node.js, with a few differences:\nNode.js uses 1 listener thread, while .NET uses N listener threads. If your Node.js application does CPU-intensive work at all, it will block the entire system and potentially cause your application to become unresponsive. .NET, on the other hand, is designed to do CPU intensive work. Tying up a thread to do some CPU work is not of concern because there are other threads available in the listener thread pool to take other requests while this is happening. However, both Node.js and .NET are limited by the server resources; in either case, if you max out the CPU or RAM, your app will perform horribly, regardless of thread delegation. This is known as resource contention. Node.js delegates I/O bound work to an I/O thread worker pool, and .NET implemented asynchronously (async methods and the async/await keywords) does the same. Node.js uses an event-based paradigm, and .NET does also when implemented asynchronously. Node.js offers high performance for I/O bound, low CPU operations, and .NET offers comparable performance when you skip the IIS pipeline. IIS tacks on a significant amount of performance overhead due to things like session state management, forms authentication, the process model, request lifecycle events, etc. These are not bad things to have and use, but if you don’t need IIS, session state, forms auth, request lifecycle events, or the process model, then don’t use them! Node.js must parse/serialize to and from JSON, and .NET must serialize to and from JSON to interact with .NET objects. Parsing is going to be much cheaper in Node.js than serializing is in .NET, but .NET also enables you to serialize to XML, Atom RSS, and anything else that you desire. With Node, this is a bit trickier, and the serialization overhead comes back into play to even the field. When someone compares Node.js to .NET, I find that they often actually compare Node.js to IIS hosted frameworks such as ASP.NET MVC, ASP.NET WebForms, and ASP.NET Web API (in IIS hosted mode). These are all tools built on top of ASP.NET to simplify enterprise web development and to do CPU-intensive calculations. In these scenarios, Node.js will have an advantage, because it is designed specifically to NOT do CPU-intensive calculations. You are effectively comparing CPU-intensive Apples to low-CPU-usage Oranges. It is not a fair comparison.\nWhen someone compares Node.js to a self-hosted .NET web app which does I/O-bound long-running operations via asynchronous thread pool delegation, they find that there is not much of a difference in performance between the two runtimes. In fact, comparing Node.js to self-hosted Web API (NOT using IIS) doing low-CPU work, the numbers are very close:\nWebAPI vs Node.js This image was taken from a benchmark done in 2012 with the Web API Release Candidate (not Web API 2, and NOT OWIN hosted). Given that Web API 2 exists, and can be self-hosted via OWIN, I’d love to see a more recent benchmark comparing the two. In fact, I will try and do this for my next blog post.\nFinal Thoughts I guess the point of all of this has been that neither Node.js or .NET is necessarily better/the best/the new hotness. They both serve a purpose, and while Node.js is much easier to use and more accessible to developers, .NET is very versatile and powerful as well. They are built to do different things: Node.js specializes in performing and scaling well for low-CPU, highly I/O-bound operations. .NET can perform well in this scenario as well, but can also perform well with high-CPU operations. In fact, I would argue that .NET excels at CPU-intensive operations, especially when compared to Node.\nThere are many .NET developers in the current tech scene that are capable and competent. This means that it’s not too hard to find, hire, and retain good .NET talent. They can pick up self-hosted OWIN web apps in a matter of days, and begin to write very scalable, high-performance web apps and services based on it. They can even easily host Web API in an OWIN web app via console, a Windows service, or Azure. There’s a community that has existed for over a decade that evolves the .NET framework with amazing tools and add-ons like Dapper, Unity, and Newtonsoft.Json. This community is mature and there are many prominent voices within it that offer advice and help.\nRelative to .NET, there aren’t as many Node.js developers in the current tech scene that are capable and competent. This is because fortune favours the bold, and only a few have been early adopters of Node.js. In my experience, few Node developers will truly understand what is going on in the Node.js processing model and how to exploit it for maximum performance, though the opinionated paradigm of Node.js will force developers to write good asynchronous code. It can be hard to find, hire, and retain good Node.js talent. This will become less of a concern as Node’s following grows. The Node.js community is budding and has created some amazing tools and add-ons for Node.js as well such as ORMs and DI frameworks. This community is not yet mature and I am not aware of many prominent voices within it that offer advice and help. As a result, it could be difficult to find support and tools for Node.js if you encounter a problem.\nConclusions In conclusion, both Node.js and .NET are great. Which one to pick for a particular solution/application, however, depends on many factors; it is not black and white but a full colour spectrum. It would be very foolish and naive for a .NET developer to use .NET to solve every single problem just because “that’s what we use.” It would be similarly foolish for a Node.js developer to propose Node.js as a solution for every project or problem that he or she encounters. One must choose the right tool for a given job, and be open to different paradigms in order to truly excel.\nIn general, use Node.js when you have highly I/O-bound operations that don’t use much CPU. Use .NET when you need to calculate things and use a lot of CPU.\nDon’t use Node.js solely on the reasoning that it’s much faster and performs way better than .NET: it depends on how you use .NET. And don’t use .NET if all you’re doing is heavily I/O-bound operations with low CPU usage: that’s where Node.js excels.\n","permalink":"https://www.davidhaney.io/to-node-js-or-not-to-node-js/","tags":["dotnet","analysis","node.js","performance","programming"],"title":"To Node.js Or Not To Node.js"},{"categories":["blog"],"content":"It looks as if the Visual Studio dev team may be implementing a new operator in a future .NET release. This is due in large part to community demand, which is pretty cool because it shows that the VS team is listening to their customer base; a key part of a successful product.\nThis new operator is likely going to take the syntax of ?. and is known as the Safe Navigation Operator.\nIts purpose is rather simple, and it eliminates a load of boiler-plate code by making the compiler do it for you.\nSay you have these classes:\npublic class A\r{\rpublic B B { get; set; }\r}\rpublic class B\r{\rpublic C C { get; set; }\r}\rpublic class C\r{\rpublic int D { get; set; }\r}\rIt’s plain to see above that it’s possible that an instance of A may have a null B property, or even that an instance of B may have a null C property. If you want to get D from A safely (so as to avoid null reference exceptions), your code often ends up looking something like this:\nint? result = A.B == null ? (int?)null : (A.B.C == null ? (int?)null : A.B.C.D);\rWhat an incredibly annoying pain to write out, and even to read. You could maybe make it clearer, at the cost of more verbosity and lines of code:\nint? result = null;\rif (A.B != null)\r{\rif (A.B.C != null)\r{\rresult = A.B.C.D;\r}\r}\rThis is still a bit convoluted and boiler-plate for my liking. Really, all we want to do in English is get the integer value of D if it exists on A. It shouldn’t be so challenging!\nEnter the new Safe Navigation Operator and how it might function:\nint? result = A?.B?.C.D;\rNote the ?. syntax that is placed at the point where the A instance references B and B instance references C. This is much more readable, and intuitive to look at. To me, it says “if A’s B is not null and B’s C is not null, return D, else return null” which is pretty streamlined. Upon simple inspection, it also clearly says that A.B is nullable and B.C is nullable, which are great side effects of the shortened syntax.\nNote that the final product, if included in C#, may behave differently than I’ve demonstrated and hypothesized here. The code in this post is mostly made on assumptions of compiler rules for this operator, and could in the future prove to be wrong. However, even still, this operator will be a great addition to C# and .NET if it makes the cut!\n","permalink":"https://www.davidhaney.io/c-probably-getting-new-safe-navigation-operator/","tags":["dotnet","csharp","programming"],"title":"C# Probably Getting New \"Safe Navigation\" Operator \"?.\""},{"categories":["blog"],"content":"I recently came across an ASP.NET MVC issue at work where the validation for my Model was not firing correctly. The Model implemented the IValidatableObject interface and thus the Validate method which did some specific logic to ensure the state of the Model (the ModelState). This Model also had some DataAnnotation attributes on it to validate basic input.\nLong story short, the issue I encountered was that when ModelState.IsValid == false due to failure of the DataAnnotation validation, the IValidatableObject.Validate method is not fired, even though I needed it to be. This problem arose due to a rare situation in which ModeState.IsValid was initially false but was later set to true in the Controller’s Action Method by some logic that removed errors from the ModelState.\nI did some research and learned that the DefaultModelBinder of ASP.NET MVC short-circuits it’s logic: if the ModelState is not valid (AKA is false), the IValidatableObject logic that runs the Validate method is never fired.\nTo thwart this, I created a custom Model Binder, a custom Model Binder Provider (to serve my custom Model Binder), and then registered the Model Binder Provider in the Application_Start method of Global.asax.cs. Here’s the code for a custom Model Binder that always fires the IValidatableObject.Validate method, even if ModelState.IsValid == false:\nForceValidationModelBinder:\n/// \u0026lt;summary\u0026gt;\r/// A custom model binder to force an IValidatableObject to execute /// the Validate method, even when the ModelState is not valid.\r/// \u0026lt;/summary\u0026gt;\rpublic class ForceValidationModelBinder : DefaultModelBinder\r{\rprotected override void OnModelUpdated(ControllerContext controllerContext, ModelBindingContext bindingContext)\r{\rbase.OnModelUpdated(controllerContext, bindingContext);\rForceModelValidation(bindingContext);\r}\rprivate static void ForceModelValidation(ModelBindingContext bindingContext)\r{\r// Only run this code for an IValidatableObject model\rIValidatableObject model = bindingContext.Model as IValidatableObject;\rif(model == null)\r{\r// Nothing to do\rreturn;\r}\r// Get the model state\rModelStateDictionary modelState = bindingContext.ModelState;\r// Get the errors\rIEnumerable\u0026lt;ValidationResult\u0026gt; errors = model.Validate(new ValidationContext(model, null, null));\r// Define the keys and values of the model state\rList\u0026lt;string\u0026gt; modelStateKeys = modelState.Keys.ToList();\rList\u0026lt;ModelState\u0026gt; modelStateValues = modelState.Values.ToList();\rforeach (ValidationResult error in errors)\r{\r// Account for errors that are not specific to a member name\rList\u0026lt;string\u0026gt; errorMemberNames = error.MemberNames.ToList();\rif (errorMemberNames.Count == 0)\r{\r// Add empty string for errors that are not specific to a member name\rerrorMemberNames.Add(string.Empty);\r}\rforeach (string memberName in errorMemberNames)\r{\r// Only add errors that haven't already been added.\r// (This can happen if the model's Validate(...) method is called more than once, which will happen when there are no property-level validation failures)\rint index = modelStateKeys.IndexOf(memberName);\r// Try and find an already existing error in the model state\rif(index == -1 || !modelStateValues[index].Errors.Any( i =\u0026gt; i.ErrorMessage == error.ErrorMessage))\r{\r// Add error\rmodelState.AddModelError(memberName, error.ErrorMessage);\r}\r}\r}\r}\r}\rForceValidationModelBinderProvider:\n/// \u0026lt;summary\u0026gt;\r/// A custom model binder provider to provide a binder that forces an IValidatableObject to execute the Validate method, even when the ModelState is not valid.\r/// \u0026lt;/summary\u0026gt;\rpublic class ForceValidationModelBinderProvider : IModelBinderProvider\r{\rpublic IModelBinder GetBinder(Type modelType)\r{\rreturn new ForceValidationModelBinder();\r}\r}\rGlobal.asax.cs:\nprotected void Application_Start()\r{\r// Register the force validation model binder provider\rModelBinderProviders.BinderProviders.Clear();\rModelBinderProviders.BinderProviders.Add(new ForceValidationModelBinderProvider());\r} ","permalink":"https://www.davidhaney.io/trigger-ivalidatableobject-validate-when-modelstate-isvalid-is-false/","tags":["dotnet","csharp","mvc","programming"],"title":"Trigger IValidatableObject.Validate When ModelState.IsValid is false"},{"categories":["blog"],"content":"Jonathan Allen of InfoQ conducted an interview with me about one of my open source initiatives, SimplSockets. We discussed the value of TCP over HTTP and why Sockets are still relevant to programming.\nI’d like to thank Jonathan and InfoQ for the opportunity – it was a great discussion. Check it out here: http://www.infoq.com/news/2013/12/SimplSockets\r","permalink":"https://www.davidhaney.io/interview-with-infoq-on-simplsockets/","tags":["interviews","open source","programming"],"title":"Interview with InfoQ on SimplSockets"},{"categories":["blog"],"content":"MVC4 made one simple and yet awesome improvement to View rendering that I don’t think many people are aware of.\nHave you ever had to conditionally add an attribute to an HTML element in your MVC View based on the presence of a variable? The typical use case is applying a CSS class to a div. Most of the time that code looks something like this:\n\u0026lt;div @(myClass == null ? \u0026quot;\u0026quot; : \u0026quot;class=\\\u0026quot;\u0026quot; + myClass + \u0026quot;\\\u0026quot;\u0026quot;)\u0026gt;\u0026lt;/div\u0026gt;\rWhat a pain – not only to write but to read… This destroys the View’s readability and clutters the HTML up big time!\nIn MVC4, this process is much simpler. Just do this instead:\n\u0026lt;div class=\u0026quot;@myClass\u0026quot;\u0026gt;\u0026lt;/div\u0026gt;\rThat’s all you need. If myClass == null it will ignore the class attribute and render without the class:\n\u0026lt;div\u0026gt;\u0026lt;/div\u0026gt;\rIf myClass != null it will apply the class to the div:\n\u0026lt;div class=\u0026quot;test\u0026quot;\u0026gt;\u0026lt;/div\u0026gt;\rThis should work with other HTML element attributes also, though I haven’t tested them all myself. What a great little improvement!\n","permalink":"https://www.davidhaney.io/mvc4-conditional-html-attributes/","tags":["dotnet","csharp","mvc","programming","razor"],"title":"MVC4 Conditional HTML Attributes"},{"categories":["blog"],"content":"The T4 template engine is insanely powerful. I didn’t really realize just how powerful it was until I had a use case for it today. I stood up a database with about 40 tables in it, and planned to use an ORM to access the database. To use the ORM, I needed POCOs (Plain Old C# Objects) that represented my database. Some of these tables had 30-50 or so columns and I didn’t want to code all of this by hand – it would take literally days.\nSurprisingly, I got the whole thing done in about an hour with the help of the T4 template engine.\nFor those who are not familiar, T4 is a text template engine created by Microsoft which combines plain text and control logic to generate text output. In reality it’s a lot like how you use Razor or WebForms view engines to generate HTML; you can embed “code nuggets” almost exactly like you do in WebForms. The only real difference is that with T4 you’re creating a text file instead of a webpage.\nTo create a T4 template in Visual Studio 2010 or 2012, simply add a text file to your project… I called mine PocoGenerator.txt for example. Then, rename the file’s extension from “.txt” to “.tt” – the file will then be treated as a T4 Template by Visual Studio. Your output will appear in the code-behind .cs file attached to the .tt file you just created. In my scenario I wanted each of my POCOs to have their own file, so I did a bit of trickery to make that happen.\nI wrote this nifty T4 template which connects to a database, queries all tables (ignoring sys tables), and then creates one POCO per table under a file named .cs which is placed in a directory relative to the template’s location. Give this a go – you won’t be disappointed! And of course, if you need it to do more than I do, just modify it and make it your own!\n\u0026lt;#@ template language=\u0026quot;C#\u0026quot; hostspecific=\u0026quot;true\u0026quot; debug=\u0026quot;True\u0026quot; #\u0026gt;\r\u0026lt;#@ assembly name=\u0026quot;System.Core\u0026quot; #\u0026gt;\r\u0026lt;#@ assembly name=\u0026quot;System.Data\u0026quot; #\u0026gt;\r\u0026lt;#@ assembly name=\u0026quot;System.Xml\u0026quot; #\u0026gt;\r\u0026lt;#@ assembly name=\u0026quot;Microsoft.SqlServer.Smo\u0026quot; #\u0026gt;\r\u0026lt;#@ assembly name=\u0026quot;Microsoft.SqlServer.ConnectionInfo\u0026quot; #\u0026gt;\r\u0026lt;#@ assembly name=\u0026quot;Microsoft.SqlServer.Management.Sdk.Sfc\u0026quot; #\u0026gt;\r\u0026lt;#@ import namespace=\u0026quot;System\u0026quot; #\u0026gt;\r\u0026lt;#@ import namespace=\u0026quot;System.IO\u0026quot; #\u0026gt;\r\u0026lt;#@ import namespace=\u0026quot;System.Linq\u0026quot; #\u0026gt;\r\u0026lt;#@ import namespace=\u0026quot;System.Text\u0026quot; #\u0026gt;\r\u0026lt;#@ import namespace=\u0026quot;Microsoft.SqlServer.Management.Smo\u0026quot; #\u0026gt;\r\u0026lt;#\r//**********************************************************************************************\r// This T4 generates POCOs from the specified DB and saves them to the specified folder which // is relative to the template's location. One file per table/POCO.\r//**********************************************************************************************\r//****************************\r// DEFINE YOUR VARIABLES HERE\r//****************************\r// The SQL server name or IP\rstring sqlServer = \u0026quot;9.9.9.9\u0026quot;;\r// The SQL username\rstring sqlLogin = \u0026quot;admin\u0026quot;;\r// The SQL password\rstring sqlPassword = \u0026quot;password\u0026quot;;\r// The SQL database to generate the POCOs for\rstring sqlDatabase = \u0026quot;MyDatabase\u0026quot;;\r// The namespace to apply to the generated classes\rstring classNamespace = \u0026quot;Your.Namespace.Here\u0026quot;;\r// The destination folder for the generated classes, relative to this file's location.\rstring destinationFolder = \u0026quot;PocoFolder\u0026quot;;\r// Loop over each table and create a class file!\rServer server = new Server(sqlServer);\rserver.ConnectionContext.LoginSecure = false;\rserver.ConnectionContext.Login = sqlLogin;\rserver.ConnectionContext.Password = sqlPassword;\rserver.ConnectionContext.Connect();\rforeach (Table table in server.Databases[sqlDatabase].Tables)\r{\r// Skip sys tables\rif (table.Name.StartsWith(\u0026quot;sys\u0026quot;))\r{\rcontinue;\r}\r#\u0026gt;\rusing System;\rnamespace \u0026lt;#= classNamespace #\u0026gt;\r{\r/// \u0026lt;summary\u0026gt;\r/// Represents a \u0026lt;#= table.Name #\u0026gt;.\r/// NOTE: This class is generated from a T4 template - you should not modify it manually.\r/// \u0026lt;/summary\u0026gt;\rpublic class \u0026lt;#= table.Name #\u0026gt; {\r\u0026lt;# // Keep count so we don't whitespace the last property/column\rint columnCount = table.Columns.Count;\rint i = 0;\r// Iterate all columns\rforeach (Column col in table.Columns)\r{\ri++;\rstring propertyType = GetNetDataType(col.DataType.Name);\r// If we can't map it, skip it\rif (string.IsNullOrWhiteSpace(propertyType))\r{\r// Skip\rcontinue;\r}\r// Handle nullable columns by making the type nullable\rif (col.Nullable \u0026amp;\u0026amp; propertyType != \u0026quot;string\u0026quot;)\r{\rpropertyType += \u0026quot;?\u0026quot;;\r}\r#\u0026gt;\rpublic \u0026lt;#= propertyType #\u0026gt; \u0026lt;#= col.Name #\u0026gt; { get; set; }\r\u0026lt;#\r// Do we insert the space?\rif (i != columnCount)\r{\r#\u0026gt;\r\u0026lt;#\r}\r#\u0026gt;\r\u0026lt;#\r}\r#\u0026gt;\r}\r} \u0026lt;#\r// Write new POCO class to its own file\rSaveOutput(table.Name + \u0026quot;.cs\u0026quot;, destinationFolder);\r} #\u0026gt;\r\u0026lt;#+\rpublic static string GetNetDataType(string sqlDataTypeName)\r{\rswitch (sqlDataTypeName.ToLower())\r{\rcase \u0026quot;bigint\u0026quot;:\rreturn \u0026quot;Int64\u0026quot;;\rcase \u0026quot;binary\u0026quot;:\rcase \u0026quot;image\u0026quot;:\rcase \u0026quot;varbinary\u0026quot;:\rreturn \u0026quot;byte[]\u0026quot;;\rcase \u0026quot;bit\u0026quot;:\rreturn \u0026quot;bool\u0026quot;;\rcase \u0026quot;char\u0026quot;:\rreturn \u0026quot;char\u0026quot;;\rcase \u0026quot;datetime\u0026quot;:\rcase \u0026quot;smalldatetime\u0026quot;:\rreturn \u0026quot;DateTime\u0026quot;;\rcase \u0026quot;decimal\u0026quot;:\rcase \u0026quot;money\u0026quot;:\rcase \u0026quot;numeric\u0026quot;:\rreturn \u0026quot;decimal\u0026quot;;\rcase \u0026quot;float\u0026quot;:\rreturn \u0026quot;double\u0026quot;;\rcase \u0026quot;int\u0026quot;:\rreturn \u0026quot;int\u0026quot;;\rcase \u0026quot;nchar\u0026quot;:\rcase \u0026quot;nvarchar\u0026quot;:\rcase \u0026quot;text\u0026quot;:\rcase \u0026quot;varchar\u0026quot;:\rcase \u0026quot;xml\u0026quot;:\rreturn \u0026quot;string\u0026quot;;\rcase \u0026quot;real\u0026quot;:\rreturn \u0026quot;single\u0026quot;;\rcase \u0026quot;smallint\u0026quot;:\rreturn \u0026quot;Int16\u0026quot;;\rcase \u0026quot;tinyint\u0026quot;:\rreturn \u0026quot;byte\u0026quot;;\rcase \u0026quot;uniqueidentifier\u0026quot;:\rreturn \u0026quot;Guid\u0026quot;;\rdefault:\rreturn null;\r}\r}\rvoid SaveOutput(string outputFileName, string destinationFolder)\r{\r// Write to destination folder\rstring templateDirectory = Path.Combine(Path.GetDirectoryName(Host.TemplateFile), destinationFolder);\rstring outputFilePath = Path.Combine(templateDirectory, outputFileName);\rFile.Delete(outputFilePath);\rFile.WriteAllText(outputFilePath, this.GenerationEnvironment.ToString()); // Flush generation\rthis.GenerationEnvironment.Remove(0, this.GenerationEnvironment.Length);\r}\r#\u0026gt;\rNote that when the files are generated, they will not automatically be included in the project. You will have to add them manually as existing items.\nThis T4 Template supports regeneration, so anytime you update your database schema just re-run the template to create updated .cs files! Enjoy!\n","permalink":"https://www.davidhaney.io/automatically-generate-pocos-from-db-with-t4/","tags":["dotnet","csharp","programming","sql","t4","visual studio"],"title":"Automatically Generate POCOs From DB With T4"},{"categories":["blog"],"content":"If you’re using the Web API as part of the MVC4 framework, you may encounter a scenario in which you must map parameters of strange names to variables for which characters of the name would be illegal. That wasn’t very clear, so let’s do this by example. Consider part of the Facebook API:\nFirstly, Facebook servers will make a single HTTP GET to your callback URL when you try to add or modify a subscription. A query string will be appended to your callback URL with the following parameters:\nhub.mode – The string “subscribe” is passed in this parameter hub.challenge – A random string hub.verify_token – The verify_token value you specified when you created the subscription Now if we wanted to use Web API to receive this data, we know that C# does not support the decimal character . existing in variable names. So, how do we bind this querystring data to variables of a different name?\nAfter a lot of searching, I discovered that the answer is surprisingly simple – just use the FromUriAttribute:\npublic string Get([FromUri(Name=\u0026quot;hub.mode\u0026quot;)]string mode, [FromUri(Name=\u0026quot;hub.challenge\u0026quot;)]string challenge, [FromUri(Name=\u0026quot;hub.verify_token\u0026quot;)]string verifyToken)\r{\r/* method body */\r}\rWorks like a charm!\n","permalink":"https://www.davidhaney.io/web-api-mapping-querystringform-input/","tags":["dotnet","csharp","mvc","programming","webapi"],"title":"Web API Mapping QueryString/Form Input"},{"categories":["blog"],"content":"Have you ever had to write a comparer for a specific type, only to be frustrated when you needed to write a second and third comparer for other types? Fear not, a generic comparer can take care of this for you!\n/// \u0026lt;summary\u0026gt;\r/// Compares two objects of any type.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;typeparam name=\u0026quot;T\u0026quot;\u0026gt;The type to be compared.\u0026lt;/typeparam\u0026gt;\rpublic class GenericComparer\u0026lt;T\u0026gt; : IComparer\u0026lt;T\u0026gt;\r{\r// The compare method\rprivate readonly Func\u0026lt;T, T, int\u0026gt; _compareMethod = null;\r/// \u0026lt;summary\u0026gt;\r/// The constructor.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;param name=\u0026quot;compareMethod\u0026quot;\u0026gt;The compare method.\u0026lt;/param\u0026gt;\rpublic GenericComparer(Func\u0026lt;T, T, int\u0026gt; compareMethod)\r{\r// Sanitize\rif (compareMethod == null)\r{\rthrow new ArgumentNullException(\u0026quot;compareMethod\u0026quot;);\r}\r_compareMethod = compareMethod;\r}\r/// \u0026lt;summary\u0026gt;\r/// Compares two objects.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;param name=\u0026quot;x\u0026quot;\u0026gt;The first object.\u0026lt;/param\u0026gt;\r/// \u0026lt;param name=\u0026quot;y\u0026quot;\u0026gt;The second object.\u0026lt;/param\u0026gt;\r/// \u0026lt;returns\u0026gt;Less than 0 if x is less than y, greater than /// 0 if x is greater than y, 0 if they are equal.\u0026lt;/returns\u0026gt;\rpublic int Compare(T x, T y)\r{\rreturn _compareMethod(x, y);\r}\r}\rJust pass a method to the constructor that takes 2 objects of type T and returns an int, and you’re all set!\n","permalink":"https://www.davidhaney.io/generic-comparer/","tags":["dotnet","csharp","generics","programming"],"title":"Generic Comparer"},{"categories":["blog"],"content":"I wanted to point people to this link at DotNetPearls:\nhttp://www.dotnetperls.com/binarysearch\rThey do an excellent, quick demonstration of List\u0026lt;T\u0026gt;.BinarySearch and show a graph that really drives home how much faster it is for large lists than a regular traversal!\n","permalink":"https://www.davidhaney.io/one-more-thing-about-list-binary-search/","tags":["dotnet","csharp","memory","performance","programming"],"title":"One More Thing About List Binary Search"},{"categories":["blog"],"content":"One of the many things that I do at work is run a full-blown Search Engine which I also developed from scratch. This Search Engine feeds all product related information to our websites. A search index consists of a pre-computed collection of products, their properties, a list of words that are correctly spelled, and some pre-computed faceted/guided navigation. A search index, until this week, took up approximately 10.7 gigs of memory. This was becoming too large as we added new products every single day.\nAs of writing this, it now takes only 4.8 gigs of memory and is only slightly (1-3%) less performant than before. How did I do it? Believe it or not, a very simple data structure and algorithm change.\nIn the Search Engine, a product’s properties are a key-value pairing of strings… Things like “isInStock” “1” or “color” “red” etc. We store the properties in a collection, per product. The collection was originally:\nDictionary\u0026lt;string, HashSet\u0026lt;string\u0026gt;\u0026gt; _entityProperties;\rThe key of the Dictionary was the property name and the HashSet of strings were the values for that property name (property names are not a “unique” key – a product could have multiple colors for example). I initially chose this data structure because we have a heavy need for DIRECT lookups to property names and values. Methods like HasProperties(string propertyName) and HasProperty(string propertyName, string propertyValue) are essential to the search engine’s function, and need to be performant. Thus, I figured that a Dictionary and HashSet would be best, since both offer O(1) lookup times and the index is read from 10000x more often than it is written to. O(1 + 1) is pretty good when it comes to complexity.\nIt turns out that there was a simpler, better data structure for my goals which also satisfies the performance requirements of the aforementioned methods.\nAs you may or may not know (and I learned the hard way), a HashSet\u0026lt;T\u0026gt; is actually not very efficient when you have only a few items in it. A List\u0026lt;T\u0026gt; is actually more performant for small collections (4 or fewer objects with simple GetHashCode() methods, such as strings, in my testing). This is true even though your average lookup/read case goes from O(1) to (1/2n) since you must traverse the List to find your desired object. The reason that List is faster is that there is no hash key computation, and the List\u0026lt;T\u0026gt; is basically an elastic array and thus takes less memory and has less overhead than a HashSet\u0026lt;T\u0026gt; with the same number of objects in it. Since my product properties typically only consist of 2 or 3 values for a given property name, I changed my data structure to this:\nDictionary\u0026lt;string, List\u0026lt;string\u0026gt;\u0026gt; _entityProperties;\rThis shaved approximately 10% off of the memory footprint and brought my memory usage down to 9.6 gigs. The performance was basically identical in all performance tests. This was better than my HashSet, but still not great. I wanted to do better. I was sure that somehow I could do better.\nI spent the good part of this week trying – and failing – to design a more efficient data structure than the above. I tried a string Trie with nodes that pointed to another Trie, I tried SortedList\u0026lt;TKey, TValue\u0026gt; instead of the above, and everything else that I could think of. Yet no matter what I did, the memory stayed the same and the performance was the same or worse. It sucked. I was still sure that somehow I could do better.\nFinally, Wednesday morning, I had a thought in the shower (where I do my best thinking): two dimensional Arrays suck. They are well documented to, in general, have worse memory usage metrics than a one dimensional array (a quick Google will fill you in). A Dictionary of Lists is certainly a two dimensional jagged Array of sorts, and it wasn’t doing what I wanted in terms of memory. So, I took another approach and changed my data structure wildly – I flattened it out and made it one dimensional:\nList\u0026lt;KeyValuePair\u0026lt;string, string\u0026gt;\u0026gt; _entityProperties;\rSeems insane, right? I go from a Dictionary with an O(1) key lookup to a linear List of all keys and values stored together. And yet, it did the trick for my memory: it went from 9.6 gigs to 4.8 gigs. Half of the amount of memory used. I was stoked.\nI saved this memory by both interning strings and taking advantage of the KeyValuePair being a struct. Structs are a lot more efficient than reference types when the object is small, and a KeyValuePair is indeed quite small.\nA new problem needed solving, however. Each product has around 60-100 properties associated with it, and I needed them to be accessed efficiently and quickly with near-direct lookups. Traversing the [now giant] List was not acceptable in terms of performance.\nAs it stood, I went from an O(1 + 1) data structure (key and value lookup costs for Dictionary and HashSet) to an O(1 + 1/2n) data structure (Dictionary and List) and finally to an O(n) data structure (List). And to top it all off, the n in the 1/2n was 3 or 4 where the n in the flat List of KeyValuePair was between 60 and 100. Truly I was getting worse performance with each improvement – at least theoretically. Still, the allure of the memory savings was too great to ignore and I wanted to use this data structure.\nIt then hit me: why not use BinarySearch on the List\u0026lt;T\u0026gt; to look up items quickly and keep the List sorted while I add AND be able to check for duplicates before adding? It was certainly worth a shot, since binary search is an O(log n) algorithm which is an improvement over the List’s O(n) traversal. So, I modified my Add(string propertyName, string propertyValue) method to keep the List sorted as things were added to it. This is surprisingly easy to do.\nnote that from here on out I’m simplifying my code greatly to basic concepts from what actually exists in order to avoid giving away trade secrets or violating my NDA and being fired\npublic void Add(string propertyName, string propertyValue)\r{ // TODO: null checks, etc.\rvar keyValuePair = new KeyValuePair\u0026lt;string, string\u0026gt;(propertyName, propertyValue);\r// Add property name and value\r// First find the identical item if it exists\rvar result = _entityProperties.BinarySearch(keyValuePair, _entityPropertiesComparer);\r// If result is \u0026gt;= 0, already exists, done\rif (result \u0026gt;= 0)\r{\rreturn;\r}\r// If it does not exist, a one's complement of the returned int tells us WHERE to insert to maintain the sort order\r_entityProperties.Insert(~result, keyValuePair);\r}\rThe secret here is two-fold:\nI created a custom KeyValuePair\u0026lt;string, string\u0026gt; comparer class that implements IComparer\u0026lt;KeyValuePair\u0026lt;string, string\u0026gt;\u0026gt; and basically does a case-insensitive string compare of first the key strings, then the value strings. This IComparer is required by the List’s BinarySearch method to determine the ordering of objects in the List.\nThe BinarySearch method returns a very useful value: a simple integer. If the int is \u0026lt; 0, it means that the item was not found in the List. If it is \u0026gt;= 0, it means that the item was found at the index of the value. If it returns a negative integer, it means not only that it was not found, but also that the proper index to insert the item at in order to keep the List sorted is the one’s complement of the value.\nA super useful return type, indeed. This allows you to add your elements to a List while preserving an order, at the cost of your add being an O(log n) operation instead of a List’s usual O(1) add operation. However, if you don’t add things as much as you read the List (we only adjust the index once a day for example, but read it thousands of times per hour), this can be worthwhile. Additionally, you could add everything in O(1) time and then do a final List Sort in order to sort the List for a single O(log n) cost if the order of elements does not matter until you’re done adding everything. In my case, the order mattered as I added to the List because I did not ever want to add duplicates (same property name and value). The HashSet handles this for me – Lists do not.\nSo, now my add costs O(log n) instead of O(n), but the payoff is now my lookups cost O(log n) instead of O(n) as well. I adjusted my earlier mentioned HasProperty and HasProperties methods accordingly:\npublic List\u0026lt;string\u0026gt; GetSpecificPropertyValues(string propertyName)\r{\r// TODO: null checks, etc.\rList\u0026lt;string\u0026gt; result = new List\u0026lt;string\u0026gt;();\r// Binary search the property name - null is the smallest value of string for comparison\rvar keyValuePair = new KeyValuePair\u0026lt;string, string\u0026gt;(propertyName, null);\r// One's complement the start index\rvar startIndex = ~_entityProperties.BinarySearch(keyValuePair, _entityPropertiesComparer);\rfor (int i = startIndex; i \u0026lt; _entityProperties.Count; i++)\r{\r// Leave the loop when the property name no longer matches\rif (!string.Equals(propertyName, _entityProperties[i].Key, StringComparison.OrdinalIgnoreCase))\r{\r// Leave the loop\rbreak;\r}\rresult.Add(_entityProperties[i].Value);\r}\rreturn result;\r}\rpublic bool HasProperty(string propertyName, string propertyValue)\r{\r// TODO: null checks, etc.\r// Binary search the property name\rvar keyValuePair = new KeyValuePair\u0026lt;string, string\u0026gt;(propertyName, propertyValue);\rvar startIndex = _entityProperties.BinarySearch(keyValuePair, _entityPropertiesComparer);\rreturn startIndex \u0026gt;= 0;\r}\rpublic bool HasProperties(string propertyName)\r{\r// TODO: null checks, etc.\r// Binary search the property name\rvar keyValuePair = new KeyValuePair\u0026lt;string, string\u0026gt;(propertyName, null);\r// One's complement the start index\rvar startIndex = ~_entityProperties.BinarySearch(keyValuePair, _entityPropertiesComparer);\rif (startIndex \u0026gt;= _entityProperties.Count)\r{\rreturn false;\r}\r// Check that the next element matches the property name\rreturn string.Equals(propertyName, _entityProperties[startIndex].Key, StringComparison.OrdinalIgnoreCase);\r}\rSuddenly, I have the same “direct lookup” methods available as I did with my Dictionary and HashSet/List structure, but in a flat List with O(log n) complexity.\nThis yielded 50% less memory usage and only a 1-3% increase in performance times. A very acceptable trade for the Search Engine.\nIf you have a List\u0026lt;T\u0026gt; with a lot of objects in it, and performance is key to your application, consider using BinarySearch and/or Sort to access it in a much more efficient way. As long as you can create an IComparer\u0026lt;T\u0026gt; where T is your List objects type, you’ll have a more efficient List.\n","permalink":"https://www.davidhaney.io/make-mostly-read-seldom-written-lists-much-more-efficient/","tags":["dotnet","csharp","memory","performance","programming"],"title":"Make Mostly Read, Seldom-Written Lists Much More Efficient"},{"categories":["blog"],"content":"In the interest of self-improvement and sharing knowledge, I felt that I should share an update to my last post. I discovered a slightly better way to create the GetMimeMapping delegate/method via reflection that involves less casting and overhead, and is more Object Oriented in a sense. It allows the signature of the reflected method to be Func\u0026lt;string, string\u0026gt; instead of MethodInfo. Code below, note the use of Delegate.CreateDelegate(Type, MethodInfo):\n/// \u0026lt;summary\u0026gt;\r/// Exposes the Mime Mapping method that Microsoft hid from us.\r/// \u0026lt;/summary\u0026gt;\rpublic static class MimeMappingStealer\r{\r// The get mime mapping method\rprivate static readonly Func\u0026lt;string, string\u0026gt; _getMimeMappingMethod = null;\r/// \u0026lt;summary\u0026gt;\r/// Static constructor sets up reflection.\r/// \u0026lt;/summary\u0026gt;\rstatic MimeMappingStealer()\r{\r// Load hidden mime mapping class and method from System.Web\rvar assembly = Assembly.GetAssembly(typeof(HttpApplication));\rType mimeMappingType = assembly.GetType(\u0026quot;System.Web.MimeMapping\u0026quot;);\r_getMimeMappingMethod = (Func\u0026lt;string, string\u0026gt;)Delegate.CreateDelegate(typeof(Func\u0026lt;string, string\u0026gt;), mimeMappingType.GetMethod(\u0026quot;GetMimeMapping\u0026quot;, BindingFlags.Instance | BindingFlags.Static | BindingFlags.Public |\rBindingFlags.NonPublic | BindingFlags.FlattenHierarchy));\r}\r/// \u0026lt;summary\u0026gt;\r/// Exposes the hidden Mime mapping method.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;param name=\u0026quot;fileName\u0026quot;\u0026gt;The file name.\u0026lt;/param\u0026gt;\r/// \u0026lt;returns\u0026gt;The mime mapping.\u0026lt;/returns\u0026gt;\rpublic static string GetMimeMapping(string fileName)\r{\rreturn _getMimeMappingMethod(fileName);\r}\r} ","permalink":"https://www.davidhaney.io/a-better-mime-mapping-stealer/","tags":["dotnet","csharp","performance","programming","reflection"],"title":"A Better MIME Mapping Stealer!"},{"categories":["blog"],"content":"I recently had a need, in an ASP.NET MVC3 application, to read raw HTML, CSS, JS, and image files from disk and return them to the user… A sort of “pass-through” if you will. Normally I’d have simply routed to a custom HTTP handler per file type or just allowed MVC3 to map existing files to supply its own .NET HTTP handlers and do all of this work for me, but in this case I needed the mapped “directory” to switch behind the scenes based on Session settings… So I ultimately had to feed these files through a Controller and Action Method to gain access to the Session.\nOne problem that came up was being able to determine the MIME type of the content that I’m reading from disk. This is done for you by the HTTP handlers provided in the .NET framework, but when you’re serving files through MVC Controllers, the default HTTP handlers are not used and thus you’re left to figure out the MIME types for yourself.\nSo, I began to investigate, using ILSpy, how the native/default ASP.NET HTTP handlers determine the MIME types. I came upon a class in the System.Web namespace called System.Web.MimeMapping – this class keeps a private, sealed dictionary of type MimeMappingDictionaryClassic (which extends a private abstract class called MimeMappingDictionaryBase) which holds all knows extensions and their associated MIME types… A sample of the decompiled code which populates it is below:\nprotected override void PopulateMappings()\r{\rbase.AddMapping(\u0026quot;.323\u0026quot;, \u0026quot;text/h323\u0026quot;);\rbase.AddMapping(\u0026quot;.aaf\u0026quot;, \u0026quot;application/octet-stream\u0026quot;);\rbase.AddMapping(\u0026quot;.aca\u0026quot;, \u0026quot;application/octet-stream\u0026quot;);\rbase.AddMapping(\u0026quot;.accdb\u0026quot;, \u0026quot;application/msaccess\u0026quot;);\rbase.AddMapping(\u0026quot;.accde\u0026quot;, \u0026quot;application/msaccess\u0026quot;);\rbase.AddMapping(\u0026quot;.accdt\u0026quot;, \u0026quot;application/msaccess\u0026quot;);\rbase.AddMapping(\u0026quot;.acx\u0026quot;, \u0026quot;application/internet-property-stream\u0026quot;);\rbase.AddMapping(\u0026quot;.afm\u0026quot;, \u0026quot;application/octet-stream\u0026quot;);\rbase.AddMapping(\u0026quot;.ai\u0026quot;, \u0026quot;application/postscript\u0026quot;);\rbase.AddMapping(\u0026quot;.aif\u0026quot;, \u0026quot;audio/x-aiff\u0026quot;);\rbase.AddMapping(\u0026quot;.aifc\u0026quot;, \u0026quot;audio/aiff\u0026quot;);\rbase.AddMapping(\u0026quot;.aiff\u0026quot;, \u0026quot;audio/aiff\u0026quot;);\r... // It goes on for a long time\rAnd so on… In total, there are 342 lines of known mappings!\nUltimately, my goal was to get a hold of this functionality in the easiest, most flexible way possible.\nIn .NET 4.5, MimeMapping exposes a public static method called GetMimeMapping which takes in a file name (or extension) and returns the appropriate MIME type from the aforementioned dictionary. Unfortunately my project is on .NET 4.0 and in that version of the framework this method is internal, not public (why, Microsoft, why?!) and thus was not available to me. So, I felt that I was left with 3 options:\nUpgrade to .NET 4.5 (not possible at this time due to corporate politics and so on)\nCopy and paste the entire list of mappings into a dictionary of my own and reference it (yuck!)\nREFLECTION TO THE RESCUE!\nSo, with a short bit of code, you too can steal the functionality of the GetMimeMapping method, even if it isn’t public!\nFirst, set up the reflection and cache the MethodInfo in an assembly that references the System.Web namespace. Below is a custom static class I built which wraps the reflective method:\n/// \u0026lt;summary\u0026gt;\r/// Exposes the Mime Mapping method that Microsoft hid from us.\r/// \u0026lt;/summary\u0026gt;\rpublic static class MimeMappingStealer\r{\r// The get mime mapping method info\rprivate static readonly MethodInfo _getMimeMappingMethod = null;\r/// \u0026lt;summary\u0026gt;\r/// Static constructor sets up reflection.\r/// \u0026lt;/summary\u0026gt;\rstatic MimeMappingStealer()\r{\r// Load hidden mime mapping class and method from System.Web\rvar assembly = Assembly.GetAssembly(typeof(HttpApplication));\rType mimeMappingType = assembly.GetType(\u0026quot;System.Web.MimeMapping\u0026quot;);\r_getMimeMappingMethod = mimeMappingType.GetMethod(\u0026quot;GetMimeMapping\u0026quot;, BindingFlags.Instance | BindingFlags.Static | BindingFlags.Public |\rBindingFlags.NonPublic | BindingFlags.FlattenHierarchy);\r}\r/// \u0026lt;summary\u0026gt;\r/// Exposes the hidden Mime mapping method.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;param name=\u0026quot;fileName\u0026quot;\u0026gt;The file name.\u0026lt;/param\u0026gt;\r/// \u0026lt;returns\u0026gt;The mime mapping.\u0026lt;/returns\u0026gt;\rpublic static string GetMimeMapping(string fileName)\r{\rreturn (string)_getMimeMappingMethod.Invoke(null /*static method*/, new[] { fileName });\r}\r}\rNow, a quick test via a console application to ensure that it works:\nstatic void Main(string[] args)\r{\rvar fileName1 = \u0026quot;whatever.js\u0026quot;;\rvar fileName2 = \u0026quot;somefile.css\u0026quot;;\rvar fileName3 = \u0026quot;myfile.html\u0026quot;;\rConsole.WriteLine(\u0026quot;Output for \u0026quot; + fileName1 + \u0026quot; = \u0026quot; + MimeMappingStealer.GetMimeMapping(fileName1));\rConsole.WriteLine(\u0026quot;Output for \u0026quot; + fileName2 + \u0026quot; = \u0026quot; + MimeMappingStealer.GetMimeMapping(fileName2));\rConsole.WriteLine(\u0026quot;Output for \u0026quot; + fileName3 + \u0026quot; = \u0026quot; + MimeMappingStealer.GetMimeMapping(fileName3));\rConsole.ReadKey();\r}\rAnd running the console application results in success!\nGetMimeMapping Works! ","permalink":"https://www.davidhaney.io/determine-mime-type-from-file-name/","tags":["dotnet","csharp","performance","programming","reflection"],"title":"Determine MIME Type from File Name"},{"categories":["blog"],"content":"As of today I’ve been published in an e-Book offered for free by Red Gate! It is called 50 Ways to Avoid, Find and Fix ASP.NET Performance Issues and contains many useful performance tips which have been contributed by various members of the .NET community. Many tips are ASP.NET MVC specific which is also a plus.\nMy tip is #3 and has to do with debugging Microsoft symbols.\nGet a free copy here – it has already taught me a few things I had never thought to consider!\n","permalink":"https://www.davidhaney.io/published-by-red-gate/","tags":["dotnet","performance","programming","published"],"title":"Published by Red Gate"},{"categories":["blog"],"content":"Most of us have been there: you’ve written a fantastic application that performs perfectly in your Development and/or QA environments, but in Production something goes wrong. Your application spins out of control, utilizing 100% of your CPU. Maybe it simply stops responding as if it were deadlocked. Or maybe it simply crashes randomly. What now?\nLogic tells you that you have a problem in the code somewhere that is only encountered in a Production-like environment… and if you could JUST get into the Production box, install Visual Studio (or at least the Remote Debugger), and debug the application, you’d be able to solve the problem. However, you can’t (because it’s Production!), and you can’t replicate the problem in any other environment. Maybe it’s because of stale Development or QA environment data compared to live Production data. Maybe it’s something else. You have no idea where to look to find and fix the problem in your application. For lack of eloquence: you’re screwed.\nFortunately, there are both tools designed for this very scenario and ways to “reproduce” the problem to determine the cause. I’m going to show you how to debug Production problems in applications where you cannot attach to the process for live debugging, and there are either no logs or the logs tell you nothing useful.\nLet’s create a simple application that is designed to take up 100% of our CPU:\nclass Program\r{\rstatic void Main(string[] args)\r{\r// Parallel to really max out the CPU\rParallel.For(0, 100, (i) =\u0026gt;\r{\rwhile (true)\r{\r// Loop forever and ever\r}\r});\r}\r}\rThis code basically spawns 100 concurrent threads that loop for all of eternity. The reason that we do this using the TPL / Parallel library is that a single threaded application would only max out 1/N of our CPU, where N is the number of cores in the processor. Verifying our simple application, we can see that it does its job and maxes out our CPU:\n100% CPU Used Now, imagine that this application is a lot more complex and that this simple method is just one part of the entire solution. Perhaps you’ve built a really huge website or service and this method exists in just one little part of it. Pretend also that this method is not hit in your Development or QA environments during testing, and so your application appears to operate normally to you.\nIn fact, pretend that you’ve never seen this source code at all. All that you know is that you have an application in Production that spins out of control, and you don’t know where the problem is – or even where to begin looking.\nSo, what do you do?\nThe first thing we need to do is ensure that we have the tools we’ll need to debug and fix the issue. Things you’re going to need:\nA tool that can create Mini-Dumps. I highly recommend Process Explorer which is available via TechNet. WinDBG, via Debugging Tools for Windows, an unintuitive yet key tool from Microsoft for debugging Mini-Dump files. When you install this, you’ll have to do it through the Windows Software Development Kit installer. You can unselect everything except Debugging Tools for Windows, since you’ll need nothing else for our purposes. Install WinDBG on the PC which you’ll use to analyze the Mini-Dump (typically your Development machine). Install Process Explorer on the Production box that hosts the application which is misbehaving.\nNow that you’ve got those installed, we’ll proceed and figure out the problem.\nSo our scenario is this: we have an application running in Production which is spinning out of control, we have the source code on a different PC, and we can’t attach a debugger to the Production environment. We can’t reproduce this behaviour in Development or QA environments at all. So, time to get down to the details of the problem.\nStep 1 is easy. You need to take a Mini-Dump of the misbehaving application on the Production machine. There’s a bit of a catch 22 here in that the rogue application is using 100% of your CPU, so this Mini-Dump could take forever. To solve that problem on a multi-core machine, simply use Task Manager to set the affinity of the misbehaving application to 1 or 2 cores to lower the total CPU used by the application, thus freeing up CPU for our Mini-Dump:\nSet the Affinity to 1 or 2 CPUs Unselect “All Processors” and pick 1 or 2:\nPick 1 or 2 CPUs There, now it’s a lot easier to do things on the affected PC since it isn’t spending 100% of its CPU spinning on your application:\nProcessor Now Freed Up Note that the affinity setting does not persist, meaning the next time you launch the application, it will go back to using all CPUs per usual.\nSo, now we’re on to Step 2. Take a Mini-Dump (haha). To do this, launch Process Explorer, find your rogue application, right click, Create Dump –\u0026gt; Minidump:\nCreate a Minidump Now save the .dmp file somewhere that your Development PC can get to it in order to debug the issue.\nStep 3. Great, so now we have our .dmp Mini-Dump file… so let’s get cracking on debugging it. Get the .dmp file to your Development PC and fire up WinDBG. NOTE: run the 64 bit version for 64 bit applications and the 32 bit version for 32 bit applications. If you experience weird behaviour, try switching the WinDBG version that you launch. You’ll be greeted with a pretty bland grey window. Go to File –\u0026gt; Open Crash Dump (or CTRL + D for the shortcut fans out there). Select your .dmp file and you’ll end up with a screen similar to this:\nMinidump File Loaded So now the “hard” part begins. We need to manually diagnose the issue. The first step in WinDBG is usually to load up the CLR runtime so that we can examine our stack. To do this, run either:\n.loadby SOS clr // for .NET 4+\rOR\n.loadby SOS mscorwks // for .NET 3.5 or lower\rMy app is .NET 4, so I ran .loadby sos clr to load the CLR. This is what it should look like if it succeeds:\nLoad the CLR Next we need to load the symbols for the application that we’re debugging. To do this, run the following commands:\n.symfix\r.sympath+ **\u0026lt;absolute path to your application's compiled code and symbols\u0026gt;**\rHere’s what I did for my Symbol Path for reference:\nRun .symfix and .sympath Next, run .symopt+0x40 to enable symbols to be used which are not perfectly matched. This setting is extremely useful in cases where the code is compiled locally and doesn’t perfectly match the production code that the Mini-Dump was taken from. If this setting is omitted, you will have a very bad time determining the issue, so make sure that you run it. This is what it looks like:\nRun symopt\u0026#43;0x40 Now that you’ve specified the path at which to look for symbols for your application, run the reload command to reload your symbols:\n.reload /v /f\rYou might get some warnings and errors here but that’s usually fine as long as they don’t relate to your application’s DLLs. Verify that it found your application’s symbols by reading the output of the .reload command:\nEnsure your symbols loaded Alright, almost there! Now we have our symbols and CLR loaded, so let’s find out what our application is spending all of its time doing. Run the command:\n!runaway\rAnd you’ll find out which threads have been using the most CPU time. Mine looks like this:\nMany Threads are Long Running So here we can see that we have 6 threads which are taking up more time than all of the others… We need to print their stack traces and see just what is going on. Execute this (super intuitive) command to see the stack trace of the first thread:\n~~[9:18a8]e!clrstack\rSuper intuitive right? Basically it says “gimme the CLR stack trace for the thread whose ID is in the square brackets”. Here’s what mine spits out:\nAHA! The culprit! And now we know the culprit! So, we proceed to our source code and sure enough, line 16 is our problem:\nThe Problem Line And there you have it. How to debug production issues from your development environment.\nNote that you can also determine what caused a crash using WinDBG… Have your system administrator enable Mini-Dumps for crashes, and then perform the same commands as we did in this post EXCEPT that at the part where we ran !runaway, instead run !analyze -v and !clrstack – these will get you on your way!\n","permalink":"https://www.davidhaney.io/but-it-didnt-happen-in-dev-or-qa/","tags":["dotnet","debugging","performance","profiling","programming","windbg"],"title":"But it Didn't Happen in DEV or QA!"},{"categories":["blog"],"content":"So, this post is about our beloved IDE instead of actual code.\nI recently upgraded my home PC from Visual Studio 2010 and 11 Beta to Visual Studio 2012. The very first thing I noticed was that after about 10 minutes of programming my Intellisense quit working and never came back. I thought to myself “what the hell Visual Studio? 2010 didn’t have these problems?!” and then, after a swig of beer, proceeded to exercise my Google-Fu to solve this issue.\nStrangely, I did not find the “correct” aka permanent solution to this problem. So, after a ton of screwing around and guess-and-check, here’s how I solved it. Note that for this fix to work, you have to abandon Visual Studio 2010 or 11 PERMANENTLY. You don’t have to uninstall either of them, but if you open a solution in them it seems you’ll re-break Visual Studio 2012. Stupid, I know. Maybe a hotfix will come out eventually to fix this issue. Anyway…\nIf your Intellisense is anything but intelligent, and has stopped working, do the following:\nOpen the start menu and type “%AppData%” and press enter to get to your Application Data Folder.\nEither you were automatically placed in the “Roaming” folder or you weren’t. If you weren’t, go to the “Roaming” folder.\nOpen the “Microsoft” folder.\nOpen the “VisualStudio” folder.\nHere you’ll see a folder titled “11.0” (the VS 2012 folder) and probably also “10.0” (the VS 2010 folder).\nDELETE (or rename) the “10.0” folder. Note that you can now kiss your Visual Studio 2010 settings and preferences goodbye (your projects will be safe and sound).\nDELETE (or rename) all other folders that are not the “11.0” folder, assuming you used to have Visual Studio 2008 or whatever.\nNow restart Visual Studio 2012 and you should be good to go!\n","permalink":"https://www.davidhaney.io/visual-studio-2012-intellisense-not-working-solved/","tags":["intellisense","visual studio"],"title":"Visual Studio 2012 Intellisense Not Working - SOLVED"},{"categories":["blog"],"content":"A friend of mine commented on my last post asking about how much faster the static string.Equals method is than the instance string.Equals method. To satiate both of our curiosities, I have created this benchmarking application:\nstatic void Main(string[] args)\r{\rvar stopwatch = new Stopwatch();\rstring a = \u0026quot;hello\u0026quot;;\rstring b = \u0026quot;hi\u0026quot;;\rstopwatch.Start();\rfor (int i = 0; i \u0026lt; 10000000; i++)\r{\ra.Equals(b);\r}\rstopwatch.Stop();\rConsole.WriteLine(\u0026quot;Instance string.Equals over 10,000,000 iterations: \u0026quot; + stopwatch.ElapsedMilliseconds + \u0026quot; ms\u0026quot;);\rstopwatch.Reset();\rstopwatch.Start();\rfor (int i = 0; i \u0026lt; 10000000; i++)\r{\rstring.Equals(a, b);\r}\rstopwatch.Stop();\rConsole.WriteLine(\u0026quot;Static string.Equals over 10,000,000 iterations: \u0026quot;\r+ stopwatch.ElapsedMilliseconds + \u0026quot; ms\u0026quot;);\rConsole.ReadKey();\r}\rThe results of 5 runs, where “I” is the instance method and “S” is the static method, and the times are in milliseconds:\nInstance Static 113ms 100ms 144ms 96ms 126ms 89ms 126ms 94ms 128ms 97ms And there you have it. Static string.Equals is reliably slightly faster… But unless you’re doing millions of comparisons, it probably doesn’t really matter much. It does, however, prevent the NullReferenceException mentioned in the last post when the string instance is null.\n","permalink":"https://www.davidhaney.io/static-vs-instance-string-equals-benchmark/","tags":["dotnet","analysis","csharp","performance","programming"],"title":"Static vs Instance string.Equals Benchmark"},{"categories":["blog"],"content":"As you may or may not know, static methods are usually faster than instance methods. This alone should be a good enough reason to use the static string.Equals method in .NET, but if that doesn’t do it for you, allow me to present a simple example.\nstring a = \u0026quot;hello\u0026quot;;\rstring b = \u0026quot;hi\u0026quot;;\rbool result = a.Equals(b);\rWhat is the expected result of these lines? A boolean value of false, of course. And it’d be true if the strings were identical. It’s also false if b is null. But what if a is null?\nstring a = null;\rstring b = \u0026quot;hi\u0026quot;;\rbool result = a.Equals(b);\rThe above code throws a NullReferenceException, since we are attempting to use the instance method of string.Equals and our instance is null. Effectively we’re calling null.Equals which of course is a NullReferenceException.\nThe static method is best to use in situations where either string (or both) can be null. Re-writing our code as:\nstring a = null;\rstring b = \u0026quot;hi\u0026quot;;\rbool result = string.Equals(a, b);\rAllows the code to run identically in function to the original code but without ever throwing a NullReferenceException for any given string input.\n","permalink":"https://www.davidhaney.io/static-vs-reference-string-equals/","tags":["dotnet","analysis","csharp","performance","programming"],"title":"Static vs Instance string.Equals"},{"categories":["blog"],"content":"Well, it’s been nearly 2 months since my last post… I’m learning that if you want a blog to be successful, you have to carve time out of your busy life and make it happen. So, with renewed focus, I re-enter the fray.\nThe Joel Test is a curious and honest thing. It has been around since the year 2000 and was invented by a guy named Joel Spolsky, as the name might imply. In short, it’s a very brief questionnaire that evaluates the quality of your software development team, and implicitly their happiness as well.\nThere are 12 questions. A score of 12 is perfect, 11 is good, 10 or lower and your team has serious problems. The questions are simple and in the format of yes/no. The Joel Test is as follows:\nDo you use source control?\nCan you make a build in one step?\nDo you make daily builds?\nDo you have a bug database?\nDo you fix bugs before writing new code?\nDo you have an up-to-date schedule?\nDo you have a spec?\nDo programmers have quiet working conditions?\nDo you use the best tools money can buy?\nDo you have testers?\nDo new candidates write code during their interview?\nDo you do hallway usability testing?\nFor those who have never heard of hallway usability testing, I offer Joel’s definition: “A hallway usability test is where you grab the next person that passes by in the hallway and force them to try to use the code you just wrote. If you do this to five people, you will learn 95% of what there is to learn about usability problems in your code.”\nThe Joel Test has been in and out of my life for quite some time. Every now and then I remember that it exists, and then I evaluate my current employer on it. My current company is suffering in the programming department; we are losing people like crazy and are having a hard time hiring good folks, despite offering reasonably competitive salaries and benefits… So, realizing that we have some issues, I conducted The Joel Test on a whim. Here’s how we did:\nDo you use source control? Yes Can you make a build in one step? Yes Do you make daily builds? No Do you have a bug database? Yes Do you fix bugs before writing new code? No Do you have an up-to-date schedule? No Do you have a spec? No Do programmers have quiet working conditions? No Do you use the best tools money can buy? No Do you have testers? Yes Do new candidates write code during their interview? No Do you do hallway usability testing? No We scored 4. 4 out of 12. Recall that 10 or worse means that you have a serious problem. Yeah.\nSo, what did I learn? I learned that part of the reason that we are losing people is that our programming practices and team environment lack something to be desired, and thus people are not happy. This isn’t really news to me, however, since the people that have quit recently have all cited these things as some of the main reasons.\nWhat’s the next play? We need to improve the aspects of our environment and team that failed the test. The question then becomes: will management improve these things, or will they choose to ignore our suggestions because most of them (using the best tools that money can buy, quiet working conditions) cost money, and the rest of them are process changes? In fact, I’d go so far as to say that for my current company, they’d be significant process changes… And if there’s one thing that I’ve learned in my career as a programmer so far, it’s that businesses love to stay in familiar territory, clinging to what they already know and feel experienced in, and are generally afraid of significant change.\nAs a result of the change not being delivered, the motivated individual realizes that change must then come from within… People quit to change the negative environment for themselves (by joining a more positive one).\nI encourage you all to conduct The Joel Test in your environments, and to try and improve your team with the results. What have you got to lose? Oh yeah, more developers.\n","permalink":"https://www.davidhaney.io/the-joel-test-really-is-meaningful/","tags":["analysis","career","culture","interviews","mistakes","workplace"],"title":"The Joel Test Really is Meaningful"},{"categories":["blog"],"content":"This isn’t a very technical post, but I was published by Microsoft recently via their MSDN UK Blog! The article is on creating and maintaining a successful User Group.\nClick here to read the article!\n","permalink":"https://www.davidhaney.io/published-by-mdsn-uk-blog/","tags":["published"],"title":"Published by MDSN UK Blog"},{"categories":["blog"],"content":"Two of my colleagues (one from work and one from a user group) kindly pointed out to me that in my last post I omitted Continuation Tasks as a means of Error Handling for the TPL. As such, I will expand upon my last post with an example of handling errors via a Continuation Task.\nContinuing where we left off last, the following code will utilize a Task Continuation to handle errors within Tasks.\nstatic void Main(string[] args)\r{\rfor (int i = 0; i \u0026lt; 5; i++)\r{\r// Initialize a Task which throws exception\rvar task = new Task(() =\u0026gt;\r{\rthrow new Exception(\u0026quot;It broke!\u0026quot;);\r});\r// Configure the Continuation to only fire on error\rtask.ContinueWith(HandleError, TaskContinuationOptions.OnlyOnFaulted);\r// Start the Task\rtask.Start();\r}\rConsole.WriteLine(\u0026quot;End of method reached\u0026quot;);\rConsole.ReadKey();\r}\rprivate static void HandleError(Task task)\r{\r// The Task has an AggregateException\rvar agex = task.Exception;\rif (agex != null)\r{\r// Output all actual Exceptions\rforeach (var ex in agex.InnerExceptions)\r{\rConsole.WriteLine(ex.Message);\r}\r}\r}\rAnd the result:\nThe Output By using a Continuation Task which wraps our private method, HandleError (a method whose signature is effectively Action), we can effectively handle errors in a more elegant, less inline way. This allows centralizing Task handling logic in such cases as where you’d always want to log to a file or database, for example. Note that there is overhead in the complexity of this architecture – since we receive an AggregateException, we must loop through it to analyze our individual errors.\nSorry that I missed this in my first post!\n","permalink":"https://www.davidhaney.io/tpl-and-error-handling-continuations/","tags":["dotnet","async","csharp","performance","programming","tasks","tpl"],"title":"TPL and Error Handling \u0026 Continuation Tasks"},{"categories":["blog"],"content":"As of .NET 4.0, the TPL or Task Parallel Library is king when it comes to parallelization. It allows for smooth, easy multi-threading for any application. There is a slight learning curve, however, and a major part of this is understanding how Exceptions bubble-up while using the TPL.\nLet’s partake in a simple example. This code will create and run a task that throws an Exception, and then attempt to catch it:\nstatic void Main(string[] args)\r{\rtry\r{\r// Initialize a Task which throws exception\rvar task = new Task(() =\u0026gt;\r{\rthrow new Exception(\u0026quot;Task broke!\u0026quot;);\r});\r// Start the Task\rtask.Start();\r}\r// Attempt to catch the Task Exception\rcatch (Exception ex)\r{\rConsole.WriteLine(\u0026quot;Caught the TPL exception\u0026quot;);\r}\rConsole.WriteLine(\u0026quot;End of method reached\u0026quot;);\rConsole.ReadKey();\r}\rWhen we run it, we get this result:\nThe Output The Exception was not caught. This is because the Task is run on a different thread, which has its own Stack memory and execution path, which consists of the code inside of the Task and not much else. As a result, when the Task is run, the flow of control returns to the next line in our application, unaware of the Task or its outcome. Once the Exception in the Task is thrown, it is thrown in a different scope, effectively outside of our Main method. It’s almost as if we ran an entirely different application, but attempted to catch its Exception in this application. This just won’t work.\nThere is a way to work around this however. One interesting thing about the TPL is the Task.WaitAll method. It allows the control of flow of your executing Tasks/threads to return to your calling method. By adding this method to our code, we can make our main thread stall until the Task completes, which also enables it to catch the Task’s exception:\nstatic void Main(string[] args)\r{\rtry\r{\r// Initialize a Task which throws exception\rvar task = new Task(() =\u0026gt;\r{\rthrow new Exception(\u0026quot;Task broke!\u0026quot;);\r});\r// Start the Task\rtask.Start();\r// Wait for the Task to complete, thus keeping control of flow\rTask.WaitAll(task);\r}\r// Attempt to catch the Task Exception\rcatch (Exception ex)\r{\rConsole.WriteLine(\u0026quot;Caught the TPL exception\u0026quot;);\r}\rConsole.WriteLine(\u0026quot;End of method reached\u0026quot;);\rConsole.ReadKey();\r}\rThe output is as follows:\nSecond Output This time we were able to catch the Exception. Of note, the Exception which is thrown by Tasks is typically the AggregateException which allows you to catch multiple, often asynchronous Exceptions as one Exception (which is easier for a single thread to handle). A quick demo of this functionality (let’s have 5 threads each throw an Exception):\nstatic void Main(string[] args)\r{\rtry\r{\r// A List of Tasks\rvar taskList = new List\u0026lt;Task\u0026gt;(5);\rfor (int i = 0; i \u0026lt; 5; i++)\r{\r// Initialize a Task which throws exception\rvar task = new Task(() =\u0026gt;\r{\rthrow new Exception(\u0026quot;It broke!\u0026quot;);\r});\r// Start the Task\rtask.Start();\r// Add to List\rtaskList.Add(task);\r}\r// Wait for all Tasks to complete, thus keeping control of flow\rTask.WaitAll(taskList.ToArray());\r}\r// Attempt to catch the Task Exception\rcatch (AggregateException agex)\r{\rConsole.WriteLine(\u0026quot;Caught the TPL exception(s)\u0026quot;);\r// Output all actual Exceptions\rforeach (var ex in agex.InnerExceptions)\r{\rConsole.WriteLine(ex.Message);\r}\r}\rConsole.WriteLine(\u0026quot;End of method reached\u0026quot;);\rConsole.ReadKey();\r}\rAnd the output:\nThird Output So as you can see, there are ways to handle Tasks throwing Exceptions, if you have your calling thread pause and wait for the Tasks to complete. This, of course, is not always practical so your other option is to handle the exception within the Task’s scope – just like you would with regular, single-threaded code. In effect you treat the code within the Task as isolated, single-threaded code, and catch and handle Exceptions accordingly. A slight mod to our application will show you this:\nstatic void Main(string[] args)\r{\rfor (int i = 0; i \u0026lt; 5; i++)\r{\r// Initialize a Task which throws exception\rvar task = new Task(() =\u0026gt;\r{\rtry\r{\rthrow new Exception(\u0026quot;It broke!\u0026quot;);\r}\rcatch (Exception ex)\r{\r// Output the Exception\rConsole.WriteLine(ex.Message);\r}\r});\r// Start the Task\rtask.Start();\r// Note: no longer waiting for Task to finish\r}\rConsole.WriteLine(\u0026quot;End of method reached\u0026quot;);\rConsole.ReadKey();\r}\rAnd the result of this change:\nFourth Output Note the interesting results of this. Our main thread did not wait for the Tasks to complete, and so it the Task results were never returned back to the calling thread’s flow of control. As a result, the “End of method reached” text actually came before the threads crashed, because it happened sooner.\nNote also that only 4 of the 5 Exceptions were written to the Console. This is due to a race condition in your parallelized application: The Console is a single/static reference and so our 5 spawned threads plus 1 main thread all race to output to it. In this particular instance of the application being run, the Console.ReadKey method was executed after the first 4 Tasks had written their output, but before the 5th Task wrote its output. This does not mean that it was not handled; it simply means that, in a way that is classic to multi-threaded applications, we encountered a race condition. I ran this application another 10 or 20 times and saw many variations: sometimes 3 Exceptions were output, sometimes 4, and rarely all 5. This is a great example of a race condition within an application, and one which you could handle using the above strategy of Task.WaitAll prior to outputting your final Console.ReadKey statement and terminating your application.\nIt’s up to you individually to decide which style of TPL error handling makes the most sense in each application. It depends on the purpose of each thread and the implications of having your application wait for threads to complete, considered on a per-application basis.\nOne final note is that other parallel operations, such as Parallel.ForEach and Parallel LINQ (PLINQ) use AggregateException as well to catch their thrown Exceptions, and that the AggregateException offers a Flatten method for re-throwing nested AggregateExceptions as a one-level deep single AggregateException to simplify handling of them.\n","permalink":"https://www.davidhaney.io/tpl-and-error-handling/","tags":["dotnet","async","csharp","performance","programming","tasks","tpl"],"title":"TPL and Error Handling"},{"categories":["blog"],"content":"The .NET compiler is a terrific thing… After all, it turns your C# into an executable program!\nOne nice feature of the .NET compiler, which is becoming better each release, is inferred typing. I’d like to lay out a few short examples that might help you develop your programming standards and practices.\nInferring a type when creating an array.\n// Create and initialize an array\rvar myArray = new int[] { 1, 2, 3 };\rBecomes:\n// Create and initialize an array\rvar myArray = new [] { 1, 2, 3 };\rThe compiler knows that your array members are integers, and thus infers the array type as int[].\nInferred types and matching Generic method parameters. Given a Generic method:\n/// \u0026lt;summary\u0026gt;\r/// A generic method that takes a thing of Type T as input.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;typeparam name=\u0026quot;T\u0026quot;\u0026gt;The Type.\u0026lt;/typeparam\u0026gt;\r/// \u0026lt;param name=\u0026quot;thing\u0026quot;\u0026gt;The thing of Type T.\u0026lt;/param\u0026gt;\rprivate void MyGenericMethod\u0026lt;T\u0026gt;(T thing)\r{\r// Do some stuff\r}\rThis:\n// Generic Type Parameters\rMyGenericMethod\u0026lt;int\u0026gt;(1);\rBecomes:\n// Generic Type Parameters\rMyGenericMethod(1);\rNo need to include the Type parameter in the method call because the Generic Type is the same Type as the parameter to the method, so the compiler figures it out.\nInferred types and the var keyword. A pretty classic example.\nint myInteger = 3;\rBecomes:\nvar myInteger = 3;\rThe compiler knows to assign the type of int to myInteger based on the evaluation of the assignment expression. Note that this can be annoying when trying to use interfaces:\nvar myList = new List\u0026lt;int\u0026gt;();\rAssigns var to List\u0026lt;int\u0026gt;, so if you want IList\u0026lt;int\u0026gt; you need to do it the usual way:\nIList\u0026lt;int\u0026gt; myList = new List\u0026lt;int\u0026gt;();\rA short but sweet post.\n","permalink":"https://www.davidhaney.io/compiler-tricks-inferred-types/","tags":["dotnet","csharp","compiler","programming"],"title":"Compiler Tricks - Inferred Types"},{"categories":["blog"],"content":"I came across a need at work today to re-implement some of the Output Caching for our MVC3 application which runs under .NET 4.0. I wanted to use standard Output Caching (via the OutputCacheAttribute class, why re-invent the well-working wheel?) but due to some of our requirements I needed more control over how my objects were cached. More specifically, I needed to cache them with a custom Cache Dependency. With a little bit of Google-Fu, I was delighted to learn of the Output Cache Provider functionality introduced in ASP.NET 4. I implemented a custom OutputCacheProvider, registered it in my Web.config file as the Default Cache Provider, and I was well on my way.\n…Or so I thought. You see, in our application we are caching both regular (Parent) and partial (Child) Controller Action Method calls. That is to say, we’re caching regular calls to Controller Action Methods, as well as the outputs of Child Action Method calls which are invoked from uncached Parent Action Method calls. While testing, some strange behaviour showed me that my Child Action Method calls were not being cached by my shiny new custom Output Cache Provider. They were instead being cached by the Default Output Cache Provider, which I have no control over. I confirmed this by debugging and seeing that my Child Action Method calls were not hitting my Custom Output Cache Provider methods… What gives?\nI did some more Googling and learned very little, but happened to come across this little tidbit of somewhat vague information. I also came across a few .NET bloggers that had solved this problem… how shall I say… VERY poorly. So, I’d like to tell you how to do it correctly.\nIn the .NET 4.0 edition of the MVC3 assemblies, the OutputCacheAttribute contains a static property called ChildActionCache which is of type ObjectCache. As you can see from the MSDN page (at least at the time of writing this), they aren’t exactly detailing what it is for or how it really works – or why you can’t just use the bloody OutputCacheAttribute for Child Action Method calls. So what is going on?\nWell, after a little investigation, I discovered the reasoning behind the madness. Basically, from a high level view, the OutputCacheAttribute works in conjunction with a Caching Http Module (the OutputCacheModule class). Each HTTP Request is passed to the OutputCacheModule LONG before it reaches your MVC application (FYI, this is called kernel-level IIS caching), and if the Http Module can pull a cached Response for that particular Request out of the Cache, it will short circuit your application and simply render the response to the user, stopping further execution. When this happens, your application never even sees the request. Neat, huh? If it can’t find the request, it exits and lets your application do its thing… And whenever you’ve placed OutputCache on your Action Method, it will cache the response in the same format that the Http Module looks for. This allows for MUCH less work to be done by your application in caching things. Cool, right?\nYou may now see why you cannot cache Child Action Method calls using the regular old OutputCacheAttribute… Your MVC application needs to execute a Parent Action Method from which the Child Action Methods are executed. If your Child Action Method was cached in the same way as a Parent Action Method, the HttpModule would always perform a Cache-miss since the Child Request originates from the Parent and has a completely different method signature, parameters, etc. upon which the Cache Key is derived. How can you cache your Child Action Methods ahead of your MVC application when your MVC application needs to execute in order to generate the Child Action Method Requests? And so, OutputCacheAttribute only works in the traditional manner for Parent Action Method calls.\nSo, how do you “fix” this and handle the Caching of Custom Child Action Methods in the same way as Parent Action Methods? First, create a custom class that inherits from the MemoryCache object. In this class you’re going to override 2 methods:\n/// \u0026lt;summary\u0026gt;\r/// A Custom MemoryCache Class.\r/// \u0026lt;/summary\u0026gt;\rpublic class CustomMemoryCache : MemoryCache\r{\rpublic CustomMemoryCache(string name)\r: base(name)\r{\r}\rpublic override bool Add(string key, object value, DateTimeOffset absoluteExpiration, string regionName = null)\r{\r// Do your custom caching here, in my example I'll use standard Http Caching\rHttpContext.Current.Cache.Add(key, value, null, absoluteExpiration.DateTime, System.Web.Caching.Cache.NoSlidingExpiration, System.Web.Caching.CacheItemPriority.Normal, null);\rreturn true;\r}\rpublic override object Get(string key, string regionName = null)\r{\r// Do your custom caching here, in my example I'll use standard Http Caching\rreturn HttpContext.Current.Cache.Get(key);\r}\r}\rYou’re going to build your custom Output Cache Provider (mentioned at the beginning of this post) similarly, inheriting from the abstract class OutputCacheProvider. Here’s an example one:\n/// \u0026lt;summary\u0026gt;\r/// A Custom OutputCacheProvider Class.\r/// \u0026lt;/summary\u0026gt;\rpublic class CustomOutputCacheProvider : OutputCacheProvider\r{\rpublic override object Add(string key, object entry, DateTime utcExpiry)\r{\r// Do the same custom caching as you did in your // CustomMemoryCache object\rvar result = HttpContext.Current.Cache.Get(key);\rif (result != null)\r{\rreturn result;\r}\rHttpContext.Current.Cache.Add(key, entry, null, utcExpiry,\rSystem.Web.Caching.Cache.NoSlidingExpiration, System.Web.Caching.CacheItemPriority.Normal, null);\rreturn entry;\r}\rpublic override object Get(string key)\r{\rreturn HttpContext.Current.Cache.Get(key);\r}\rpublic override void Remove(string key)\r{\rHttpContext.Current.Cache.Remove(key);\r}\rpublic override void Set(string key, object entry, DateTime utcExpiry)\r{\rHttpContext.Current.Cache.Add(key, entry, null, utcExpiry,\rSystem.Web.Caching.Cache.NoSlidingExpiration, System.Web.Caching.CacheItemPriority.Normal, null);\r}\r}\rYou’ll notice that Add and Set are similar, but that Add checks for and returns the object from Cache if it exists, before attempting any Caching. This is the expected behaviour of the Add method according to MSDN and thus you should code it as above.\nNow your Web.config needs a few simple lines added in order to be configured to use your CustomOutputCacheProvider:\n\u0026lt;system.web\u0026gt;\r\u0026lt;caching\u0026gt;\r\u0026lt;outputCache defaultProvider=\u0026quot;CustomProvider\u0026quot;\u0026gt;\r\u0026lt;providers\u0026gt;\r\u0026lt;clear/\u0026gt;\r\u0026lt;add name=\u0026quot;CustomProvider\u0026quot; type=\u0026quot;MyMvcApp.Caching.CustomOutputCacheProvider, MyMvcApp\u0026quot;/\u0026gt;\r\u0026lt;/providers\u0026gt;\r\u0026lt;/outputCache\u0026gt;\r\u0026lt;/caching\u0026gt;\r\u0026lt;/system.web\u0026gt;\rThe defaultProvider segment above allows you to set the Named Output Cache Provider that should be used by default for all Output Caching.\nWith the classes and configuration in place, you’ve now configured all Parent Action Methods which are decorated with [OutputCache] to use your new Custom Output Cache Provider! But we still need to configure Child Action Methods to do the same Caching as Parent Action Methods. This is where your custom MemoryCache object comes into play. Modify your Global.asax to wire your CustomMemoryCache into the OutputCacheAttribute:\nprotected void Application_Start()\r{\r// Register Custom Memory Cache for Child Action Method Caching\rOutputCacheAttribute.ChildActionCache = new CustomMemoryCache(\u0026quot;My Cache\u0026quot;);\r}\rAs an FYI, for your CustomMemoryCache object, and MemoryCache objects in general, here’s some information how to configure them by Name using your Web.config or App.config – very useful. You’ll note that I named my MemoryCache “My Cache” above – the name isn’t optional, but it has no effect unless it matches a Named Cache entry in your Web.config or App.config file; if it does match, it will adhere to the rules of your Named Cache. If, on the other hand, your MemoryCache object doesn’t use Runtime Caching and instead writes to a database or other external source such as AppFabric, the Named Cache will have no effect since it applies only to in-process Runtime Caching.\nAnd that’s it! You’ve got a fully custom Output Caching solution in .NET 4.0 for your MVC3 application that correctly leverages standard Microsoft hooks and components! Thanks for reading this long post – comments and criticisms welcomed as always.\n","permalink":"https://www.davidhaney.io/custom-output-caching-with-mvc3-and-net-4-0-done-right/","tags":["dotnet","csharp","caching","mvc","programming"],"title":"Custom Output Caching with MVC3 and .NET 4.0 - Done Properly!"},{"categories":["blog"],"content":"As of .NET 3.0, LINQ (and the often related Lambda Expressions) have been available for our use and abuse. LINQ stands for Language INtegrated Query, and is a method of modelling OO data in a more or less relational sense that is not unlike databases. And just like databases, it comes with a cost.\nTo offset this cost, LINQ uses Deferred Execution. Deferred Execution means that the code is not executed until it is needed. This means that the LINQ code that you write is not actually executed until you NEED to execute it – typically during an enumeration of the results.\nAn example. Let’s create an array of 1,000,000 integers, all of random values between 1 and 10000, and sort them in an ascending fashion using LINQ:\nstatic void Main(string[] args)\r{\r// Anytime that you know the size of your list, specify // it in the constructor. This enables much more efficient // processor and memory usage\rvar myIntegers = new List\u0026lt;int\u0026gt;(1000000);\r// Initialize RNG\rvar random = new Random();\r// Populate the list with random numbers\rfor (int i = 0; i \u0026lt; 1000000; i++)\r{\rmyIntegers.Add(random.Next(1, 10000));\r}\rvar stopwatch = new Stopwatch();\rstopwatch.Start();\r// LINQ time, let's sort them\rvar result = myIntegers.OrderBy(i =\u0026gt; i);\rstopwatch.Stop();\rConsole.WriteLine(\u0026quot;LINQ OrderBy Time: \u0026quot; + stopwatch.ElapsedMilliseconds + \u0026quot; ms\u0026quot;);\rConsole.ReadKey();\r}\rAnd the output:\nLINQ OrderBy Result Note how little time it took to Order the results – only 0 ms! Seems a bit fishy that we sorted 1,000,000 integers in less than 1 millisecond, doesn’t it? This is because our application didn’t actually sort them at all. Via the power of Deferred Execution, your .NET application is intelligent enough to know not to actually execute any LINQ queries until you NEED to. Note that since we never did anything with the result, the sort never actually happened.\nWhat did happen, however, was the creation of a .NET object called an Expression Tree. An Expression Tree is used for “meta programming” – basically writing code that writes code. LINQ automatically generates an Expression Tree for your query – which it can build upon as you tag on additional queries – that isn’t executed until it needs to be executed. This allows you to do all of the neat things that LINQ supports like joins, selecting based on a particular class property or value, and so on. The generation of this Expression Tree is actually all that has happened so far in our application, and it took approximately 0 ms – cheap!\nRecall now that LINQ can be performed on 2 types of objects: IEnumerable\u0026lt;T\u0026gt; and IQueryable\u0026lt;T\u0026gt;. Since IEnumerable\u0026lt;T\u0026gt; exposes only one method, GetEnumerator, it is safe to say that LINQ queries against IEnumerable\u0026lt;T\u0026gt; objects are only actually executed whenever enumeration actually occurs. To prove this, let’s alter the code slightly by adding a ToList() call to our OrderBy() call – ToList() forces an enumeration of the collection to gather results, so our OrderBy will actually do the work of ordering things:\nstatic void Main(string[] args)\r{\r// Anytime that you know the size of your list, specify // it in the constructor. This enables much more efficient // processor and memory usage\rvar myIntegers = new List\u0026lt;int\u0026gt;(1000000);\r// Initialize RNG\rvar random = new Random();\r// Populate the list with random numbers\rfor (int i = 0; i \u0026lt; 1000000; i++)\r{\rmyIntegers.Add(random.Next(1, 10000));\r}\rvar stopwatch = new Stopwatch();\rstopwatch.Start();\r// LINQ time, let's sort them\r// This time we force the enumeration with ToList()\rvar result = myIntegers.OrderBy(i =\u0026gt; i).ToList();\rstopwatch.Stop();\rConsole.WriteLine(\u0026quot;LINQ OrderBy Time: \u0026quot; + stopwatch.ElapsedMilliseconds + \u0026quot; ms\u0026quot;);\rConsole.ReadKey();\r}\rAnd the result:\nLINQ OrderBy Then Any Result A whopping 356 ms – seems a little more realistic for performing a sort than 0 ms!\nWhy is LINQ done this way? Why not execute each query immediately as the LINQ method is called? The answer is efficiency. I’d love to get further into the nitty gritty details of why, but Charlie Calvert explains it better than I probably would. I will, however, cover Expression Trees in a future post – they’re a ton of fun.\n","permalink":"https://www.davidhaney.io/linq-and-deferred-execution/","tags":["dotnet","csharp","linq","performance","programming"],"title":"LINQ and Deferred Execution"},{"categories":["blog"],"content":"Sorry for the delay in posts, May has been a very busy month.\nIn order to accurately debug or profile an external assembly or library (AKA one you’re not directly compiling), you need the associated PDB files to accompany each of the DLLs. These files give the debugger some information about the compiled assembly so that your debugger or profiler can become aware of function names, line numbers, and other related meta data.\nOne thing that sucks is debugging and profiling native Microsoft .NET assemblies. When debugging an exception where you have no line number, or performance profiling an application and not knowing what method is being referred to, it’s easy to become very frustrated, very quickly. The latter scenario happened to me just this week. I was performance profiling my application at work and found that a large portion of the application’s time (~43%) was spent in a method called “clr.dll” (displayed here as memory addresses):\nPerformance Profiling - No Symbols This was not exactly a useful indication of what REALLY happened. What am I supposed to do with the knowledge that over 40% of my application’s time is spent in a Microsoft assembly called “clr.dll”? Not much, which is a little concerning. I needed to know what was really happening!\nFortunately, there’s a solution for this very issue. A little known feature implemented in Visual Studio 2010 is the ability to connect to Microsoft’s Symbol Servers and obtain most of the debugging symbols for their assemblies and libraries!\nJust go to: Tools –\u0026gt; Options –\u0026gt; (expand) Debugging –\u0026gt; Symbols\nHere, select/check the “Microsoft Symbol Servers” as a source for Symbols. Now, getting the symbols from Microsoft every time you debug or profile is going to be slow and painful (and it’ll even give you a pop-up saying as much once you check the Microsoft Symbol Servers), so be sure that you specify a directory for the “Cache symbols in this directory” input – it will keep a local copy of the PDBs and simply check for updates every so often. As a result, you get your regular debugging/profiling AND you can see the function names of the Microsoft assemblies!\nUsing this feature, I was able to re-evaluate my latest performance tests and see that the supposed “clr.dll” method was actually “TransparentProxyStub_CrossContext” – a method buried deep within the WCF framework:\nPerformance Profiling - Symbols A little Google-Fu and a discussion with a co-worker who is well-versed in WCF told me that my application was actually spending its time waiting for a reply from a WCF request. Since this was expected behaviour (the application calls out to a service for every request), it put my performance profiling mind at ease.\nTake advantage of Microsoft’s PDBs, especially when the price is right – free. You’d be amazed how useful they are in your day-to-day debugging and profiling.\n","permalink":"https://www.davidhaney.io/make-your-debugging-life-easier/","tags":["dotnet","csharp","debugging","performance","profiling","visual studio"],"title":"Make Your Debugging Life Easier"},{"categories":["blog"],"content":"The topic at hand is interning. More specifically, string interning.\n“What is string interning?” you ask? Good question. As you may or may not know, strings are immutable reference types. This means that they are read-only and a pointer will refer to the string’s location on the heap. Typically, a new string is created and stored within your application’s memory each time that you assign a string – even if the same string is defined repeatedly. What this means is that you can define the same string N times and have it take up the string’s memory N times. This sucks when dealing with repeating string data.\nEnter string interning. String interning is the process by which you defer your string reference to the .NET runtime’s string intern pool, thereby conserving memory as N identical strings point to the same single reference. From MSDN:\n“The common language runtime conserves string storage by maintaining a table, called the intern pool, that contains a single reference to each unique literal string declared or created programmatically in your program. Consequently, an instance of a literal string with a particular value only exists once in the system. For example, if you assign the same literal string to several variables, the runtime retrieves the same reference to the literal string from the intern pool and assigns it to each variable. The Intern method uses the intern pool to search for a string equal to the value of str. If such a string exists, its reference in the intern pool is returned. If the string does not exist, a reference to str is added to the intern pool, then that reference is returned.”\nUp until this point, you may not have been aware of this pool or the string interning behaviour at all. This is understandable because the .NET compiler does a bloody good job of interning strings for you. In fact, any literal string will be automatically interned by the compiler. Almost any late-bound (at runtime) string, however, is not. A quick code example – let us define a program that simply creates 10,000,000 of the same string and assigns it to a Dictionary:\nclass Program\r{\r// The Dictionary that will be used to store our strings at an int // (so that they are not de-referenced and GC'd)\rprivate static readonly IDictionary\u0026lt;int, string\u0026gt; _dict = new Dictionary\u0026lt;int, string\u0026gt;();\rstatic void Main(string[] args)\r{\rConsole.WriteLine(\u0026quot;Storing a non-constant string 10000000 times\u0026quot;);\rvar stopwatch = new Stopwatch();\rstopwatch.Start();\r// Loop 10,000,000 times\rfor (int i = 0; i \u0026lt; 10000000; i++)\r{\r// Define the same string repeatedly\rstring blah = \u0026quot;Hello! This Is A String!\u0026quot;;\r// Add it to the Dictionary at the index of i\r_dict.Add(i, blah);\r}\rstopwatch.Stop();\rConsole.WriteLine(\u0026quot;Memory Used: \u0026quot; + Process.GetCurrentProcess().PagedMemorySize64.ToString(\u0026quot;n\u0026quot;) + \u0026quot; bytes\u0026quot;);\rConsole.WriteLine(\u0026quot;Elapsed milliseconds: \u0026quot; + stopwatch.ElapsedMilliseconds);\rConsole.WriteLine(\u0026quot;Press any key\u0026quot;);\rConsole.ReadKey();\r}\r}\rRunning this program results in the following output:\nA Repeated Literal String It uses a relatively small amount of memory (269 megs) and takes very little time, since the compiler detects that the string which we’re creating is a literal and thus interns it for us automatically. Now, let us make a slight change to the application by creating a late-bound string which won’t be interned automatically:\nclass Program\r{\r// The Dictionary that will be used to store our strings at an int // (so that they are not de-referenced and GC'd)\rprivate static readonly IDictionary\u0026lt;int, string\u0026gt; _dict = new Dictionary\u0026lt;int, string\u0026gt;();\rstatic void Main(string[] args)\r{\r// Define a string which will be concatenated with another string later\rstring dynamicString = \u0026quot;Some Other Large String\u0026quot;;\rConsole.WriteLine(\u0026quot;Storing a non-constant string 10000000 times\u0026quot;);\rvar stopwatch = new Stopwatch();\rstopwatch.Start();\r// Loop 10,000,000 times\rfor (int i = 0; i \u0026lt; 10000000; i++)\r{\r// Define the same string repeatedly\rstring blah = \u0026quot;Hello! This Is A String!\u0026quot; + dynamicString;\r// Add it to the Dictionary at the index of i\r_dict.Add(i, blah);\r}\rstopwatch.Stop();\rConsole.WriteLine(\u0026quot;Memory Used: \u0026quot; + Process.GetCurrentProcess().PagedMemorySize64.ToString(\u0026quot;n\u0026quot;) + \u0026quot; bytes\u0026quot;);\rConsole.WriteLine(\u0026quot;Elapsed milliseconds: \u0026quot; + stopwatch.ElapsedMilliseconds);\rConsole.WriteLine(\u0026quot;Press any key\u0026quot;);\rConsole.ReadKey();\r}\r}\rRunning THIS code yields crappier results:\nRepeated Late Bound String Note that we use nearly five times as much memory (1.379 gigs) as we did before! And that our application got considerably slower! Sadly, we can’t do much about the slower part because concatenating strings takes time and effort. However, we can intern the string to return our memory usage back to something realistic while adding minimal computational cost. We do this with the string.Intern(string) method:\nclass Program\r{\r// The Dictionary that will be used to store our strings at an int // (so that they are not de-referenced and GC'd)\rprivate static readonly IDictionary\u0026lt;int, string\u0026gt; _dict = new Dictionary\u0026lt;int, string\u0026gt;();\rstatic void Main(string[] args)\r{\r// Define a string which will be concatenated with another string later\rstring dynamicString = \u0026quot;Some Other Large String\u0026quot;;\rConsole.WriteLine(\u0026quot;Storing a non-constant string 10000000 times\u0026quot;);\rvar stopwatch = new Stopwatch();\rstopwatch.Start();\r// Loop 10,000,000 times\rfor (int i = 0; i \u0026lt; 10000000; i++)\r{\r// Define the same string repeatedly\rstring blah = \u0026quot;Hello! This Is A String!\u0026quot; + dynamicString;\r// Add it to the Dictionary at the index of i\r// Intern string \u0026quot;blah\u0026quot; to save memory!\r_dict.Add(i, string.Intern(blah));\r}\rstopwatch.Stop();\rConsole.WriteLine(\u0026quot;Memory Used: \u0026quot; + Process.GetCurrentProcess()\r.PagedMemorySize64.ToString(\u0026quot;n\u0026quot;) + \u0026quot; bytes\u0026quot;);\rConsole.WriteLine(\u0026quot;Elapsed milliseconds: \u0026quot; + stopwatch.ElapsedMilliseconds);\rConsole.WriteLine(\u0026quot;Press any key\u0026quot;);\rConsole.ReadKey();\r}\r}\rAnd the results:\nRepeated Late Bound String (Interned) Note that by interning the late-bound string, we reduced memory usage considerably (272 megs, with only an additional 4 megs used for pointers) while adding only a minimal amount of additional computation (600 milliseconds)… 80% less memory used for the cost of an additional 20% computation is generally a good trade.\nNow the icing on the cake. Compile and run your code in Release mode to allow for even more compiler optimizations:\nRepeated Late Bound String (Interned) - Release Mode Even less memory and even less interning costs! It’s win-win and just a friendly reminder to always release your code in Release mode. 🙂\nSo, who loves interns? You do. Remember: when you must use the same string repeatedly in an application (perhaps when storing a huge collection of User objects with a FirstName property, where many of your users are “David”, “John”, “Tim”, etc.), intern the string. Why create N copies of the exact same object, all using up your precious managed memory?\n","permalink":"https://www.davidhaney.io/who-loves-interns/","tags":["dotnet","csharp","memory","performance","programming"],"title":"Who Loves Interns?"},{"categories":["blog"],"content":"Something which I feel carries a lot of confusion in the .NET realm is virtual methods. During interviews, I tend to ask candidates about virtual methods: why and when they’d use one, what the purposes is, how a virtual method “works” under the hood, and how it differs from “shadowing”. Surprisingly, in what has probably been over one hundred interviews with senior-ish candidates, I don’t believe that more than one or two of them have answered anything about virtual methods correctly. From this I conclude that the understanding of virtual methods is not strong among the typical developer… And so, let us dive in.\nVirtual methods exist to allow you to not just override, but completely replace functionality within a derivation hierarchy. In other words, you can change the functionality of a less derived class from within a more derived class. This is different from shadowing in that shadowing overrides the functionality of a method for the given derived class only and not the entire class hierarchy.\nShadowing is accomplished by declaring the same non-virtual method which exists in the base class, while adding the “new” keyword to indicate that shadowing behaviour was intended.\nOverriding is accomplished by declaring the same virtual method which exists in the base class, while adding the “override” keyword to indicate that overriding behaviour was intended.\nA code sample will make these differences very clear. Let us define a base Vehicle class (as abstract so that it cannot be instantiated) and a deriving Motorcycle class. Both will output information to the Console about the number of wheels that they have:\n/// \u0026lt;summary\u0026gt;\r/// Represents a Vehicle.\r/// \u0026lt;/summary\u0026gt;\rpublic abstract class Vehicle\r{\r/// \u0026lt;summary\u0026gt;\r/// Prints the Number of Wheels to the Console. /// Virtual so can be changed by more derived types.\r/// \u0026lt;/summary\u0026gt;\rpublic virtual void VirtualPrintNumberOfWheels()\r{\rConsole.WriteLine(\u0026quot;Number of Wheels: 4\u0026quot;);\r}\r/// \u0026lt;summary\u0026gt;\r/// Prints the Number of Wheels to the Console.\r/// \u0026lt;/summary\u0026gt;\rpublic void ShadowPrintNumberOfWheels()\r{\rConsole.WriteLine(\u0026quot;Number of Wheels: 4\u0026quot;);\r}\r}\r/// \u0026lt;summary\u0026gt;\r/// Represents a Motorcycle.\r/// \u0026lt;/summary\u0026gt;\rpublic class Motorcycle : Vehicle\r{\r/// \u0026lt;summary\u0026gt;\r/// Prints the Number of Wheels to the Console. /// Overrides base method.\r/// \u0026lt;/summary\u0026gt;\rpublic override void VirtualPrintNumberOfWheels()\r{\rConsole.WriteLine(\u0026quot;Number of Wheels: 2\u0026quot;);\r}\r/// \u0026lt;summary\u0026gt;\r/// Prints the Number of Wheels to the Console. /// Shadows base method.\r/// \u0026lt;/summary\u0026gt;\rpublic new void ShadowPrintNumberOfWheels()\r{\rConsole.WriteLine(\u0026quot;Number of Wheels: 2\u0026quot;);\r}\r}\rAbove we’ve defined two classes: an abstract base Vehicle class which has a virtual and non-virtual method which both do the same thing, and a Motorcycle class which implements the Vehicle class while overriding the virtual method and shadowing the normal method. Now we will call the methods with different Type signatures to see the differences:\nstatic void Main(string[] args)\r{\r// Instantiate a Motorcycle as type Motorcycle\rMotorcycle vehicle = new Motorcycle();\rConsole.WriteLine(\u0026quot;Calling Shadow on Motorcycle as Type Motorcycle\u0026quot;);\rvehicle.ShadowPrintNumberOfWheels();\rConsole.WriteLine(\u0026quot;Calling Virtual on Motorcycle as Type Motorcycle\u0026quot;);\rvehicle.VirtualPrintNumberOfWheels();\r// Instantiate a Motorcycle as type Vehicle\rVehicle otherVehicle = new Motorcycle();\rConsole.WriteLine(\u0026quot;Calling Shadow on Motorcycle as Type Vehicle\u0026quot;);\rotherVehicle.ShadowPrintNumberOfWheels();\rConsole.WriteLine(\u0026quot;Calling Virtual on Motorcycle as Type Vehicle\u0026quot;);\rotherVehicle.VirtualPrintNumberOfWheels();\rConsole.ReadKey();\r}\rBefore we reveal the results, ask yourself: what do you expect the outcomes for each call to be? You may be surprised. And now, the results:\nShadow vs Virtual Results Were your assumptions about the code correct? The virtual method’s override has changed the functionality within the base Vehicle class as well. Therefore, it should be clear to see that virtual methods should be used whenever you wish to change not just the functionality of a method within your currently derived class, but within all deriving and base classes.\nFinally, I’d like to address one more constant “debate”. You will undoubtedly hear repeatedly in your career that virtual methods are very expensive to override and use. This is because of the fact that the actual instance method to call can only be determined at runtime, and not compile time. This is an old wives tale that has very little basis in reality (due to the intelligent use of “vtables”): virtual methods have little to no additional cost over regular methods..\n","permalink":"https://www.davidhaney.io/what-is-a-virtual-method-anyway/","tags":["dotnet","csharp","performance","programming"],"title":"What is a Virtual Method, Anyway?"},{"categories":["blog"],"content":"I’m a bit tipsy at the moment, so hopefully this post goes well.\nA question that I like to ask while interviewing individuals is: “why would you want to use an interface?” I get a ton of answers that span the supposed gamut of programming; some are good and some are of course terrible, however I’d like to share some input on what I feel is the importance of interfaces.\nInterfaces allow for the application of polymorphism (the ability of an object to exhibit multiple behaviours). In short: more polymorphism = less refactoring in the future when you change code. Less changes to do new things = happier programmers.\nLet’s say that your customer/business asks you to write an application that makes a Dog speak. So you code something similar to this:\n/// \u0026lt;summary\u0026gt;\r/// Represents a Dog.\r/// \u0026lt;/summary\u0026gt;\rpublic class Dog\r{\r/// \u0026lt;summary\u0026gt;\r/// Makes the Dog Speak.\r/// \u0026lt;/summary\u0026gt;\rpublic void Speak()\r{\rConsole.WriteLine(\u0026quot;Woof!\u0026quot;);\r}\r}\r/// \u0026lt;summary\u0026gt;\r/// Makes a Dog Speak.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;param name=\u0026quot;dog\u0026quot;\u0026gt;The Dog to make Speak.\u0026lt;/param\u0026gt;\rpublic static void MakeDogSpeak(Dog dog)\r{\r// Sanitize\rif (dog == null)\r{\rreturn;\r}\r// Make the Dog Speak\rdog.Speak();\r}\rIn this scenario, making the Dog Speak is quite easy:\nstatic void Main(string[] args)\r{\rvar dog = new Dog();\rMakeDogSpeak(dog);\r}\rNow imagine that the customer/business changes their mind – which of course never ever happens. They tell you that they have a Cat which can also speak, and that you need to change your program to allow both the Dog and Cat to speak. What a pain in the butt! Now we need to create the Cat class and add another method:\n/// \u0026lt;summary\u0026gt;\r/// Represents a Cat.\r/// \u0026lt;/summary\u0026gt;\rpublic class Cat\r{\r/// \u0026lt;summary\u0026gt;\r/// Makes the Cat Speak.\r/// \u0026lt;/summary\u0026gt;\rpublic void Speak()\r{\rConsole.WriteLine(\u0026quot;Meow! (while glaring at you since Cats are evil)\u0026quot;);\r}\r}\r/// \u0026lt;summary\u0026gt;\r/// Makes a Cat Speak.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;param name=\u0026quot;dog\u0026quot;\u0026gt;The Cat to make Speak.\u0026lt;/param\u0026gt;\rpublic static void MakeCatSpeak(Cat cat)\r{\r// Sanitize\rif (cat == null)\r{\rreturn;\r}\r// Make the Cat Speak\rcat.Speak();\r}\rNow, making the Dog and Cat Speak is also quite easy, if a bit cumbersome:\nstatic void Main(string[] args)\r{\rvar dog = new Dog();\rMakeDogSpeak(dog);\rvar cat = new Cat();\rMakeCatSpeak(cat);\r}\rAre you seeing the tedious trend here? What happens when the customer/business now wants a Horse to speak, and a Lamb to speak, and a Tiger to speak, and even an Elephant to speak? What about a Unicorn or Human or Computer that needs to speak? Creating a method for each of these objects totally sucks and destroys your ability to write efficient OO code. So, what are we to do? Enter interfaces.\nInterfaces exist to define a contract for common functionality which can be used in polymorphic ways. Let me repeat that once more to stress importance, interfaces are an important part of OO programming that allows you to capture common functionality and avoid tedious (and unnecessary) code.\nLet’s look at the prior examples. We know that we have a Dog and a Cat, and that they both Speak. From this we can analyze and see some common functionality. We realize that we can define a common interface for these objects that encapsulates speaking:\n/// \u0026lt;summary\u0026gt;\r/// Represents an object that can Speak.\r/// \u0026lt;/summary\u0026gt;\rpublic interface ICanSpeak\r{\r/// \u0026lt;summary\u0026gt;\r/// Makes the object Speak.\r/// \u0026lt;/summary\u0026gt;\rvoid Speak();\r}\rLet us now update our Dog and Cat such that they implement this interface:\n/// \u0026lt;summary\u0026gt;\r/// Represents a Dog.\r/// \u0026lt;/summary\u0026gt;\rpublic class Dog : ICanSpeak\r{\r/// \u0026lt;summary\u0026gt;\r/// Makes the Dog Speak.\r/// \u0026lt;/summary\u0026gt;\rpublic void Speak()\r{\rConsole.WriteLine(\u0026quot;Woof!\u0026quot;);\r}\r}\r/// \u0026lt;summary\u0026gt;\r/// Represents a Cat.\r/// \u0026lt;/summary\u0026gt;\rpublic class Cat : ICanSpeak\r{\r/// \u0026lt;summary\u0026gt;\r/// Makes the Cat Speak.\r/// \u0026lt;/summary\u0026gt;\rpublic void Speak()\r{\rConsole.WriteLine(\u0026quot;Meow! (while glaring at you since Cats are evil)\u0026quot;);\r}\r}\rFinally, let’s take advantage of the polymorphism that this interface now offers and reduce our two speaking methods into one useful method:\n/// \u0026lt;summary\u0026gt;\r/// Makes something Speak.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;param name=\u0026quot;speaker\u0026quot;\u0026gt;The Speaker to make Speak.\u0026lt;/param\u0026gt;\rpublic static void MakeSomethingSpeak(ICanSpeak speaker)\r{\r// Sanitize\rif (speaker == null)\r{\rreturn;\r}\r// Make the Speaker Speak\rspeaker.Speak();\r}\rTwo methods become one. In fact, N methods become one if they implement the common functionality encapsulated in the interface. This means that you can sleep peacefully at night knowing that no matter how many things the customer/business wants to have speak tomorrow, it’s not that hard to update your code to handle it. Finally, let us update our Main which runs the speak commands just to close the example:\nstatic void Main(string[] args)\r{\rvar dog = new Dog();\rMakeSomethingSpeak(dog);\rvar cat = new Cat();\rMakeSomethingSpeak(cat);\r}\rNow, no matter how many objects are defined that are to speak, we need only the single method as long as those objects implement the appropriate interface.\nThere are many other benefits to interfaces which we will discuss in future posts. However, I hope that this begins to exhibit the purpose and usefulness of defining interfaces and having types which share common functionality implement them. Cheers!\n","permalink":"https://www.davidhaney.io/why-use-interfaces/","tags":["dotnet","csharp","programming"],"title":"Why Use Interfaces?"},{"categories":["blog"],"content":"This is my first post. I hope that it doesn’t suck.\nAs of .NET 2.0, Microsoft introduced the concept of generics. Generics is a concept that allow you to “template” methods and types such as classes and interfaces in a (generally) type-safe way. Upon compilation, generic type metadata is stored in IL, and JIT’d as you reference the generic method or class with an actual type at runtime. Value types each get their own “copy” of the JIT’d generic code, whereas reference types share a single instance of the code. This is because the generic implementation is identical for reference types – they’re all just pointers.\nAnyway, you probably use generics daily, whether or not you know them by name. Ever used List\u0026lt;T\u0026gt;? IEnumerable\u0026lt;T\u0026gt;? Dictionary\u0026lt;TKey, TValue\u0026gt;? Those are pre-built generic classes and interfaces which are included in the .NET libraries.\nYou can build your own generic classes and methods in C#. Many people do this also daily. Perhaps to create a custom collection or caching method… but, have you ever used generic constraints?\nGeneric constraints allow you to constrain, or restrict, the types that can be used by your generic classes and methods. This enables you to examine the generic type with certainty about it’s functionality. Why does this matter? It allows you to invoke methods or properties on your type without using reflection.\nThe syntax is such that at the type or method definition, you add “where T : your constraints here“. An example:\npublic static T Create\u0026lt;T\u0026gt; where T : new()\rThe “new()” in this case indicates that whatever type is passed to this method MUST have a parameterless constructor. What does this afford you? You can now new up T in your method!\nAs a simple example, let us create the static factory method above that serves only to return a newly created instance of the generic type parameter:\n/// \u0026lt;summary\u0026gt;\r/// Creates an object of Type T.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;typeparam name=\u0026quot;T\u0026quot;\u0026gt;The Type to Create.\u0026lt;/typeparam\u0026gt;\r/// \u0026lt;returns\u0026gt;An instantiated Type of T.\u0026lt;/returns\u0026gt;\rpublic static T Create\u0026lt;T\u0026gt;() where T : new()\r{\rreturn new T();\r}\rSweet! Now we can new up stuff like crazy. Let’s define a “Hat” class that represents a hat:\n/// \u0026lt;summary\u0026gt;\r/// A Hat.\r/// \u0026lt;/summary\u0026gt;\rpublic class Hat\r{\r/// \u0026lt;summary\u0026gt;\r/// The Size of the Hat.\r/// \u0026lt;/summary\u0026gt;\rpublic int Size { get; set; }\r}\rNow let’s new that bad boy up:\nstatic void Main(string[] args)\r{\rvar myHat = Create\u0026lt;Hat\u0026gt;();\r}\rVoila, we just made a hat with generics and generic constraints! But that’s pretty boring, and you might ask “why not just new up the bloody hat and skip all of that factory stuff?” Good question. We’ll discuss that in the future, but there are many reasons you might wish not to directly instantiate a concrete type in your methods or classes.\nLet’s take a more practical example. Let us define the Size property in an interface and make the Hat implement that interface:\n/// \u0026lt;summary\u0026gt;\r/// A Hat.\r/// \u0026lt;/summary\u0026gt;\rpublic class Hat : IHaveASize\r{\r/// \u0026lt;summary\u0026gt;\r/// The Size of the Hat.\r/// \u0026lt;/summary\u0026gt;\rpublic int Size { get; set; }\r}\r/// \u0026lt;summary\u0026gt;\r/// Represents an object that has a Size.\r/// \u0026lt;/summary\u0026gt;\rpublic interface IHaveASize\r{\r/// \u0026lt;summary\u0026gt;\r/// The Size.\r/// \u0026lt;/summary\u0026gt;\rint Size { get; set; }\r}\rNow let’s create a generic method that checks if a size is “valid” which we’ll define as being greater than 0.\n/// \u0026lt;summary\u0026gt;\r/// Determines whether or not Type T has a Valid Size.\r/// \u0026lt;/summary\u0026gt;\r/// \u0026lt;typeparam name=\u0026quot;T\u0026quot;\u0026gt;The Type.\u0026lt;/typeparam\u0026gt;\r/// \u0026lt;param name=\u0026quot;sizedObject\u0026quot;\u0026gt;The object which has a Size.\u0026lt;/param\u0026gt;\r/// \u0026lt;returns\u0026gt;true if the Size is greater than 0, false otherwise.\u0026lt;/returns\u0026gt;\rpublic static bool IsSizeValid\u0026lt;T\u0026gt;(T sizedObject) where T : IHaveASize\r{\rreturn sizedObject.Size \u0026gt; 0;\r}\rNote that we can access the Size property in the IsSizeValid method because of our generic constraint! Now to execute the code:\nstatic void Main(string[] args)\r{\rvar myHat = Create\u0026lt;Hat\u0026gt;();\r// We don't have to say IsSizeValid\u0026lt;Hat\u0026gt;(myHat) - the type is inferred by the parameter!\rvar result = IsSizeValid(myHat);\r}\rAnd there you have it. An introduction into generic constraints!\n","permalink":"https://www.davidhaney.io/an-overview-of-generic-constraints/","tags":["dotnet","csharp","generics","programming"],"title":"An Overview of Generic Constraints"},{"categories":null,"content":"","permalink":"https://www.davidhaney.io/manifest.json","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://www.davidhaney.io/search/_index.de/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://www.davidhaney.io/search/_index.es/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://www.davidhaney.io/search/_index.fr/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://www.davidhaney.io/search/_index.hi/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://www.davidhaney.io/search/_index.jp/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://www.davidhaney.io/search/_index.nl/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://www.davidhaney.io/search/_index.pl/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://www.davidhaney.io/search/_index.ru/","tags":null,"title":""},{"categories":null,"content":"","permalink":"https://www.davidhaney.io/search/_index.zh-cn/","tags":null,"title":""}]